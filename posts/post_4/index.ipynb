{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Post 4 - Implementing Perceptron\"\n",
    "author: Col McDermott\n",
    "date: \"03-26-2025\"\n",
    "description: \"An introductory exploration of the Perceptron algorithm -- FINISH.\"\n",
    "format: html\n",
    "code-fold: true\n",
    "execute:\n",
    "  warning: false\n",
    "  message: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "The primary goal of this brief study on the perceptron algorithm is to explore and investigate the processes under the hood of the perceptron algorithm $-$ one of the oldest machine learning algorithms to exist.  The functionality of and logic behind the perceptron algorithm is a backbone to many modern ML methods and models.  It is crucial to develop at least a basic understanding of how perceptron works and why it's design is as such.  This introductory dive into the inner-workings of perceptron involves examining the conditions in which the algorithm is successful, the conditions in which the algorithm must be manually adjusted to prevent non-convergence, various ways the algorithm can be refined to operate on more complex data, and the general limitations and implications associated with this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Including all additional imports\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import torch as tch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Porting over perceptron implementation\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from perceptron import Perceptron, PerceptronOptimizer\n",
    "\n",
    "# Generating the data - Some code borrowed from Prof. Chodrow\n",
    "## Linearly separable 2D data\n",
    "y1 = tch.arange(500) >= int(500 / 2)\n",
    "X1 = y1[:, None] + tch.normal(0.0, 0.2, size = (500, 2))\n",
    "X1 = tch.cat((X1, tch.ones((X1.shape[0], 1))), 1)\n",
    "\n",
    "## Not linearly separable 2D data\n",
    "y2 = tch.arange(500) >= int(500 / 2)\n",
    "X2 = y2[:, None] + tch.normal(0.0, 0.3, size = (500, 2))\n",
    "X2 = tch.cat((X2, tch.ones((X2.shape[0], 1))), 1)\n",
    "\n",
    "## 6D data\n",
    "y3 = tch.arange(500) >= int(500 / 2)\n",
    "X3 = y3[:, None] + tch.normal(0.0, 0.2, size = (500, 6))\n",
    "X3 = tch.cat((X3, tch.ones((X3.shape[0], 1))), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code above generates three random data sets: 1. Linearly separable data with two features.  2. Not linearly separable data with two features.  3. Possibly linearly separable data with 6 features*\n",
    "\n",
    "To test and investigate the perceptron algorithm, I have created three random datasets.  The first data set has two features and is intentionally linearly separable $-$ the perceptron algorithm should converge to a loss of 0 for this dataset.  The second data set also has two features but is intentionally *not* linearly separable $-$ the perceptron algorithm will not be able to converge to a loss of 0 for this data set and will need to be manually terminated after a certain number of iterations.  The third data set has 6 features and is used to show how the perceptron works with data that can't be easily visualized $-$ this dataset is possibly linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Perceptron Algorithm\n",
    "\n",
    "For this introductory study, I have implemented a rudimentary version of the perceptron algorithm.  This implementation involves three class definitions: `LinearModel`,   `Perceptron`, and `PerceptronOptimizer`.\n",
    "\n",
    "**`LinearModel`**:\n",
    "-   `self.w`: An instance variable to store the weights vector of a linear model\n",
    "-   `score(X)`: Method to compute the score $s_i$ for each data point in the feature matrix **$X$**\n",
    "-   `predict(X)`: Method to compute the classification prediction $\\hat{y}_i$ $\\in\\{0, 1\\}$ for each data point\n",
    "\n",
    "**`Perceptron` (inherits from `LinearModel`)**:\n",
    "-   `loss(X, y)`: Method to compute the misclassification rate in the data $-$ A point i is classified correctly if $s_i\\bar{y}_i > 0$, where $\\bar{y}_i \\in \\{-1, 1\\}$ is the modified classification label.\n",
    "-   `grad(x, y)`: Method to compute the perceptron update for a sampled data point\n",
    "    -   This method takes as arguments `x` $-$ the row of the feature matrix **$X$** corresponding to the sampled data point $-$ and `y` $-$ the classification target vector\n",
    "    -   This method first computes the score $s_i$ of the sampled data point with $<$**$w$**$, x_i>$\n",
    "    -   This method then computes the vector $-I[s_i(2y_i - 1) < 0]y_ix_i$ which represents the perceptron update (moving the score $s_i$ closer to the target $y_i$) with respect to the sampled data point.\n",
    "    -   Ultimately, this method computes an update to a sampled data point that later adjusts the weight vector of the perceptron algorithm to better fit the sampled data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Perceptron Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [3] doesn't match the broadcast shape [1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     y_i \u001b[38;5;241m=\u001b[39m y1[i]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# perform a perceptron update using the random data point\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Observe the algorithm's performance\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/S_25/CSCI 0451A/Blog/Col-McDermott.github.io/posts/post_4/perceptron.py:102\u001b[0m, in \u001b[0;36mPerceptronOptimizer.step\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Computing the loss for the sampled point\u001b[39;00m\n\u001b[1;32m    101\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss(X, y)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgrad(x_i, y_i)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [3] doesn't match the broadcast shape [1, 3]"
     ]
    }
   ],
   "source": [
    "# Code provided by Prof. Chodrow\n",
    "\n",
    "# Instantiate a model and an optimizer\n",
    "p = Perceptron()\n",
    "opt = PerceptronOptimizer(p)\n",
    "\n",
    "# Initialize the loss\n",
    "loss = 1.0\n",
    "\n",
    "# Keeping track of loss values\n",
    "loss_vec = []\n",
    "\n",
    "# Iteration counter\n",
    "j = 0\n",
    "n = X1.size()[0]\n",
    "while (loss > 0) and (j < 1000):\n",
    "    \n",
    "    # not part of the update: just for tracking our progress    \n",
    "    loss = p.loss(X1, y1) \n",
    "    loss_vec.append(loss)\n",
    "    \n",
    "    # pick a random data point\n",
    "    i = tch.randint(n, size = (1,))\n",
    "    x_i = X1[[i],:]\n",
    "    y_i = y1[i]\n",
    "    \n",
    "    # perform a perceptron update using the random data point\n",
    "    opt.step(x_i, y_i)\n",
    "    j += 1\n",
    "\n",
    "# Observe the algorithm's performance\n",
    "print(\"Changing loss values:\\n\")\n",
    "for i in range(len(loss_vec)):\n",
    "    print(f\"Iteration: {i} | Loss: {loss_vec[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code above checks the implementation of the perceptron algorithm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting With Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linearly Separable Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code description*\n",
    "\n",
    "Analysis of perceptron on linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not Linearly Separable Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code description*\n",
    "\n",
    "Analysis of perceptron on not linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Higher-Dimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code description*\n",
    "\n",
    "Analysis of perceptron on higher dimensional, possibly linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code description*\n",
    "\n",
    "Exploration of the minibatch perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minibatch Perceptron Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code Description*\n",
    "\n",
    "Evaluation of the minibatch perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Thoughts\n",
    "\n",
    "Analysis of the findings from this study.\n",
    "\n",
    "*During the implementation process of this replication study, I collaborated with _____.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
