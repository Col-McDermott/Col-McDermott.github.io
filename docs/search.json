[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Blog Info",
    "section": "",
    "text": "Middlebury College CSCI 0451A Blog - Col McDermott\n\n\n\nCheck out my completed blog posts from\n\n\nCSCI 0451 Machine Learning\n\n\nwith Professor Phil Chodrow"
  },
  {
    "objectID": "posts/post_5/index.html",
    "href": "posts/post_5/index.html",
    "title": "Post 5 - Implementing Logistic Regression",
    "section": "",
    "text": "The main purpose of this brief study is to implement from scratch and investigate the procedures under the hood of logistic regression. Logistic regression is a well-known machine-learning method used primarily for binary classification. This widely-recognized ML technique involves several common ML algorithm characteristics such as a linear model, model scores, a weights vector, and the corresponding empirical risk optimization problem. This base-level analysis of logistic regression serves to develop a general understanding of many common ML binary classification architectures as well as introduce several relevant topics exercised and enhanced in more advanced ML algorithms.\nThe optimization problem in logistic regression aims to find the specific weights vector that minimizes the “loss” of the linear model and is solved using a gradient descent procedure. This introductory exploration of logistic regression involves implementing a basic linear model with a corresponding weights vector, designing a linear regression object to compute the empirical risk loss values and corresponding gradients, and a gradient descent optimizer that performs the weights vector optimization task.\nTo analyze my version of logistic regression, I have conducted the following experiments on my implementation:\n\nEvaluating a standard gradient descent procedure to optimize the weights vector of the linear model to achieve high classification accuracy.\n\nEvaluating a gradient descent method with momentum to achieve faster convergence (i.e. optimizing the weights vector to minimize the empirical risk) than standard gradient descent.\nExploring overfitting in which logistic regression is used to produce a linear model and a weights vector that performs flawlessly on training data but less well on unseen testing data.\n\nEvaluating the efficacy of my logistic regression implementation on real-world, empirical data (using standard train-test-split, model fitting, and model scoring methods).\n\nFor my implementation of logistic regression, check out logistic.py.\n\n\n\n\nCode\n# Including all additional imports\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport torch as tch\nimport pandas as pd\nimport numpy as np\n\n# Porting over logistic regression implementation\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\ntch.manual_seed(100) # For consistent data generation\nplt.style.use('seaborn-v0_8-whitegrid') # For consistent plotting\n\n\nFor this introductory study, I have implemented a rudimentary version of logistic regression. This implementation involves three class definitions: LinearModel, LogisticRegression, and GradientDescentOptimizer.\nLinearModel:\n\nself.w: An instance variable to store the weights vector \\(\\mathbf{w}\\) of a linear model.\nscore(X): A method to compute the score \\(s_i\\) for each data point in the feature matrix \\(X\\) using: \\[\ns_i = \\langle\\mathbf{w}, x_i\\rangle\n\\]\npredict(X): A method to compute the vector of classification predictions \\(\\hat{y}\\) for each data point in the feature matrix \\(X\\): \\[\n\\hat{y}_i = \\begin{cases}\n1 & \\text{if} & s_i &gt; 0.5\\\\\n0 & \\text{else}\n\\end{cases}\n\\]\n\nLogisticRegression (inherits from LinearModel):\n\nsig(x): A method that represents the logistic sigmoid function. This method computes the vector of values \\(\\sigma(x_i)\\) given an vector of inputs \\(\\mathbf{x}\\): \\[\n\\sigma(x_i) = \\frac{1}{1 + e^{-x_i}}\n\\]\nloss(X, y): A method to compute the empirical risk \\(L(\\mathbf{w})\\) using the logistic loss function. Note that the scores are computed using \\(X\\) and the loss computation involves \\(\\mathbf{y}\\). The returned value is a scalar/real number that gives a quantitative measure of the empirical risk: \\[\nL(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n{[-y_ilog(\\sigma(s_i)) - (1 - y_i)log(1 - \\sigma(s_i))]}\n\\]\ngrad(X, y): A method to compute the gradient \\(\\nabla L(\\mathbf{w})\\) of the empirical risk function \\(L(\\mathbf{w})\\). Note that the scores are computed using \\(X\\) and the gradient computation involves \\(\\mathbf{y}\\). The returned value is a vector with \\(p\\) entries that represents the direction and rate of change of the empirical risk function given the current weights vector \\(\\mathbf{w}\\). Ultimately, this method computes the gradient used to perform a step of gradient descent in logistic regression: \\[\n\\nabla L(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n{(\\sigma(s_i) - y_i)x_i}\n\\]\n\nGradientDescentOptimizer:\n\nself.lr: An instance variable of a LogisticRegression object. This variable is used to access the implicit model’s current weights vector \\(\\mathbf{w_k}\\) during a gradient descent step.\nself.prev_w: An instance variable of the previous weights vector \\(\\mathbf{w_{k-1}}\\) (initialized to self.lr.w). This variable is used to access the model’s previous weights vector \\(\\mathbf{w_{k-1}}\\) during a gradient descent step.\nstep(X, y, alpha, beta): A method that computes a step of gradient descent with momentum. This method performs the optimization update to \\(\\mathbf{w}\\) using the gradient of the LogisticRegression object (which needs \\(X\\) and \\(\\mathbf{y}\\)). The arguments alpha and beta are hyperparameters for the optimization update; \\(\\alpha\\) is the learning rate and \\(\\beta\\) is the momentum scalar. The update to the weights vector \\(\\mathbf{w}\\) performed by this method is: \\[\n\\mathbf{w_{k+1}} = \\mathbf{w_{k}} - \\alpha\\nabla L(\\mathbf{w}) + \\beta(\\mathbf{w_k} - \\mathbf{w_{k-1}})\n\\]\n\n\n\n\nIn order to conduct some basic experiments with my logistic regression implementation, it is necessary to generate some simulated data for a binary classification problem. This experiment data is generated and visualized in the code cell below:\n\n\nCode\n# Generating data for binary classification - code provided by Prof. Chodrow\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    y = tch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + tch.normal(0.0, noise, size = (n_points,p_dims))\n    X = tch.cat((X, tch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\nX, y = classification_data(noise = 0.5)\n\n# Visualizing the generated data above\nfig, ax = plt.subplots(1, 1, figsize = (5, 5))\ntargets = [0, 1]\nmarkers = [\"o\" , \",\"]\n\n# Custom color map\ncolors = [\"purple\", \"darkorange\"]  \ncmap = LinearSegmentedColormap.from_list(\"my_cmap\", colors, N = 256)\n\n# Some code provided by Prof. Chodrow\nfor i in range(2):\n    ix = y == targets[i]\n    ax.scatter(X[ix, 0], X[ix, 1], s = 20,  c = 2 * y[ix] - 1, facecolors = \"none\", edgecolors = \"none\", cmap = cmap, vmin = -2, vmax = 2, alpha = 0.75, marker = markers[i])\nax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\nax.set_title(\"Binary Classification Data\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above generates some random binary classification data and visualizes the generated data (some code provided by Prof. Chodrow).\nFigure 1\nAbove is a visualization of the generated binary classification data. While the data above does not appear to be highly conducive to perfectly separable decision boundaries, it still displays clear general regions of the two groups of data points.\n\n\nCode\n# Model interpretation helper methods\n## Loss value plotter\ndef loss_plot(loss_vec1, loss_vec2 = None):\n    \n    # Plotting the loss values of the model across each optimization iteration\n    fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n    ax.plot(loss_vec1, color = \"purple\", linewidth = 2)\n    title = \"Evolution of Empirical Loss Value\"\n    if (loss_vec2 != None):\n        title = \"Gradient Descent Method Comparison\\nof Empirical Loss Value Convergence\"\n        ax.plot(loss_vec2, color = \"darkorange\", linewidth = 2)\n        ax.legend([\"Standard\", \"Momentum\"], frameon = True)\n        ax.axhline(loss_vec2[-2], color = \"black\", linestyle = \"--\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Optimization Iteration\")\n    ax.set_ylabel(\"Loss\")\n    plt.tight_layout()\n\n# Model accuracy plotter\ndef acc_plot(accs1, accs2 = None):\n    \n    # Plotting the accuracies of the model across each optimization iteration\n    fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n    ax.plot(accs1, color = \"purple\", linewidth = 2)\n    if (accs2 != None):\n        ax.plot(accs2, color = \"darkorange\", linewidth = 2)\n        ax.legend([\"Training Accuracy\", \"Testing Accuracy\"], frameon = True)\n    ax.set_title(\"Model Accuracy Across Optimization Iteration\")\n    ax.set_xlabel(r\"Gradient Descent Iteration\")\n    ax.set_ylabel(\"Accuarcy\")\n    plt.tight_layout()\n\n# Decision line plotting helper method - code provided by Prof. Chodrow\ndef draw_line(X, w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = tch.linspace(x_min, x_max, X.shape[0])\n    y = -1 * (((w_[0] * x) + w_[2])/w_[1])\n    ax.plot(x, y, **kwargs)\n\n# Decision region plotter\ndef decision_bound(model, X, y):\n\n    # Creating a mesh grid\n    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n    fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n    \n    # Drawing the decision line\n    draw_line(X, model.w, x_min, x_max, ax, color = \"slategray\", linewidth = 2)\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n\n    # Custom color map\n    colors = [\"purple\", \"darkorange\"]  \n    cmap = LinearSegmentedColormap.from_list(\"my_cmap\", colors, N=256)\n\n    # Some code below provided by Prof. Chodrow\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix, 0], X[ix, 1], s = 20,  c = 2 * y[ix] - 1, facecolors = \"none\", edgecolors = \"none\", cmap = cmap, vmin = -2, vmax = 2, alpha = 0.75, marker = markers[i])\n    \n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    ax.set_title(\"Decision Regions of Logistic Regression Model\")\n    ax.text(X[:, 0].max() * 0.8, X[:, 1].max() * 0.85, f\"Model Accuracy:\\n{round(acc(model, X, y), 4) * 100}%\", fontsize = 10, ha = \"center\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad = 0.3\"))\n    plt.tight_layout()\n\n# Function to calculate model accuracy\ndef acc(model, X, y):\n    \n    # Compute model predictions\n    preds = model.predict(X)\n\n    # Determine the number of correct predictions\n    correct_preds = ((preds == y) * 1).float()\n\n    # Return the rate of correct predictions\n    return tch.mean(correct_preds).item()\n\n\nCode above defines plotting methods for observing the model’s empirical loss value evolution, the model’s accuracy, and the model’s classification decision boundaries (some code provided by Prof. Chodrow).\n\n\n\n\nCode\n# Testing vanilla gradient descent - some code provided by Prof. Chodrow\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\n# Keeping track of the initial model accuracy\nit = 100\nlosses = []\naccs = []\nfor i in range(it):\n    \n    # Recording current loss value and model accuracy\n    losses.append(LR.loss(X, y))\n    accs.append(acc(LR, X, y))\n\n    # Displaying gradient descent progress for the first 5 iterations\n    if (i % 20 == 0) & (i &lt; 100):\n        print(f\"Iteration {i}:\")\n        print(f\"Current Loss value: {round(losses[-1], 3)}\")\n        print(f\"Current Model Accuracy: {round(accs[-1], 4) * 100}%\\n\")\n    opt.step(X, y, alpha = 0.1, beta = 0)\n\nprint(\"...\\n\")\nprint(f\"Iteration {it}:\")\nprint(f\"Final Loss value: {round(losses[-1], 3)}\")\nprint(f\"Final Model Accuracy: {round(accs[-1], 4) * 100}%\")\n\n\nIteration 0:\nCurrent Loss value: 0.861\nCurrent Model Accuracy: 89.33%\n\nIteration 20:\nCurrent Loss value: 0.734\nCurrent Model Accuracy: 91.67%\n\nIteration 40:\nCurrent Loss value: 0.646\nCurrent Model Accuracy: 92.33%\n\nIteration 60:\nCurrent Loss value: 0.583\nCurrent Model Accuracy: 92.33%\n\nIteration 80:\nCurrent Loss value: 0.536\nCurrent Model Accuracy: 92.67%\n\n...\n\nIteration 100:\nFinal Loss value: 0.501\nFinal Model Accuracy: 92.67%\n\n\nCode above conducts a standard gradient descent procedure with \\(100\\) iterations using LogisticRegression and GradientDescent objects and outputs the results of every 20th iteration (some code provided by Prof. Chodrow)\nThe output above highlights the model’s empirical loss value and accuracy at every 20th iteration (\\(100\\) iterations total) during a standard gradient descent procedure. Over the optimization iterations displayed above, the model’s empirical loss value decreases while its accuracy increases.\n\n\nCode\n# Visualizing standard gradient descent\nloss_plot(losses)\n\n\n\n\n\n\n\n\n\nCode above plots the evolution of the model’s empirical loss value across the optimization iterations of standard gradient descent.\nFigure 2\nAs further shown in the figure above, the model’s empirical loss value decreases across each iteration of the optimization process. The initial empirical loss value is approximately \\(0.86\\) while the post-optimization loss value is about \\(0.5\\), indicating a ~\\(48\\%\\) loss-value decrease. This suggests that my implementation of logistic regression works as is generally expected. That is, during a standard gradient procedure, the current linear model’s weights vector \\(\\mathbf{w}\\) yields a lower empirical loss value than that of the previous model during each iteration of optimization.\n\n\nCode\n# Plotting the evolution of the model accuracy\nacc_plot(accs)\n\n\n\n\n\n\n\n\n\nCode above plots the model accuracy at each iteration of the standard gradient descent procedure above.\nFigure 3\nAs displayed above, the accuracy of the model generally increases with each iteration of the gradient procedure, climbing from ~\\(89\\%\\) to ~\\(93\\%\\) accuracy. This aligns with the fact that the empirical loss value of the model is shown to decrease with each iteration of gradient descent. That is, the model’s accuracy and its empirical loss value are inversely related.\n\n\nCode\n# Plotting the decision boundaries\ndecision_bound(LR, X, y)\n\n\n\n\n\n\n\n\n\nCode above plots the data along with the separation line of the refined model (displaying the model accuracy as well).\nFigure 4\nThe figure above depicts the classification decision boundary of the refined model. While the classification line does not indicate flawless classification for all data points, it illustrates correct classification for most of the data. This is supported by the model’s fairly high ~\\(93\\%\\) accuracy.\n\n\n\n\n\nCode\n# Testing gradient descent with momentum - some code provided by Prof. Chodrow\nLR_v = LogisticRegression()\nopt_v = GradientDescentOptimizer(LR_v) # Vanilla gradient descent optimizer\nLR_m = LogisticRegression()\nopt_m = GradientDescentOptimizer(LR_m) # Momentum gradient descent optimizer\n\n# Arrays to store the loss values of the models optimized with and without momentum\nlosses_v = []\nlosses_m = []\nat_tol = False\ni = 0\n\n# Iteration counters to track convergence\nv_i = np.iinfo(np.int64).max\nm_i = np.iinfo(np.int64).max\nwhile (at_tol != True):\n    \n    # Recording current loss value and model accuracy\n    loss_v = LR_v.loss(X, y)\n    loss_m = LR_m.loss(X, y)\n    if (loss_v &gt; 0.3):\n        losses_v.append(loss_v)\n        opt_v.step(X, y, alpha = 0.1, beta = 0)\n    elif (v_i &gt; i):\n        v_i = i\n    if (loss_m &gt; 0.3):\n        losses_m.append(loss_m)\n        opt_m.step(X, y, alpha = 0.1, beta = 0.9)\n    elif (m_i &gt; i):\n        m_i = i\n    \n    # Terminating condition\n    if (loss_v &lt; 0.3) & (loss_m &lt; 0.3):\n        at_tol = True\n    \n    # Displaying gradient descent progress for the first 5 iterations\n    if (i % 20 == 0) & (i &lt; 100):\n        print(f\"Iteration {i}:\")\n        print(f\"Current Loss value (Standard Gradient Descent): {round(losses_v[-1], 3)}\")\n        print(f\"Current Loss value (Momentum Gradient Descent): {round(losses_m[-1], 3)}\\n\")\n    i += 1\n\nprint(\"...\\n\")\nprint(f\"Standard Gradient Descent Converges at {v_i} Iterations\")\nprint(f\"Momentum Gradient Descent Converges at {m_i} Iterations\")\n\n\nIteration 0:\nCurrent Loss value (Standard Gradient Descent): 0.749\nCurrent Loss value (Momentum Gradient Descent): 0.982\n\nIteration 20:\nCurrent Loss value (Standard Gradient Descent): 0.651\nCurrent Loss value (Momentum Gradient Descent): 0.437\n\nIteration 40:\nCurrent Loss value (Standard Gradient Descent): 0.584\nCurrent Loss value (Momentum Gradient Descent): 0.311\n\nIteration 60:\nCurrent Loss value (Standard Gradient Descent): 0.535\nCurrent Loss value (Momentum Gradient Descent): 0.301\n\nIteration 80:\nCurrent Loss value (Standard Gradient Descent): 0.498\nCurrent Loss value (Momentum Gradient Descent): 0.301\n\n...\n\nStandard Gradient Descent Converges at 576 Iterations\nMomentum Gradient Descent Converges at 47 Iterations\n\n\nCode above initializes LogisticRegression and GradientDescentOptimizer objects for both standard gradient descent and gradient descent with momentum optimizers. The output above shows convergence progress every \\(20\\) iterations and depicts the number of optimization iterations needed for each gradient descent process to achieve a loss value \\(&lt; 0.3\\) (some code provided by Prof. Chodrow).\nIn this experiment, I compare the rate of convergence between the standard gradient descent and gradient descent with momentum procedures. For this experiment, I loosely define “convergence” to be when the optimized model’s empirical loss value is \\(&lt; 0.3\\). Note that for this analysis, I have set the hyperparameters to \\(\\alpha_s, \\alpha_m = 0.1\\) for both optimizers and \\(\\beta_s = 0\\) (standard gradient descent), \\(\\beta_m = 0.9\\) (gradient descent with momentum). As presented in the output above, the gradient descent with momentum optimizer achieves a loss value \\(&lt; 0.3\\) in significantly fewer iterations than that of the standard gradient descent optimizer. That is, the gradient descent with momentum process appears to converge in about \\(\\frac{1}{12}\\) of the iterations needed for the standard gradient descent process to converge.\n\n\nCode\n# Comparing the convergence rates of the two gradient descent methods\nloss_plot(losses_v, losses_m)\n\n\n\n\n\n\n\n\n\nCode above plots the convergence rates of the empirical loss values for both the standard gradient descent and gradient descent with momentum procedures.\nFigure 5\nThis figure above is a visual accompaniment to the output from above. In line with the aforementioned output, the gradient descent with momentum optimizer clearly converges to the tolerance (\\(&lt; 0.3\\)) empirical loss value rapidly quicker (at \\(47\\) iterations) than the standard gradient descent optimizer (at \\(576\\) iterations).\n\n\n\nTo observe an instance of my logistic regression implementation leading to overfitting, I have generated two new sets of binary classification data. The two new data sets are identically designed, where the number of features per data point exceeds the number of data points. One dataset will be used as the “training data” and the other the “testing data”.\n\n\nCode\n# Generating binary classification data for overfitting - code provided by Prof. Chodrow\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n\nCode above generates the “training” and “testing” data sets for the overfitting experiment (some code provided by Prof. Chodrow).\n\n\nCode\n# Investigating overfitting as the number of dimensions in the data grows past the number of data points\nLR  = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\n# Current accuracy variable\naccs = []\ntst_accs = []\ncurr_acc = 0\nj = 0\nwhile (curr_acc &lt; 1.0):\n    \n    # Calculating the current model accuracy\n    curr_acc = acc(LR, X_train, y_train)\n    tst_acc = acc(LR, X_test, y_test)\n    accs.append(curr_acc)\n    tst_accs.append(tst_acc)\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9) # Using gradient descent with momentum\n    \n    # Iteration counter\n    j+= 1\n\nprint(f\"Model Achieved 100.0% Accuracy on Training Data in {j} Iterations (Gradient Descent w/ Momentum)\")\nprint(f\"Model Accuracy on Testing Data: {round(tst_accs[-1], 4) * 100}%\")\n\n\nModel Achieved 100.0% Accuracy on Training Data in 14 Iterations (Gradient Descent w/ Momentum)\nModel Accuracy on Testing Data: 86.0%\n\n\nCode above conducts a model overfitting experiment by first setting the number of data features to be greater than the number of data points, then fitting a model using LogisticRegression and GradientDescentOptimizer objects, and finally comparing the model’s training-data performance to its testing-data performance.\nAs indicated by the output above, the logistic regression model is able to achieve maximum classification accuracy on the training data in a fairly small number of iterations. However, when evaluated on the unseen testing data, the model’s accuracy drops significantly to about \\(86\\%\\). This is clear sign of overfitting as the model’s flawless performance on the training data does not translate to its performance on the testing data.\n\n\nCode\n# Plotting the model's training and testing accuracies as training accuracy approaches 100%\nacc_plot(accs, tst_accs)\n\n\n\n\n\n\n\n\n\nCode above plots the model’s training and testing accuracies at each iteration of optimization during the overfitting experiment.\nFigure 6\nAs shown in the above figure, the model’s training accuracy climbs all the way to \\(100\\%\\) while its testing accuracy plateaus at around \\(86\\%\\) and does not increase further (in \\(14\\) iterations). This figure stands as a visual representation of how the model is overfitting to the training data and consequently sacrificing performance on the unseen testing data.\n\n\n\nThe final experiment in this brief study involves evaluating the performance of my logistic regression implementation on real-world data. For this experiment, I will be using a public data set published to kaggle.com by Aleksei Zagorskii. This dataset contains user profile data from the popular Russian social media and networking platform VKontakte (VK.com) and is used to classify users as either real-human users or bots. The data features a combination of numerical and categorical information extracted from public VK.com user profiles.\n\n\nCode\n# Reading-in and processing the data to evaluate the model with\ndf = pd.read_csv(\"./bots_vs_users.csv\")\n\n# Selecting a subset of the features to use - selecting only the features with float64 data types\ncols = []\nfor col in df.columns:\n    if (df[col].dtype == \"float64\"):\n        cols.append(col)\ncols.append(\"target\")\ndf_pruned = df[cols].copy()\ndf_pruned = df_pruned.dropna() # Dropping any rows with missing values\n\n# Converting the dataframe into tch.tensors\nX = tch.tensor(df_pruned[cols[:-1]].values).float()\ny = tch.tensor(df_pruned[cols[-1]].values).float()\n\n# Normalizing the data through standardization to ensure effective and expected behavior from logistic regression\nmean = tch.mean(X, dim = 0, keepdim = True)\nstd = tch.std(X, dim = 0, keepdim = True)\nX_s = (X - mean) / std\n\n# Splitting the data into training (60%) and testing/validation (40%) data\nX_train, X_tmp, y_train, y_tmp = train_test_split(X_s, y, test_size = 0.4, random_state = 1, stratify = y)\n\n# Splitting the testing/validation data into testing (50%, 20% overall) and validation (40%, 20% overall) data\nX_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size = 0.5, random_state = 1, stratify = y_tmp)\n\n\nCode above imports the dataset used for this experiment, preprocesses it by extracting only the columns with a float64 data type, converts the data into tch.tensors for the feature matrix \\(\\mathbf{X}\\) and the target vector \\(\\mathbf{y}\\), and standardizes the data to ensure expected logistic regression behavior and avoid machine-precision error. Finally, I conduct a test-train-split procedure to split the data into \\(60\\%\\) for training, \\(20\\%\\) for validation, and \\(20\\%\\) for testing.\nOriginally, the full dataset contained \\(60\\) features, many of which have missing values (NaN, or \"Unknown\"). To avoid problems converting the missing data into a form that is both interpretable by my logistic regression model and will not distort the classification performance, I have chosen to take a subset of the original columns present in the data. For the purposes of this experiment, I have chosen to select only the columns with a float64 data type. The pruned dataset I used for this experiment has \\(15\\) columns (including the target column):\n\nposts_count\navg_likes\nlinks_ratio\nhashtags_ratio\navg_keywords\navg_text_length\nattachments_ratio\navg_comments\nreposts_ratio\nads_ratio\navg_views\nposting_frequency_days\nphone_numbers_ratio\navg_text_uniqueness\ntarget\n\nBefore fitting my logistic regression models, the data is first standardized (using \\(\\frac{\\mathbf{x} - \\mu}{\\sigma}\\) where \\(\\mu, \\sigma\\) are the mean and STD. of the data). The purpose of this is to ensure numerical stability and optimization symmetry to support my logistic regression model’s ability to converge to an appropriate weights vector \\(\\mathbf{w}\\) and minimize the empirical loss function. Finally, the data is split into training (\\(60\\%\\) of original), validation (\\(20\\%\\) of original), and testing (\\(20\\%\\) of original) subsets.\n\n\nCode\n# Evlauating my logistic repression implementation on binary classification on the real-world data\n## Initializing the models for standard gradient descent and gradient descent with momentum\nLR_v = LogisticRegression()\nopt_v = GradientDescentOptimizer(LR_v)\nLR_m = LogisticRegression()\nopt_m = GradientDescentOptimizer(LR_m)\n\n# Bookkeeping arrays\nit = 100\nlosses_v_tr = []\nlosses_m_tr = []\nlosses_v_val = []\nlosses_m_val = []\naccs_v_tr = []\naccs_m_tr = []\naccs_v_val = []\naccs_m_val = []\nfor i in range(it):\n    \n    # Recording current loss and accuracy values on the training and validation data for standard and momentum gradient descent\n    losses_v_tr.append(LR_v.loss(X_train, y_train))\n    losses_m_tr.append(LR_m.loss(X_train, y_train))\n    losses_v_val.append(LR_v.loss(X_val, y_val))\n    losses_m_val.append(LR_m.loss(X_val, y_val))\n    accs_v_tr.append(acc(LR_v, X_train, y_train))\n    accs_m_tr.append(acc(LR_m, X_train, y_train))\n    accs_v_val.append(acc(LR_v, X_val, y_val))\n    accs_m_val.append(acc(LR_m, X_val, y_val))\n    opt_v.step(X_train, y_train, alpha = 0.1, beta = 0)\n    opt_m.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n    if (True):\n        if (i % 25 == 0) & ((i &gt; 0) & (i &lt; 76)):\n            print(\"--------------\")\n            print(f\"Iteration: {i}\\n\")\n            print(\"Training Performance:\")\n            print(f\"Current Loss value (Standard Gradient Descent): {round(losses_v_tr[-1], 3)}\")\n            print(f\"Current Loss value (Momentum Gradient Descent): {round(losses_m_tr[-1], 3)}\")\n            print(f\"Current Accuracy (Standard Gradient Descent): {(round(accs_v_tr[-1], 4) * 100):.2f}%\")\n            print(f\"Current Accuracy (Momentum Gradient Descent): {(round(accs_m_tr[-1], 4) * 100):.2f}%\\n\")\n            print(\"Validation Performance:\")\n            print(f\"Current Loss value (Standard Gradient Descent): {round(losses_v_val[-1], 3)}\")\n            print(f\"Current Loss value (Momentum Gradient Descent): {round(losses_m_val[-1], 3)}\")\n            print(f\"Current Accuracy (Standard Gradient Descent): {(round(accs_v_val[-1], 4) * 100):.2f}%\")\n            print(f\"Current Accuracy (Momentum Gradient Descent): {(round(accs_m_val[-1], 4) * 100):.2f}%\\n\")\nprint(\"...\")\nprint(f\"Final Iteration: {it}\")\nprint(\"Final Training Performance:\")\nprint(f\"Loss value (Standard Gradient Descent): {round(losses_v_tr[-1], 3)}\")\nprint(f\"Loss value (Momentum Gradient Descent): {round(losses_m_tr[-1], 3)}\")\nprint(f\"Accuracy (Standard Gradient Descent): {(round(accs_v_tr[-1], 4) * 100):.2f}%\")\nprint(f\"Accuracy (Momentum Gradient Descent): {(round(accs_m_tr[-1], 4) * 100):.2f}%\\n\")\nprint(\"Final Validation Performance:\")\nprint(f\"Loss value (Standard Gradient Descent): {round(losses_v_val[-1], 3)}\")\nprint(f\"Loss value (Momentum Gradient Descent): {round(losses_m_val[-1], 3)}\")\nprint(f\"Accuracy (Standard Gradient Descent): {(round(accs_v_val[-1], 4) * 100):.2f}%\")\nprint(f\"Accuracy (Momentum Gradient Descent): {(round(accs_m_val[-1], 4) * 100):.2f}%\\n\")\n\n\n--------------\nIteration: 25\n\nTraining Performance:\nCurrent Loss value (Standard Gradient Descent): 0.905\nCurrent Loss value (Momentum Gradient Descent): 0.622\nCurrent Accuracy (Standard Gradient Descent): 79.62%\nCurrent Accuracy (Momentum Gradient Descent): 85.37%\n\nValidation Performance:\nCurrent Loss value (Standard Gradient Descent): 0.998\nCurrent Loss value (Momentum Gradient Descent): 0.762\nCurrent Accuracy (Standard Gradient Descent): 78.42%\nCurrent Accuracy (Momentum Gradient Descent): 82.37%\n\n--------------\nIteration: 50\n\nTraining Performance:\nCurrent Loss value (Standard Gradient Descent): 0.74\nCurrent Loss value (Momentum Gradient Descent): 0.592\nCurrent Accuracy (Standard Gradient Descent): 84.53%\nCurrent Accuracy (Momentum Gradient Descent): 88.01%\n\nValidation Performance:\nCurrent Loss value (Standard Gradient Descent): 0.823\nCurrent Loss value (Momentum Gradient Descent): 0.73\nCurrent Accuracy (Standard Gradient Descent): 82.37%\nCurrent Accuracy (Momentum Gradient Descent): 85.61%\n\n--------------\nIteration: 75\n\nTraining Performance:\nCurrent Loss value (Standard Gradient Descent): 0.672\nCurrent Loss value (Momentum Gradient Descent): 0.587\nCurrent Accuracy (Standard Gradient Descent): 85.97%\nCurrent Accuracy (Momentum Gradient Descent): 87.89%\n\nValidation Performance:\nCurrent Loss value (Standard Gradient Descent): 0.752\nCurrent Loss value (Momentum Gradient Descent): 0.715\nCurrent Accuracy (Standard Gradient Descent): 83.45%\nCurrent Accuracy (Momentum Gradient Descent): 85.97%\n\n...\nFinal Iteration: 100\nFinal Training Performance:\nLoss value (Standard Gradient Descent): 0.644\nLoss value (Momentum Gradient Descent): 0.584\nAccuracy (Standard Gradient Descent): 86.93%\nAccuracy (Momentum Gradient Descent): 88.01%\n\nFinal Validation Performance:\nLoss value (Standard Gradient Descent): 0.728\nLoss value (Momentum Gradient Descent): 0.707\nAccuracy (Standard Gradient Descent): 84.53%\nAccuracy (Momentum Gradient Descent): 85.97%\n\n\n\nCode above fits my logistic regression model to the real-world training data and records the classification accuracy on the real-world validation data during model training.\nTo evaluate my implementation of logistic regression on the real-world data, I fit two logistic regression models to the training data: One model is optimized with standard gradient descent and the other uses gradient descent with momentum. As displayed by the output above, the empirical losses for each model decrease over the iterations of gradient descent on both the training and validation data. Along these lines, the corresponding accuracies of each model gradually increase, which is expected.\n\n\n\n\nCode\n# Plotting the losses for training data\nloss_plot(losses_v_tr, losses_m_tr)\n\n\n\n\n\n\n\n\n\nCode above plots the loss value evolution for standard and momentum gradient descent on the training data.\nFigure 7 (7a above & 7b below)\nAbove shows the loss value evolution of the standard and momentum gradient descent models optimizing over the training data. As expected the loss values decrease over the optimization iterations for both models, but the model using momentum gradient descent minimizes the loss much faster. Below are the changing model accuracies for the training data. Inline with the plot above, the accuracies for each model improve during the optimization process, but the model using momentum gradient descent is able to achieve a higher accuracy faster than the model using standard gradient descent.\n\n\nCode\n# Plotting the accuracies for training data\nacc_plot(accs_v_tr, accs_m_tr)\nplt.legend([\"Standard\", \"Momentum\"], frameon = True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above plots the accuracies for standard and momentum gradient descent on the training data.\n\n\n\n\n\nCode\n# Plotting the losses for training data\nloss_plot(losses_v_val, losses_m_val)\n\n\n\n\n\n\n\n\n\nCode above plots the loss value evolution for standard and momentum gradient descent on the validation data.\nFigure 8 (8a above & 8b below)\nAbove shows the loss value evolution of the standard and momentum gradient descent models now optimizing over the validation data. Again as expected the loss values decrease over the optimization iterations for both models, and the model using momentum gradient descent continues to minimize the loss much faster. Below shows the changing model accuracies now for the validation data. Further aligned with the plot above, the accuracies for each model still improve during the optimization process, and the model using momentum gradient descent is again able to achieve a higher accuracy faster than the model using standard gradient descent.\n\n\nCode\n# Plotting the accuracies for training data\nacc_plot(accs_v_val, accs_m_val)\nplt.legend([\"Standard\", \"Momentum\"], frameon = True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above plots the accuracies for standard and momentum gradient descent on the validation data.\n\n\n\n\n\nCode\n# Evaluating the loss and accuracy of each model on the testing data\nloss_v_tst = LR_v.loss(X_test, y_test)\nloss_m_tst = LR_m.loss(X_test, y_test)\nacc_v_tst = acc(LR_v, X_test, y_test)\nacc_m_tst = acc(LR_m, X_test, y_test)\n\nprint(\"Testing Data Empirical Loss:\")\nprint(f\"Model 1 (Standard Gradient Descent) | {round(loss_v_tst, 3)}\")\nprint(f\"Model 2 (Momentum Gradient Descent) | {round(loss_m_tst, 3)}\")\nprint(\"-----\\n\")\nprint(\"Testing Data Accuracy:\")\nprint(f\"Model 1 (Standard Gradient Descent) | {(round(acc_v_tst, 4) * 100):.2f}%\")\nprint(f\"Model 2 (Momentum Gradient Descent) | {(round(acc_m_tst, 4) * 100):.2f}%\")\n\n\nTesting Data Empirical Loss:\nModel 1 (Standard Gradient Descent) | 0.622\nModel 2 (Momentum Gradient Descent) | 0.538\n-----\n\nTesting Data Accuracy:\nModel 1 (Standard Gradient Descent) | 89.61%\nModel 2 (Momentum Gradient Descent) | 91.04%\n\n\nCode above computes the empirical loss and model accuracy on the testing data for each logistic regression model.\nFinally, on the unseen testing data, the model using gradient descent with momentum yields both a lower loss value (\\(0.538\\)) and higher accuracy (\\(91.04\\%\\)) than those of the model using standard gradient descent (\\(0.622\\), \\(89.61\\%\\)). Interestingly, both models yield smaller loss values and achieve higher accuracies from the unseen testing data than the training or validation data."
  },
  {
    "objectID": "posts/post_5/index.html#implementing-logistic-regression",
    "href": "posts/post_5/index.html#implementing-logistic-regression",
    "title": "Post 5 - Implementing Logistic Regression",
    "section": "",
    "text": "Code\n# Including all additional imports\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport torch as tch\nimport pandas as pd\nimport numpy as np\n\n# Porting over logistic regression implementation\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\ntch.manual_seed(100) # For consistent data generation\nplt.style.use('seaborn-v0_8-whitegrid') # For consistent plotting\n\n\nFor this introductory study, I have implemented a rudimentary version of logistic regression. This implementation involves three class definitions: LinearModel, LogisticRegression, and GradientDescentOptimizer.\nLinearModel:\n\nself.w: An instance variable to store the weights vector \\(\\mathbf{w}\\) of a linear model.\nscore(X): A method to compute the score \\(s_i\\) for each data point in the feature matrix \\(X\\) using: \\[\ns_i = \\langle\\mathbf{w}, x_i\\rangle\n\\]\npredict(X): A method to compute the vector of classification predictions \\(\\hat{y}\\) for each data point in the feature matrix \\(X\\): \\[\n\\hat{y}_i = \\begin{cases}\n1 & \\text{if} & s_i &gt; 0.5\\\\\n0 & \\text{else}\n\\end{cases}\n\\]\n\nLogisticRegression (inherits from LinearModel):\n\nsig(x): A method that represents the logistic sigmoid function. This method computes the vector of values \\(\\sigma(x_i)\\) given an vector of inputs \\(\\mathbf{x}\\): \\[\n\\sigma(x_i) = \\frac{1}{1 + e^{-x_i}}\n\\]\nloss(X, y): A method to compute the empirical risk \\(L(\\mathbf{w})\\) using the logistic loss function. Note that the scores are computed using \\(X\\) and the loss computation involves \\(\\mathbf{y}\\). The returned value is a scalar/real number that gives a quantitative measure of the empirical risk: \\[\nL(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n{[-y_ilog(\\sigma(s_i)) - (1 - y_i)log(1 - \\sigma(s_i))]}\n\\]\ngrad(X, y): A method to compute the gradient \\(\\nabla L(\\mathbf{w})\\) of the empirical risk function \\(L(\\mathbf{w})\\). Note that the scores are computed using \\(X\\) and the gradient computation involves \\(\\mathbf{y}\\). The returned value is a vector with \\(p\\) entries that represents the direction and rate of change of the empirical risk function given the current weights vector \\(\\mathbf{w}\\). Ultimately, this method computes the gradient used to perform a step of gradient descent in logistic regression: \\[\n\\nabla L(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n{(\\sigma(s_i) - y_i)x_i}\n\\]\n\nGradientDescentOptimizer:\n\nself.lr: An instance variable of a LogisticRegression object. This variable is used to access the implicit model’s current weights vector \\(\\mathbf{w_k}\\) during a gradient descent step.\nself.prev_w: An instance variable of the previous weights vector \\(\\mathbf{w_{k-1}}\\) (initialized to self.lr.w). This variable is used to access the model’s previous weights vector \\(\\mathbf{w_{k-1}}\\) during a gradient descent step.\nstep(X, y, alpha, beta): A method that computes a step of gradient descent with momentum. This method performs the optimization update to \\(\\mathbf{w}\\) using the gradient of the LogisticRegression object (which needs \\(X\\) and \\(\\mathbf{y}\\)). The arguments alpha and beta are hyperparameters for the optimization update; \\(\\alpha\\) is the learning rate and \\(\\beta\\) is the momentum scalar. The update to the weights vector \\(\\mathbf{w}\\) performed by this method is: \\[\n\\mathbf{w_{k+1}} = \\mathbf{w_{k}} - \\alpha\\nabla L(\\mathbf{w}) + \\beta(\\mathbf{w_k} - \\mathbf{w_{k-1}})\n\\]"
  },
  {
    "objectID": "posts/post_5/index.html#experimenting-with-logistic-regression",
    "href": "posts/post_5/index.html#experimenting-with-logistic-regression",
    "title": "Post 5 - Implementing Logistic Regression",
    "section": "",
    "text": "In order to conduct some basic experiments with my logistic regression implementation, it is necessary to generate some simulated data for a binary classification problem. This experiment data is generated and visualized in the code cell below:\n\n\nCode\n# Generating data for binary classification - code provided by Prof. Chodrow\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    y = tch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + tch.normal(0.0, noise, size = (n_points,p_dims))\n    X = tch.cat((X, tch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\nX, y = classification_data(noise = 0.5)\n\n# Visualizing the generated data above\nfig, ax = plt.subplots(1, 1, figsize = (5, 5))\ntargets = [0, 1]\nmarkers = [\"o\" , \",\"]\n\n# Custom color map\ncolors = [\"purple\", \"darkorange\"]  \ncmap = LinearSegmentedColormap.from_list(\"my_cmap\", colors, N = 256)\n\n# Some code provided by Prof. Chodrow\nfor i in range(2):\n    ix = y == targets[i]\n    ax.scatter(X[ix, 0], X[ix, 1], s = 20,  c = 2 * y[ix] - 1, facecolors = \"none\", edgecolors = \"none\", cmap = cmap, vmin = -2, vmax = 2, alpha = 0.75, marker = markers[i])\nax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\nax.set_title(\"Binary Classification Data\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above generates some random binary classification data and visualizes the generated data (some code provided by Prof. Chodrow).\nFigure 1\nAbove is a visualization of the generated binary classification data. While the data above does not appear to be highly conducive to perfectly separable decision boundaries, it still displays clear general regions of the two groups of data points.\n\n\nCode\n# Model interpretation helper methods\n## Loss value plotter\ndef loss_plot(loss_vec1, loss_vec2 = None):\n    \n    # Plotting the loss values of the model across each optimization iteration\n    fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n    ax.plot(loss_vec1, color = \"purple\", linewidth = 2)\n    title = \"Evolution of Empirical Loss Value\"\n    if (loss_vec2 != None):\n        title = \"Gradient Descent Method Comparison\\nof Empirical Loss Value Convergence\"\n        ax.plot(loss_vec2, color = \"darkorange\", linewidth = 2)\n        ax.legend([\"Standard\", \"Momentum\"], frameon = True)\n        ax.axhline(loss_vec2[-2], color = \"black\", linestyle = \"--\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Optimization Iteration\")\n    ax.set_ylabel(\"Loss\")\n    plt.tight_layout()\n\n# Model accuracy plotter\ndef acc_plot(accs1, accs2 = None):\n    \n    # Plotting the accuracies of the model across each optimization iteration\n    fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n    ax.plot(accs1, color = \"purple\", linewidth = 2)\n    if (accs2 != None):\n        ax.plot(accs2, color = \"darkorange\", linewidth = 2)\n        ax.legend([\"Training Accuracy\", \"Testing Accuracy\"], frameon = True)\n    ax.set_title(\"Model Accuracy Across Optimization Iteration\")\n    ax.set_xlabel(r\"Gradient Descent Iteration\")\n    ax.set_ylabel(\"Accuarcy\")\n    plt.tight_layout()\n\n# Decision line plotting helper method - code provided by Prof. Chodrow\ndef draw_line(X, w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = tch.linspace(x_min, x_max, X.shape[0])\n    y = -1 * (((w_[0] * x) + w_[2])/w_[1])\n    ax.plot(x, y, **kwargs)\n\n# Decision region plotter\ndef decision_bound(model, X, y):\n\n    # Creating a mesh grid\n    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n    fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n    \n    # Drawing the decision line\n    draw_line(X, model.w, x_min, x_max, ax, color = \"slategray\", linewidth = 2)\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n\n    # Custom color map\n    colors = [\"purple\", \"darkorange\"]  \n    cmap = LinearSegmentedColormap.from_list(\"my_cmap\", colors, N=256)\n\n    # Some code below provided by Prof. Chodrow\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix, 0], X[ix, 1], s = 20,  c = 2 * y[ix] - 1, facecolors = \"none\", edgecolors = \"none\", cmap = cmap, vmin = -2, vmax = 2, alpha = 0.75, marker = markers[i])\n    \n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    ax.set_title(\"Decision Regions of Logistic Regression Model\")\n    ax.text(X[:, 0].max() * 0.8, X[:, 1].max() * 0.85, f\"Model Accuracy:\\n{round(acc(model, X, y), 4) * 100}%\", fontsize = 10, ha = \"center\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad = 0.3\"))\n    plt.tight_layout()\n\n# Function to calculate model accuracy\ndef acc(model, X, y):\n    \n    # Compute model predictions\n    preds = model.predict(X)\n\n    # Determine the number of correct predictions\n    correct_preds = ((preds == y) * 1).float()\n\n    # Return the rate of correct predictions\n    return tch.mean(correct_preds).item()\n\n\nCode above defines plotting methods for observing the model’s empirical loss value evolution, the model’s accuracy, and the model’s classification decision boundaries (some code provided by Prof. Chodrow).\n\n\n\n\nCode\n# Testing vanilla gradient descent - some code provided by Prof. Chodrow\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\n# Keeping track of the initial model accuracy\nit = 100\nlosses = []\naccs = []\nfor i in range(it):\n    \n    # Recording current loss value and model accuracy\n    losses.append(LR.loss(X, y))\n    accs.append(acc(LR, X, y))\n\n    # Displaying gradient descent progress for the first 5 iterations\n    if (i % 20 == 0) & (i &lt; 100):\n        print(f\"Iteration {i}:\")\n        print(f\"Current Loss value: {round(losses[-1], 3)}\")\n        print(f\"Current Model Accuracy: {round(accs[-1], 4) * 100}%\\n\")\n    opt.step(X, y, alpha = 0.1, beta = 0)\n\nprint(\"...\\n\")\nprint(f\"Iteration {it}:\")\nprint(f\"Final Loss value: {round(losses[-1], 3)}\")\nprint(f\"Final Model Accuracy: {round(accs[-1], 4) * 100}%\")\n\n\nIteration 0:\nCurrent Loss value: 0.861\nCurrent Model Accuracy: 89.33%\n\nIteration 20:\nCurrent Loss value: 0.734\nCurrent Model Accuracy: 91.67%\n\nIteration 40:\nCurrent Loss value: 0.646\nCurrent Model Accuracy: 92.33%\n\nIteration 60:\nCurrent Loss value: 0.583\nCurrent Model Accuracy: 92.33%\n\nIteration 80:\nCurrent Loss value: 0.536\nCurrent Model Accuracy: 92.67%\n\n...\n\nIteration 100:\nFinal Loss value: 0.501\nFinal Model Accuracy: 92.67%\n\n\nCode above conducts a standard gradient descent procedure with \\(100\\) iterations using LogisticRegression and GradientDescent objects and outputs the results of every 20th iteration (some code provided by Prof. Chodrow)\nThe output above highlights the model’s empirical loss value and accuracy at every 20th iteration (\\(100\\) iterations total) during a standard gradient descent procedure. Over the optimization iterations displayed above, the model’s empirical loss value decreases while its accuracy increases.\n\n\nCode\n# Visualizing standard gradient descent\nloss_plot(losses)\n\n\n\n\n\n\n\n\n\nCode above plots the evolution of the model’s empirical loss value across the optimization iterations of standard gradient descent.\nFigure 2\nAs further shown in the figure above, the model’s empirical loss value decreases across each iteration of the optimization process. The initial empirical loss value is approximately \\(0.86\\) while the post-optimization loss value is about \\(0.5\\), indicating a ~\\(48\\%\\) loss-value decrease. This suggests that my implementation of logistic regression works as is generally expected. That is, during a standard gradient procedure, the current linear model’s weights vector \\(\\mathbf{w}\\) yields a lower empirical loss value than that of the previous model during each iteration of optimization.\n\n\nCode\n# Plotting the evolution of the model accuracy\nacc_plot(accs)\n\n\n\n\n\n\n\n\n\nCode above plots the model accuracy at each iteration of the standard gradient descent procedure above.\nFigure 3\nAs displayed above, the accuracy of the model generally increases with each iteration of the gradient procedure, climbing from ~\\(89\\%\\) to ~\\(93\\%\\) accuracy. This aligns with the fact that the empirical loss value of the model is shown to decrease with each iteration of gradient descent. That is, the model’s accuracy and its empirical loss value are inversely related.\n\n\nCode\n# Plotting the decision boundaries\ndecision_bound(LR, X, y)\n\n\n\n\n\n\n\n\n\nCode above plots the data along with the separation line of the refined model (displaying the model accuracy as well).\nFigure 4\nThe figure above depicts the classification decision boundary of the refined model. While the classification line does not indicate flawless classification for all data points, it illustrates correct classification for most of the data. This is supported by the model’s fairly high ~\\(93\\%\\) accuracy.\n\n\n\n\n\nCode\n# Testing gradient descent with momentum - some code provided by Prof. Chodrow\nLR_v = LogisticRegression()\nopt_v = GradientDescentOptimizer(LR_v) # Vanilla gradient descent optimizer\nLR_m = LogisticRegression()\nopt_m = GradientDescentOptimizer(LR_m) # Momentum gradient descent optimizer\n\n# Arrays to store the loss values of the models optimized with and without momentum\nlosses_v = []\nlosses_m = []\nat_tol = False\ni = 0\n\n# Iteration counters to track convergence\nv_i = np.iinfo(np.int64).max\nm_i = np.iinfo(np.int64).max\nwhile (at_tol != True):\n    \n    # Recording current loss value and model accuracy\n    loss_v = LR_v.loss(X, y)\n    loss_m = LR_m.loss(X, y)\n    if (loss_v &gt; 0.3):\n        losses_v.append(loss_v)\n        opt_v.step(X, y, alpha = 0.1, beta = 0)\n    elif (v_i &gt; i):\n        v_i = i\n    if (loss_m &gt; 0.3):\n        losses_m.append(loss_m)\n        opt_m.step(X, y, alpha = 0.1, beta = 0.9)\n    elif (m_i &gt; i):\n        m_i = i\n    \n    # Terminating condition\n    if (loss_v &lt; 0.3) & (loss_m &lt; 0.3):\n        at_tol = True\n    \n    # Displaying gradient descent progress for the first 5 iterations\n    if (i % 20 == 0) & (i &lt; 100):\n        print(f\"Iteration {i}:\")\n        print(f\"Current Loss value (Standard Gradient Descent): {round(losses_v[-1], 3)}\")\n        print(f\"Current Loss value (Momentum Gradient Descent): {round(losses_m[-1], 3)}\\n\")\n    i += 1\n\nprint(\"...\\n\")\nprint(f\"Standard Gradient Descent Converges at {v_i} Iterations\")\nprint(f\"Momentum Gradient Descent Converges at {m_i} Iterations\")\n\n\nIteration 0:\nCurrent Loss value (Standard Gradient Descent): 0.749\nCurrent Loss value (Momentum Gradient Descent): 0.982\n\nIteration 20:\nCurrent Loss value (Standard Gradient Descent): 0.651\nCurrent Loss value (Momentum Gradient Descent): 0.437\n\nIteration 40:\nCurrent Loss value (Standard Gradient Descent): 0.584\nCurrent Loss value (Momentum Gradient Descent): 0.311\n\nIteration 60:\nCurrent Loss value (Standard Gradient Descent): 0.535\nCurrent Loss value (Momentum Gradient Descent): 0.301\n\nIteration 80:\nCurrent Loss value (Standard Gradient Descent): 0.498\nCurrent Loss value (Momentum Gradient Descent): 0.301\n\n...\n\nStandard Gradient Descent Converges at 576 Iterations\nMomentum Gradient Descent Converges at 47 Iterations\n\n\nCode above initializes LogisticRegression and GradientDescentOptimizer objects for both standard gradient descent and gradient descent with momentum optimizers. The output above shows convergence progress every \\(20\\) iterations and depicts the number of optimization iterations needed for each gradient descent process to achieve a loss value \\(&lt; 0.3\\) (some code provided by Prof. Chodrow).\nIn this experiment, I compare the rate of convergence between the standard gradient descent and gradient descent with momentum procedures. For this experiment, I loosely define “convergence” to be when the optimized model’s empirical loss value is \\(&lt; 0.3\\). Note that for this analysis, I have set the hyperparameters to \\(\\alpha_s, \\alpha_m = 0.1\\) for both optimizers and \\(\\beta_s = 0\\) (standard gradient descent), \\(\\beta_m = 0.9\\) (gradient descent with momentum). As presented in the output above, the gradient descent with momentum optimizer achieves a loss value \\(&lt; 0.3\\) in significantly fewer iterations than that of the standard gradient descent optimizer. That is, the gradient descent with momentum process appears to converge in about \\(\\frac{1}{12}\\) of the iterations needed for the standard gradient descent process to converge.\n\n\nCode\n# Comparing the convergence rates of the two gradient descent methods\nloss_plot(losses_v, losses_m)\n\n\n\n\n\n\n\n\n\nCode above plots the convergence rates of the empirical loss values for both the standard gradient descent and gradient descent with momentum procedures.\nFigure 5\nThis figure above is a visual accompaniment to the output from above. In line with the aforementioned output, the gradient descent with momentum optimizer clearly converges to the tolerance (\\(&lt; 0.3\\)) empirical loss value rapidly quicker (at \\(47\\) iterations) than the standard gradient descent optimizer (at \\(576\\) iterations).\n\n\n\nTo observe an instance of my logistic regression implementation leading to overfitting, I have generated two new sets of binary classification data. The two new data sets are identically designed, where the number of features per data point exceeds the number of data points. One dataset will be used as the “training data” and the other the “testing data”.\n\n\nCode\n# Generating binary classification data for overfitting - code provided by Prof. Chodrow\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n\nCode above generates the “training” and “testing” data sets for the overfitting experiment (some code provided by Prof. Chodrow).\n\n\nCode\n# Investigating overfitting as the number of dimensions in the data grows past the number of data points\nLR  = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\n# Current accuracy variable\naccs = []\ntst_accs = []\ncurr_acc = 0\nj = 0\nwhile (curr_acc &lt; 1.0):\n    \n    # Calculating the current model accuracy\n    curr_acc = acc(LR, X_train, y_train)\n    tst_acc = acc(LR, X_test, y_test)\n    accs.append(curr_acc)\n    tst_accs.append(tst_acc)\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9) # Using gradient descent with momentum\n    \n    # Iteration counter\n    j+= 1\n\nprint(f\"Model Achieved 100.0% Accuracy on Training Data in {j} Iterations (Gradient Descent w/ Momentum)\")\nprint(f\"Model Accuracy on Testing Data: {round(tst_accs[-1], 4) * 100}%\")\n\n\nModel Achieved 100.0% Accuracy on Training Data in 14 Iterations (Gradient Descent w/ Momentum)\nModel Accuracy on Testing Data: 86.0%\n\n\nCode above conducts a model overfitting experiment by first setting the number of data features to be greater than the number of data points, then fitting a model using LogisticRegression and GradientDescentOptimizer objects, and finally comparing the model’s training-data performance to its testing-data performance.\nAs indicated by the output above, the logistic regression model is able to achieve maximum classification accuracy on the training data in a fairly small number of iterations. However, when evaluated on the unseen testing data, the model’s accuracy drops significantly to about \\(86\\%\\). This is clear sign of overfitting as the model’s flawless performance on the training data does not translate to its performance on the testing data.\n\n\nCode\n# Plotting the model's training and testing accuracies as training accuracy approaches 100%\nacc_plot(accs, tst_accs)\n\n\n\n\n\n\n\n\n\nCode above plots the model’s training and testing accuracies at each iteration of optimization during the overfitting experiment.\nFigure 6\nAs shown in the above figure, the model’s training accuracy climbs all the way to \\(100\\%\\) while its testing accuracy plateaus at around \\(86\\%\\) and does not increase further (in \\(14\\) iterations). This figure stands as a visual representation of how the model is overfitting to the training data and consequently sacrificing performance on the unseen testing data.\n\n\n\nThe final experiment in this brief study involves evaluating the performance of my logistic regression implementation on real-world data. For this experiment, I will be using a public data set published to kaggle.com by Aleksei Zagorskii. This dataset contains user profile data from the popular Russian social media and networking platform VKontakte (VK.com) and is used to classify users as either real-human users or bots. The data features a combination of numerical and categorical information extracted from public VK.com user profiles.\n\n\nCode\n# Reading-in and processing the data to evaluate the model with\ndf = pd.read_csv(\"./bots_vs_users.csv\")\n\n# Selecting a subset of the features to use - selecting only the features with float64 data types\ncols = []\nfor col in df.columns:\n    if (df[col].dtype == \"float64\"):\n        cols.append(col)\ncols.append(\"target\")\ndf_pruned = df[cols].copy()\ndf_pruned = df_pruned.dropna() # Dropping any rows with missing values\n\n# Converting the dataframe into tch.tensors\nX = tch.tensor(df_pruned[cols[:-1]].values).float()\ny = tch.tensor(df_pruned[cols[-1]].values).float()\n\n# Normalizing the data through standardization to ensure effective and expected behavior from logistic regression\nmean = tch.mean(X, dim = 0, keepdim = True)\nstd = tch.std(X, dim = 0, keepdim = True)\nX_s = (X - mean) / std\n\n# Splitting the data into training (60%) and testing/validation (40%) data\nX_train, X_tmp, y_train, y_tmp = train_test_split(X_s, y, test_size = 0.4, random_state = 1, stratify = y)\n\n# Splitting the testing/validation data into testing (50%, 20% overall) and validation (40%, 20% overall) data\nX_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size = 0.5, random_state = 1, stratify = y_tmp)\n\n\nCode above imports the dataset used for this experiment, preprocesses it by extracting only the columns with a float64 data type, converts the data into tch.tensors for the feature matrix \\(\\mathbf{X}\\) and the target vector \\(\\mathbf{y}\\), and standardizes the data to ensure expected logistic regression behavior and avoid machine-precision error. Finally, I conduct a test-train-split procedure to split the data into \\(60\\%\\) for training, \\(20\\%\\) for validation, and \\(20\\%\\) for testing.\nOriginally, the full dataset contained \\(60\\) features, many of which have missing values (NaN, or \"Unknown\"). To avoid problems converting the missing data into a form that is both interpretable by my logistic regression model and will not distort the classification performance, I have chosen to take a subset of the original columns present in the data. For the purposes of this experiment, I have chosen to select only the columns with a float64 data type. The pruned dataset I used for this experiment has \\(15\\) columns (including the target column):\n\nposts_count\navg_likes\nlinks_ratio\nhashtags_ratio\navg_keywords\navg_text_length\nattachments_ratio\navg_comments\nreposts_ratio\nads_ratio\navg_views\nposting_frequency_days\nphone_numbers_ratio\navg_text_uniqueness\ntarget\n\nBefore fitting my logistic regression models, the data is first standardized (using \\(\\frac{\\mathbf{x} - \\mu}{\\sigma}\\) where \\(\\mu, \\sigma\\) are the mean and STD. of the data). The purpose of this is to ensure numerical stability and optimization symmetry to support my logistic regression model’s ability to converge to an appropriate weights vector \\(\\mathbf{w}\\) and minimize the empirical loss function. Finally, the data is split into training (\\(60\\%\\) of original), validation (\\(20\\%\\) of original), and testing (\\(20\\%\\) of original) subsets.\n\n\nCode\n# Evlauating my logistic repression implementation on binary classification on the real-world data\n## Initializing the models for standard gradient descent and gradient descent with momentum\nLR_v = LogisticRegression()\nopt_v = GradientDescentOptimizer(LR_v)\nLR_m = LogisticRegression()\nopt_m = GradientDescentOptimizer(LR_m)\n\n# Bookkeeping arrays\nit = 100\nlosses_v_tr = []\nlosses_m_tr = []\nlosses_v_val = []\nlosses_m_val = []\naccs_v_tr = []\naccs_m_tr = []\naccs_v_val = []\naccs_m_val = []\nfor i in range(it):\n    \n    # Recording current loss and accuracy values on the training and validation data for standard and momentum gradient descent\n    losses_v_tr.append(LR_v.loss(X_train, y_train))\n    losses_m_tr.append(LR_m.loss(X_train, y_train))\n    losses_v_val.append(LR_v.loss(X_val, y_val))\n    losses_m_val.append(LR_m.loss(X_val, y_val))\n    accs_v_tr.append(acc(LR_v, X_train, y_train))\n    accs_m_tr.append(acc(LR_m, X_train, y_train))\n    accs_v_val.append(acc(LR_v, X_val, y_val))\n    accs_m_val.append(acc(LR_m, X_val, y_val))\n    opt_v.step(X_train, y_train, alpha = 0.1, beta = 0)\n    opt_m.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n    if (True):\n        if (i % 25 == 0) & ((i &gt; 0) & (i &lt; 76)):\n            print(\"--------------\")\n            print(f\"Iteration: {i}\\n\")\n            print(\"Training Performance:\")\n            print(f\"Current Loss value (Standard Gradient Descent): {round(losses_v_tr[-1], 3)}\")\n            print(f\"Current Loss value (Momentum Gradient Descent): {round(losses_m_tr[-1], 3)}\")\n            print(f\"Current Accuracy (Standard Gradient Descent): {(round(accs_v_tr[-1], 4) * 100):.2f}%\")\n            print(f\"Current Accuracy (Momentum Gradient Descent): {(round(accs_m_tr[-1], 4) * 100):.2f}%\\n\")\n            print(\"Validation Performance:\")\n            print(f\"Current Loss value (Standard Gradient Descent): {round(losses_v_val[-1], 3)}\")\n            print(f\"Current Loss value (Momentum Gradient Descent): {round(losses_m_val[-1], 3)}\")\n            print(f\"Current Accuracy (Standard Gradient Descent): {(round(accs_v_val[-1], 4) * 100):.2f}%\")\n            print(f\"Current Accuracy (Momentum Gradient Descent): {(round(accs_m_val[-1], 4) * 100):.2f}%\\n\")\nprint(\"...\")\nprint(f\"Final Iteration: {it}\")\nprint(\"Final Training Performance:\")\nprint(f\"Loss value (Standard Gradient Descent): {round(losses_v_tr[-1], 3)}\")\nprint(f\"Loss value (Momentum Gradient Descent): {round(losses_m_tr[-1], 3)}\")\nprint(f\"Accuracy (Standard Gradient Descent): {(round(accs_v_tr[-1], 4) * 100):.2f}%\")\nprint(f\"Accuracy (Momentum Gradient Descent): {(round(accs_m_tr[-1], 4) * 100):.2f}%\\n\")\nprint(\"Final Validation Performance:\")\nprint(f\"Loss value (Standard Gradient Descent): {round(losses_v_val[-1], 3)}\")\nprint(f\"Loss value (Momentum Gradient Descent): {round(losses_m_val[-1], 3)}\")\nprint(f\"Accuracy (Standard Gradient Descent): {(round(accs_v_val[-1], 4) * 100):.2f}%\")\nprint(f\"Accuracy (Momentum Gradient Descent): {(round(accs_m_val[-1], 4) * 100):.2f}%\\n\")\n\n\n--------------\nIteration: 25\n\nTraining Performance:\nCurrent Loss value (Standard Gradient Descent): 0.905\nCurrent Loss value (Momentum Gradient Descent): 0.622\nCurrent Accuracy (Standard Gradient Descent): 79.62%\nCurrent Accuracy (Momentum Gradient Descent): 85.37%\n\nValidation Performance:\nCurrent Loss value (Standard Gradient Descent): 0.998\nCurrent Loss value (Momentum Gradient Descent): 0.762\nCurrent Accuracy (Standard Gradient Descent): 78.42%\nCurrent Accuracy (Momentum Gradient Descent): 82.37%\n\n--------------\nIteration: 50\n\nTraining Performance:\nCurrent Loss value (Standard Gradient Descent): 0.74\nCurrent Loss value (Momentum Gradient Descent): 0.592\nCurrent Accuracy (Standard Gradient Descent): 84.53%\nCurrent Accuracy (Momentum Gradient Descent): 88.01%\n\nValidation Performance:\nCurrent Loss value (Standard Gradient Descent): 0.823\nCurrent Loss value (Momentum Gradient Descent): 0.73\nCurrent Accuracy (Standard Gradient Descent): 82.37%\nCurrent Accuracy (Momentum Gradient Descent): 85.61%\n\n--------------\nIteration: 75\n\nTraining Performance:\nCurrent Loss value (Standard Gradient Descent): 0.672\nCurrent Loss value (Momentum Gradient Descent): 0.587\nCurrent Accuracy (Standard Gradient Descent): 85.97%\nCurrent Accuracy (Momentum Gradient Descent): 87.89%\n\nValidation Performance:\nCurrent Loss value (Standard Gradient Descent): 0.752\nCurrent Loss value (Momentum Gradient Descent): 0.715\nCurrent Accuracy (Standard Gradient Descent): 83.45%\nCurrent Accuracy (Momentum Gradient Descent): 85.97%\n\n...\nFinal Iteration: 100\nFinal Training Performance:\nLoss value (Standard Gradient Descent): 0.644\nLoss value (Momentum Gradient Descent): 0.584\nAccuracy (Standard Gradient Descent): 86.93%\nAccuracy (Momentum Gradient Descent): 88.01%\n\nFinal Validation Performance:\nLoss value (Standard Gradient Descent): 0.728\nLoss value (Momentum Gradient Descent): 0.707\nAccuracy (Standard Gradient Descent): 84.53%\nAccuracy (Momentum Gradient Descent): 85.97%\n\n\n\nCode above fits my logistic regression model to the real-world training data and records the classification accuracy on the real-world validation data during model training.\nTo evaluate my implementation of logistic regression on the real-world data, I fit two logistic regression models to the training data: One model is optimized with standard gradient descent and the other uses gradient descent with momentum. As displayed by the output above, the empirical losses for each model decrease over the iterations of gradient descent on both the training and validation data. Along these lines, the corresponding accuracies of each model gradually increase, which is expected.\n\n\n\n\nCode\n# Plotting the losses for training data\nloss_plot(losses_v_tr, losses_m_tr)\n\n\n\n\n\n\n\n\n\nCode above plots the loss value evolution for standard and momentum gradient descent on the training data.\nFigure 7 (7a above & 7b below)\nAbove shows the loss value evolution of the standard and momentum gradient descent models optimizing over the training data. As expected the loss values decrease over the optimization iterations for both models, but the model using momentum gradient descent minimizes the loss much faster. Below are the changing model accuracies for the training data. Inline with the plot above, the accuracies for each model improve during the optimization process, but the model using momentum gradient descent is able to achieve a higher accuracy faster than the model using standard gradient descent.\n\n\nCode\n# Plotting the accuracies for training data\nacc_plot(accs_v_tr, accs_m_tr)\nplt.legend([\"Standard\", \"Momentum\"], frameon = True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above plots the accuracies for standard and momentum gradient descent on the training data.\n\n\n\n\n\nCode\n# Plotting the losses for training data\nloss_plot(losses_v_val, losses_m_val)\n\n\n\n\n\n\n\n\n\nCode above plots the loss value evolution for standard and momentum gradient descent on the validation data.\nFigure 8 (8a above & 8b below)\nAbove shows the loss value evolution of the standard and momentum gradient descent models now optimizing over the validation data. Again as expected the loss values decrease over the optimization iterations for both models, and the model using momentum gradient descent continues to minimize the loss much faster. Below shows the changing model accuracies now for the validation data. Further aligned with the plot above, the accuracies for each model still improve during the optimization process, and the model using momentum gradient descent is again able to achieve a higher accuracy faster than the model using standard gradient descent.\n\n\nCode\n# Plotting the accuracies for training data\nacc_plot(accs_v_val, accs_m_val)\nplt.legend([\"Standard\", \"Momentum\"], frameon = True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above plots the accuracies for standard and momentum gradient descent on the validation data.\n\n\n\n\n\nCode\n# Evaluating the loss and accuracy of each model on the testing data\nloss_v_tst = LR_v.loss(X_test, y_test)\nloss_m_tst = LR_m.loss(X_test, y_test)\nacc_v_tst = acc(LR_v, X_test, y_test)\nacc_m_tst = acc(LR_m, X_test, y_test)\n\nprint(\"Testing Data Empirical Loss:\")\nprint(f\"Model 1 (Standard Gradient Descent) | {round(loss_v_tst, 3)}\")\nprint(f\"Model 2 (Momentum Gradient Descent) | {round(loss_m_tst, 3)}\")\nprint(\"-----\\n\")\nprint(\"Testing Data Accuracy:\")\nprint(f\"Model 1 (Standard Gradient Descent) | {(round(acc_v_tst, 4) * 100):.2f}%\")\nprint(f\"Model 2 (Momentum Gradient Descent) | {(round(acc_m_tst, 4) * 100):.2f}%\")\n\n\nTesting Data Empirical Loss:\nModel 1 (Standard Gradient Descent) | 0.622\nModel 2 (Momentum Gradient Descent) | 0.538\n-----\n\nTesting Data Accuracy:\nModel 1 (Standard Gradient Descent) | 89.61%\nModel 2 (Momentum Gradient Descent) | 91.04%\n\n\nCode above computes the empirical loss and model accuracy on the testing data for each logistic regression model.\nFinally, on the unseen testing data, the model using gradient descent with momentum yields both a lower loss value (\\(0.538\\)) and higher accuracy (\\(91.04\\%\\)) than those of the model using standard gradient descent (\\(0.622\\), \\(89.61\\%\\)). Interestingly, both models yield smaller loss values and achieve higher accuracies from the unseen testing data than the training or validation data."
  },
  {
    "objectID": "posts/post_5/index.html#references",
    "href": "posts/post_5/index.html#references",
    "title": "Post 5 - Implementing Logistic Regression",
    "section": "References",
    "text": "References\nZagorskii Aleksei. (2025). Users vs bots classification [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/6999922"
  },
  {
    "objectID": "posts/post_3/index.html",
    "href": "posts/post_3/index.html",
    "title": "Post 3 - Auditing Bias in Machine Learning Models",
    "section": "",
    "text": "In this analysis, I explore the construction of a predictive classification model and subsequently examine the model’s properties and behavior to audit for bias. Specifically, the model I constructed aims to predict the employment status of a given individual. Following the refining process, the behavior, accuracy, and error rates of the model are investigated and compared across each racial group outlined in the data. The purpose of this is to uncover any biases on the basis of race the model may be exercising when making predictions for certain individuals. While the model is blind to racial identity during training, this does not eliminate the possibility that systemic racial biases are reflected in the model’s predictive behavior. To audit for potential bias, the model is checked for three primary “fairness” criteria: calibration, error rate balance, and statistical parity. In the search for bias, it is found that the refined model is relatively well-calibrated across each of the racial groups present in the data, but the model generally fails to satisfy the error-balance and statistical parity conditions of accepted “fairness”. Ultimately, the model is shown to exhibit “unfair” (i.e. inconsistent) predictive behavior pertaining to the racial identity of individuals in the data. The results of this introductory study highlight the crucial role of diligently investigating the presence of group-based bias and “unfairness” exercised by classification models even when such models are blind to the group identity. Considering these findings in the context of employment prediction, this study serves as an example of how systemic biases perpetuated through an automated decision classifier can contribute and compound upon past injustices that have discriminantly affected individuals of certain racial identities more than others.\n\n\n\n\nCode\n# Includuing all additional imports\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom folktables import ACSDataSource, BasicProblem\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.pipeline import make_pipeline\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import SVC\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Downloading ACS PUMS data for the state of Massachusetts - Code provided by Prof. Chodrow\nSTATE = \"MA\"\nds = ACSDataSource(survey_year = \"2018\", \n                            horizon = \"1-Year\", \n                            survey = \"person\")\nma = ds.get_data(states = [STATE], download = True)\n\n\nIncluding all additional imports\nThe data for this study comes from an ACS (American Community Survey) data set. Specifically, the data is from a 2018 PUMS (Public Use Microdata Sample) survey for the state of Massachusetts. Each observation from this data set is an individual massachusetts resident who completed the 2018 PUMS survey. The data is downloaded using the ACSDataSource class from the folktables package.\n\n\n\n\nCode\nvars = [\"AGEP\", \"SCHL\", \"MAR\", \"RELP\", \"DIS\", \"ESP\", \"CIT\", \"MIG\", \"MIL\", \"ANC\", \"NATIVITY\", \"DEAR\", \"DEYE\", \"DREM\", \"SEX\", \"RAC1P\", \"ESR\"]\n\n\nUntouched, the data contains nearly 300 different features. However, for this study, I will only be using a subset of the columns:\n\nAGEP: Age (Range 0 - 99)\nSCHL: Educational Attainment (Range 1 - 24)\nMAR: Marital Status (Range 1 - 5)\nRELP: Relationship to Householder (Range 1 - 17)\nDIS: Disability Present (Binary)\nESP: Employment Status of Parents (Range 1 - 8)\nCIT: Citizenship Status (Range 1 - 5)\nMIG: Mobility Status - Lived in Specified Location 1 Year Ago (Range 1 - 3)\nMIL: Military Service (Range 1 - 5)\nANC: Ancestry (Range 1 - 4, 8)\nNATIVITY: Nativity (Binary)\nDEAR: Hearing Difficulty (Binary)\nDEYE: Vision Difficulty (Binary)\nDREM: Cognitive Difficulty (Binary)\nSEX: Sex (Binary)\nRAC1P: Race (Range 1 - 9)\nESR: Employment Status (Range 1 - 6)\nPINCP: Total Person Income (Integer Range of Income in US Dollars: -19997 - 4209995)\n\n\n\n\n\n\n\n\n\nCode\n# Filtering out employment status (target) and race variables\nvars1 = [v for v in vars if v not in [\"ESR\", \"RAC1P\"]]\n\n# Defining the predictive modeling task - Code provided by Prof. Chodrow\nEmploymentProblem = BasicProblem(\n    features = vars1,\n    target = \"ESR\",\n    target_transform = lambda x: x == 1,\n    group = \"RAC1P\",\n    preprocess = lambda x: x,\n    postprocess = lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = EmploymentProblem.df_to_numpy(ma)\n\n# Test, train split procedure\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features, label, group, test_size = 0.2, random_state = 69)\n\n\nCode above defines the predictive modeling task and constructs the training data for the employment prediction model.\nBefore constructing the first predictive model, the predictive modeling task is defined. Afterwards, a test-train-split procedure is conducted on the data to prepare it for model fitting.\n\n\n\n\n\nCode\n# Observing some general descriptives in the training data\ntrain = pd.DataFrame(X_train, columns = vars1)\ntrain[\"emp_status\"] = y_train.astype(int)\ntrain[\"race\"] = group_train\ntrain.dropna(inplace = True)\n\n# Total number of individuals in the training data\nn = train.shape[0]\n\n# Proportion of employed individuals in the training data\nemp_prop = train[\"emp_status\"].mean()\n\n# Proportion of employed individuals of each race\ntot_emp = (train[\"emp_status\"] == 1).sum()\nrace_prop_emp_tot = (train.groupby(\"race\")[\"emp_status\"].sum() / tot_emp) * 100\n\n# Proportion of employed individuals within each race\nrace_prop_emp = train.groupby(\"race\").aggregate({\"emp_status\": \"mean\"})\nrace_prop_emp[\"u_emp\"] = 1 - race_prop_emp[\"emp_status\"]\n\n# Plotting proportion of employed individuals (total)\n## Creating dataframe for plotting\nemp_prop_tot = pd.DataFrame({\n    \"emp\": ['Unemployed', 'Employed'],\n    \"prop\": [1 - emp_prop, emp_prop]\n})\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nsns.barplot(emp_prop_tot, x = \"prop\", y = \"emp\", hue = \"prop\", palette = [\"#D8BFD8\", \"#5D3A6D\"], legend = False, width = 0.6, ax = ax)\nax.set_title(\"Overall Proportion of Employed Individuals\", fontsize = 16)\nax.set_yticks([0, 1])\nax.set_yticklabels([\"Unemployed\", \"Employed\"], fontsize = 12, rotation = 60)\nax.set_xlim(0.25, 0.55)\nax.set_xticks([i / 10 for i in range(3, 6)])\nax.set_xticklabels([f\"{i * 10}%\" for i in range(3, 6)], fontsize = 12)\nax.set_xlabel(\"Proportion to all Individuals\", fontsize = 14)\nax.set_ylabel(\"Employment Status\", fontsize = 14)\nax.text((1 - emp_prop) + 0.01, 0, f\"{round((1 - emp_prop) * 100, 2)}%\", ha = \"center\", va = \"center\", fontsize = 12, color = \"black\", rotation = 60)\nax.text(emp_prop + 0.01, 1, f\"{round( emp_prop * 100, 2)}%\", ha = \"center\", va = \"center\", fontsize = 12, color = \"black\", rotation = 60)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCode above calculates three proportions of employed individuals in the data: 1. General proportion of employment across all individuals 2. Proportion of each race in all employed individuals 3. Proportion of employed individuals within each race.\nFigure 1\nPrior to constructing and fitting the predictive model, it’s useful to explore some general descriptives of the data. In total, there are 56104 individuals in the training data. Approximately \\(50\\%\\) of individuals from the data are employed (as of their filling out the survey in 2018).\n\n\nCode\n# Plotting proportion of employed individuals across each race\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nsns.barplot(race_prop_emp, x = \"race\", y = \"emp_status\", hue = \"race\", palette = sns.color_palette()[:-1], width = 0.9, ax = ax, legend = False)\nsns.barplot(race_prop_emp, x = \"race\", y = \"u_emp\", hue = \"race\", palette = sns.color_palette()[:-1], width = 0.9, ax = ax, legend = False, alpha = 0.5, edgecolor = \"black\")\n\n# Adding space between bars\nfor patch in ax.patches:\n    patch.set_width(patch.get_width() * 0.8)\nax.set_title(\"Proportion of Employed/Unemployed Individuals Within Each Race Category\", fontsize = 16)\nax.set_xticks(range(9))\nax.set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax.set_xlabel(\"Racial Groups\", fontsize = 14)\nax.set_ylabel(\"Proportion of Employed Individuals\", fontsize = 14)\nax.set_yticks([(0.2 * i) for i in range(6)])\nax.set_yticklabels([f\"{(0.2 * i) * 100: .1f}%\" for i in range(6)], fontsize = 12)\nax.text(1, 0.8, f\"   : Unemployed\\n   : Employed\", ha = \"left\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax.text(1.075, 0.82, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax.text(1.075, 0.78, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCode above plots the proportions calculated above\nFigure 2\nOf the individuals across each racial group:\n\nAbout \\(51.5\\%\\) of white individuals are employed\nAbout \\(46\\%\\) of black or African American individuals are employed\nAbout \\(47\\%\\) of American Indian individuals are employed\nSeemingly all (\\(100\\%\\)) of Alaska Native individuals are employed\n\\(^1\\) About \\(47\\%\\) of individuals identifying as American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, or not specified and of no other races are employed\nAbout \\(50\\%\\) of Asian individuals are employed\nAbout \\(61\\%\\) of Native Hawaiian and Other Pacific Islander individuals are employed\nAbout \\(47.3\\%\\) of individuals identifying as some other race are employed\nAbout \\(38\\%\\) of individuals identifying as two or more races are employed\n\n\n\nCode\n# Race and Sex intersection\nsex_recode = {1.0: \"Male\", 2.0: \"Female\"}\ntrain[\"SEX_rc\"] = train[\"SEX\"].replace(sex_recode)\nixn_data = train.groupby([\"race\", \"SEX_rc\"]).agg({\"emp_status\": \"mean\"})\n\n# Barplot to display the intersection of race and gender in employment status\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nsns.barplot(ixn_data, x = \"race\", y = \"emp_status\", hue = \"SEX_rc\", palette = [\"#FFDAB9\", \"darkorange\"], ax = ax)\nax.set_title(\"Proportion of Employed Individuals for Each Race Category by Sex\", fontsize = 16)\nax.set_ylabel(\"Proportion of Employed Individuals\", fontsize = 14)\nax.set_xlabel(\"Race (Abbreviated Category Names)\", fontsize = 14)\nax.set_xticks(range(0, 9))\nax.set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], \n                   fontsize = 12, rotation = 15)\nax.set_yticks([(0.2 * i) for i in range(6)])\nax.set_yticklabels([f\"{(0.2 * i) * 100: .1f}%\" for i in range(6)], fontsize = 12)\nax.legend(frameon = True, fontsize = 12, title = \"Sex\", title_fontsize = 14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCode above calculates and plots the proportions of employment within each racial group by sex.\n\\(^1\\)“A.I./A.N.Specified”: Individuals who are American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, or not specified and of no other races.\nFigure 3\nThe figure above displays the possible intersectionality between race and sex among employed individuals. For five of the nine race categories (all but American Indian, Alaska Native, Asian, and Native Hawaiian & Other Pacific Islander) found in this data set, there does not appear to be a significant difference (\\(\\leq5\\%\\)) in employment rates between males and females. However, for the remaining racial groups, there does appear to be a noteworthy difference (about \\(\\geq10\\%\\)) in employment rates between males and females. Therefore, based on this figure, there generally does not appear to be a significant intersectional effect of race and sex on employment rates across all racial groups, but for some groups, an intersectional effect may be present. Please note that this finding does not dismiss the presence of intersectional biases and systemic injustices that may impact certain individuals from this data set. The existence of such intersectionality may very well be present in the data and but just not captured by this figure. Observing the intersectionality of race and gender relating to employment may be more clearly observed through other statistical processes.\n\n\n\n\n\nCode\n# Constructing and fitting the model\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier())\nmodel.fit(X_train, y_train)\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier', DecisionTreeClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier', DecisionTreeClassifier())]) StandardScaler?Documentation for StandardScalerStandardScaler() DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n\nCode\n# Refining the model\n## Identifying the optimal max_depth value - Model Complexity parameter for DecisionTreeCLassifier model - though iterative process\n# #Updated variables\nbest_max_depth = None\nbest_avg_score = 0\nfor md in np.random.randint(1, 101, size = 10):\n    model.set_params(decisiontreeclassifier__max_depth = md)\n    avg_score = cross_val_score(model, X_train, y_train, cv = 5).mean()\n    if avg_score &gt; best_avg_score:\n        best_avg_score = avg_score\n        best_max_depth = md\n\n# Fitting the model with the optimal max_depth value\ndtc_refined = model.set_params(decisiontreeclassifier__max_depth = best_max_depth)\ndtc_refined.fit(X_train, y_train)\ngen_score = dtc_refined.score(X_train, y_train)\nprint(f\"Refined Model Overall Accuracy: {gen_score * 100: .3f}%\")\n\n\nRefined Model Overall Accuracy:  81.475%\n\n\nCode above uses the make_pipeline method from the sklearn package, the training data is first standardized and then used to fit a DecisionTreeClassifier model (also from sklearn). The max_depth parameter for the model is tuned iteratively with corss-validation to set the tree depth that maximizes overall accuracy while mitigating model over-fitting.\nTo refine the model, an iterative process tuning the model complexity (using the max_depth parameter) with cross-validation is conducted. This process aims at model maximizing trining accuracy while reducing the presence of model over-fitting. Following this tuning procedure, the model has an overall accuracy of approximately \\(83.6\\%\\) – as an initial metric, this indicates a considerable accurate model.\n\n\n\n\n\nCode\n# Calculating various accuracy metrics\n## Overall Test data accuracy\nacc_t = dtc_refined.score(X_test, y_test)\ny_preds_t = dtc_refined.predict(X_test).astype(int)\nacc_t = (y_preds_t == y_test).mean()\n\n# Confusion matrix for refined model\nC = confusion_matrix(y_test, y_preds_t)\nemp_status = [\"Unemployed\", \"Employed\"]\n\n# Creating a heatmap for better confusion matrix visualization\nfig, ax = plt.subplots(1, 1, figsize = (5, 5))\nsns.heatmap(C, annot = True, annot_kws = {\"size\": 12}, fmt = \"d\", cmap = \"Reds\", cbar = False, ax = ax)\n\n# Plot styling\nax.set_xlabel(\"Predicted Employment Status\", fontsize = 14)\nax.set_ylabel(\"True Employment Status\", fontsize = 14)\nax.set_xticklabels(emp_status, fontsize = 12)\nax.set_yticklabels(emp_status, fontsize = 12)\nax.set_title(\"Employment Status Classification Confusion Matrix\", fontsize = 16)\n\nplt.show()\n\n# Printing confusion matrix results\nprint(f\"There were {C[0, 0]} unemployed individuals that were predicted to be unemployed.\")\nprint(f\"There were {C[0, 1]} unemployed individuals that were predicted to be employed.\")\nprint(f\"There were {C[1, 0]} employed individuals that were predicted to be unemployed.\")\nprint(f\"There were {C[1, 1]} unemployed individuals that were predicted to be employed.\")\n\n# Calculating various accuracies and error rates\ng_fnr = C[1, 0] / (C[1, 0] + C[1, 1])\ng_tnr = C[0, 0] / (C[0, 0] + C[0, 1])\ng_fpr = 1 - g_tnr\ng_tpr = 1 - g_fnr\ng_ppv = C[1, 1] / (C[1, 1] + C[0, 1])\ng_npv = C[0, 0] / (C[0, 0] + C[1, 0])\n\nprint(f\"Overall Testing Accuracy: {acc_t * 100: .3f}%\")\nprint(f\"General False Negative Rate: {g_fnr * 100: .3f}%\")\nprint(f\"General True Negative Rate: {g_tnr * 100: .3f}%\")\nprint(f\"General False Positive Rate: {g_fpr * 100: .3f}%\")\nprint(f\"General True Positive Rate: {g_tpr * 100: .3f}%\")\nprint(f\"General Positive Predictive Value: {g_ppv * 100: .3f}%\")\nprint(f\"General Negative Predictive Value: {g_npv * 100: .3f}%\")\n\n\n\n\n\n\n\n\n\nThere were 5690 unemployed individuals that were predicted to be unemployed.\nThere were 1284 unemployed individuals that were predicted to be employed.\nThere were 1271 employed individuals that were predicted to be unemployed.\nThere were 5782 unemployed individuals that were predicted to be employed.\nOverall Testing Accuracy:  81.785%\nGeneral False Negative Rate:  18.021%\nGeneral True Negative Rate:  81.589%\nGeneral False Positive Rate:  18.411%\nGeneral True Positive Rate:  81.979%\nGeneral Positive Predictive Value:  81.828%\nGeneral Negative Predictive Value:  81.741%\n\n\nCode above constructs the general confusion matrix for the refined model and calculates the typical accuracy/error rates: FPR, FNR, TPR, TNR, PPV, and NPV\nFigure 4\nSome key overall accuracy and error rates of the refined model are:\n\nOverall Test Accuracy: About \\(83\\%\\)\nFNR: About \\(13\\%\\) of truly employed individuals were misclassified as unemployed.\nTNR: About \\(79\\%\\) of truly unemployed individuals were correctly classified as unemployed.\nFPR: About \\(21\\%\\) of truly unemployed individuals were misclassified as employed.\nTPR: About \\(87\\%\\) of truly employed individuals were correctly classified as employed\nPPV: About \\(81\\%\\) of “employed” model predictions are correct while only about \\(20\\%\\) are incorrect.\nNPV: About \\(86\\%\\) of “unemployed” model predictions are correct while only about \\(14\\%\\) are incorrect.\n\nThe general accuracy and error rates outlined above indicate that the refined model is approximately similarly but not equally accurate in predicting that truly unemployed individuals are unemployed versus predicting that truly employed individuals are employed. Additionally, the refined misclassifies truly unemployed individuals and truly employed individuals at similar but notably unequal rates. The refined model yields a PPV and NPV that are close in value (with a difference of \\(&lt;7\\%\\)). This suggests that, for any given individual, the refined model is generally similarly good at predicting if this individual is employed as it is at predicting if this individual is unemployed.\nWhile the overall accuracy of the refined model is appears decently high (\\(83\\%\\)), it is crucial to investigate the same accuracy rates across different groups of observations found in the data. In this context, the prediction task is to identify if an individual is employed or not. In the data, the race of each individual is provided, and not only is race undoubtedly related to employment in the US but race is also a factor by which systemic biases and systematic discrimination occurs in the American workforce. Although the model was not trained using this variable (race), there is no guarantee that the model does not rely on variables highly related to race or that could stand as proxies for race. Thus, it is critically important to examine if the model perpetuates systemic biases and past injustices effecting certain individuals differently on the bases of racial identity.\n\n\n\n\n\n\n\n\nCode\n# Adding a column for indicating correct predictions\ntest = pd.DataFrame(X_test, columns = vars1)\ntest[\"emp_status\"] = y_test\ntest[\"pred\"] = y_preds_t\ntest[\"correct_pred\"] = (y_preds_t == y_test).astype(int)\n\n# Recoding race variable for easier visualization\nrace_recode = {\n    1.0: \"White\",\n    2.0: \"Black or African American\",\n    3.0: \"American Indian\",\n    4.0: \"Alaska Native\",\n    5.0: f\"A.I./A.N. Tribe Specified\\u00B9\",\n    6.0: \"Asian\",\n    7.0: \" Native Hawaiian and Other Pacific Islander\",\n    8.0: \"Some Other Race\",\n    9.0: \" Two or More Races\"\n}\ntest[\"race\"] = group_test\ntest[\"Racial Group (Abbreviated Categories)\"] = test[\"race\"].map(race_recode)\n\n# Helper functions to calculate various accuracy and error rate metrics\ndef acc(x):\n    if (x.shape[0] == 0):\n        return \"NA\"\n    \n    return str(round(x[\"correct_pred\"].mean() * 100, 1)) + \" %\"\n\ndef prop(x):\n    return str(round((x.shape[0] / train.shape[0]) * 100, 1)) + \" %\"\n\ndef fpr(x):\n    fp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 0)).sum()\n    tn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 0)).sum()\n    if ((fp + tn) == 0):\n        return \"NA\"\n    fpr = fp / (fp + tn)\n    \n    return str(round(fpr * 100, 1)) + \" %\"\n\ndef fnr(x):\n    fn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 1)).sum()\n    tp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 1)).sum()\n    if ((fn + tp) == 0):\n        return \"NA\"\n    fnr = fn / (fn + tp)\n\n    return str(round(fnr * 100, 1)) + \" %\"\n\ndef tpr(x):\n    fn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 1)).sum()\n    tp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 1)).sum()\n    if ((fn + tp) == 0):\n        return \"NA\"\n    tpr = tp / (fn + tp)\n\n    return str(round(tpr * 100, 1)) + \" %\"\n\ndef tnr(x):\n    fp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 0)).sum()\n    tn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 0)).sum()\n    if ((fp + tn) == 0):\n        return \"NA\"\n    tnr = tn / (fp + tn)\n    \n    return str(round(tnr * 100, 1)) + \" %\"\n\ndef ppv(x):\n    tp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 1)).sum()\n    fp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 0)).sum()\n    if ((tp + fp) == 0):\n        return \"NA\"\n    ppv = tp / (tp + fp)\n    \n    return str(round(ppv * 100, 1)) + \" %\"\n\ndef npv(x):\n    tn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 0)).sum()\n    fn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 1)).sum()\n    if ((tn + fn) == 0):\n        return \"NA\"\n    npv = tn / (tn + fn)\n\n    return str(round(npv * 100, 1)) + \" %\"\n\n# Initializing data frame to visualize model accuracy and error rates for each race\nrace_acc = pd.DataFrame(index = range(1, 10), columns = [\"Racial Group (Abbr. Cats.)\", \"Model Accuracy\", \"Proportion to Tot. (Aprx.)\", \"FPR\", \"FNR\", \"TPR\", \"TNR\", \"PPV\", \"NPV\"])\n\n# Subsetting training data into each race category\nr1 = test[test[\"race\"] == 1.0]\nr2 = test[test[\"race\"] == 2.0]\nr3 = test[test[\"race\"] == 3.0]\nr4 = test[test[\"race\"] == 4.0]\nr5 = test[test[\"race\"] == 5.0]\nr6 = test[test[\"race\"] == 6.0]\nr7 = test[test[\"race\"] == 7.0]\nr8 = test[test[\"race\"] == 8.0]\nr9 = test[test[\"race\"] == 9.0]\n\n# Populating the accuracy/error rates table\nrace_acc.loc[1] = [race_recode.get(1.0), acc(r1), prop(r1), fpr(r1), fnr(r1), tpr(r1), tnr(r1), ppv(r1), npv(r1)]\nrace_acc.loc[3] = [race_recode.get(2.0), acc(r2), prop(r2), fpr(r2), fnr(r2), tpr(r2), tnr(r2), ppv(r2), npv(r2)]\nrace_acc.loc[2] = [race_recode.get(3.0), acc(r3), prop(r3), fpr(r3), fnr(r3), tpr(r3), tnr(r3), ppv(r3), npv(r3)]\nrace_acc.loc[4] = [race_recode.get(4.0), acc(r4), prop(r4), fpr(r4), fnr(r4), tpr(r4), tnr(r4), ppv(r4), npv(r4)]\nrace_acc.loc[5] = [race_recode.get(5.0), acc(r5), prop(r5), fpr(r5), fnr(r5), tpr(r5), tnr(r5), ppv(r5), npv(r5)]\nrace_acc.loc[6] = [race_recode.get(6.0), acc(r6), prop(r6), fpr(r6), fnr(r6), tpr(r6), tnr(r6), ppv(r6), npv(r6)]\nrace_acc.loc[7] = [race_recode.get(7.0), acc(r7), prop(r7), fpr(r7), fnr(r7), tpr(r7), tnr(r7), ppv(r7), npv(r7)]\nrace_acc.loc[8] = [race_recode.get(8.0), acc(r8), prop(r8), fpr(r8), fnr(r8), tpr(r8), tnr(r8), ppv(r8), npv(r8)]\nrace_acc.loc[9] = [race_recode.get(9.0), acc(r9), prop(r9), fpr(r9), fnr(r9), tpr(r9), tnr(r9), ppv(r9), npv(r9)]\nrace_acc\n\n\n\n\n\n\n\n\n\nRacial Group (Abbr. Cats.)\nModel Accuracy\nProportion to Tot. (Aprx.)\nFPR\nFNR\nTPR\nTNR\nPPV\nNPV\n\n\n\n\n1\nWhite\n81.9 %\n20.2 %\n18.2 %\n17.9 %\n82.1 %\n81.8 %\n82.4 %\n81.5 %\n\n\n2\nAmerican Indian\n88.2 %\n0.0 %\n11.1 %\n12.5 %\n87.5 %\n88.9 %\n87.5 %\n88.9 %\n\n\n3\nBlack or African American\n84.4 %\n1.5 %\n13.8 %\n17.7 %\n82.3 %\n86.2 %\n84.0 %\n84.6 %\n\n\n4\nAlaska Native\nNA\n0.0 %\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n5\nA.I./A.N. Tribe Specified¹\n80.0 %\n0.0 %\n0.0 %\n33.3 %\n66.7 %\n100.0 %\n100.0 %\n66.7 %\n\n\n6\nAsian\n79.0 %\n1.7 %\n28.2 %\n14.2 %\n85.8 %\n71.8 %\n76.4 %\n82.7 %\n\n\n7\nNative Hawaiian and Other Pacific Islander\n77.8 %\n0.0 %\n33.3 %\n16.7 %\n83.3 %\n66.7 %\n83.3 %\n66.7 %\n\n\n8\nSome Other Race\n79.8 %\n0.8 %\n16.8 %\n24.1 %\n75.9 %\n83.2 %\n79.8 %\n79.8 %\n\n\n9\nTwo or More Races\n80.7 %\n0.7 %\n14.0 %\n26.8 %\n73.2 %\n86.0 %\n78.3 %\n82.2 %\n\n\n\n\n\n\n\nCode above subsets the training data into each race category and creates a table displaying the model’s accuracy and error rates in employment/unemployment predictions for each racial group.\n\\(^1\\)“A.I./A.N.Specified”: Individuals who are American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, or not specified and of no other races.\nTable 1\nThe table above depicts the refined model’s accuracy and error rates for employment prediction of individuals across each racial group in the training data. The general accuracies for each racial group are all largely similar – with values ranging from around \\(80\\%\\) to \\(89\\%\\) accuracy.\n\\[FPR = \\frac{FP}{FP + TN}\\] \\[FNR = \\frac{FN}{FN + TP}\\]\nThe FPR and FNR for each race are slightly less equal across all racial groups (FPR range from roughly \\(0\\%\\) to \\(33\\%\\) and FNR range also from \\(0\\%\\) to \\(33\\%\\)), but these error rates do not differ dramatically.\n\\[TPR = \\frac{TP}{TP + FN}\\] \\[TNR = \\frac{TN}{TN + FP}\\]\nA similar trend is generally observable for TPR across each racial group (majority TPR range from about \\(83\\%\\) to \\(100\\%\\)) - with the exception of the “American Indian and Alaska Native tribes specified” group with a notably lower TPR: \\(67\\%\\)). As for TNR, there is a larger spread in values across all racial groups with a general TNR range of \\(67\\%\\) to \\(100\\%\\).\n\\[PPV = \\frac{TP}{TP + FP}\\] \\[NPV = \\frac{TN}{TN + FN}\\]\nFurther, the PPV and NPV for each racial group are primarily similar and display a smaller spread of values – with PPV metrics ranging from approximately \\(77\\%\\) to \\(100\\%\\) and NPV values ranging from approximately \\(67\\%\\) to \\(100\\%\\).\n\n\nCode\n# Changing column types for easier visualization\n## Saving a data frame for late use\nerb = race_acc.copy()\nfor col in race_acc.columns[1:]:\n    if (race_acc[col].dtype == \"object\"):\n        race_acc[col] = pd.to_numeric(race_acc[col].str.replace(\"%\", \"\").replace(\"NA\", np.nan))\n\n# Plotting the information described in the table above for easier visualization\nfig, ax = plt.subplots(1, 2, figsize = (20, 10))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Accuracy by Race\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"Model Accuracy\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], ax = ax[0])\nax[0].axhline(gen_score * 100, color = \"black\", linestyle = \"--\")\nax[0].set_ylim(40, 95)\nax[0].set_title(\"Model Accuracy\", fontsize = 16)\nax[0].set_xlabel(\"\")\nax[0].set_ylabel(\"Model Accuracy\", fontsize = 14)\nax[0].set_xticks(range(9))\nax[0].set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax[0].set_yticks([(10 * i) for i in range(4, 10)])\nax[0].set_yticklabels([f\"{10 * i}%\" for i in range(4, 10)], fontsize = 12)\nax[0].text(6.15, 92, f\"\\u2013 Overall Model Accuracy: {gen_score * 100: .2f}%\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\n\n# FPR & FNR by Race\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"FPR\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], ax = ax[1])\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"FNR\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], alpha = 0.5, ax = ax[1], edgecolor = \"black\")\nax[1].axhline(g_fpr * 100, color = \"purple\", linestyle = \"--\")\nax[1].axhline(g_fnr * 100, color = \"black\", linestyle = \"--\")\nax[1].set_title(\"FPR & FNR\", fontsize = 16)\nax[1].set_xlabel(\"\")\nax[1].set_ylabel(\"FPR & FNR\", fontsize = 14)\nax[1].set_xticks(range(9))\nax[1].set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax[1].set_yticks([i for i in range(0, 35, 5)])\nax[1].set_yticklabels([f\"{i}%\" for i in range(0, 35, 5)], fontsize = 12)\nax[1].text(6.5, 29.5, f\"\\u2013 Overall Model FNR: {g_fnr * 100: .2f}%\\n\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(6.5, 29, f\"   Overall Model FPR: {g_fpr * 100: .2f}%\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\")\nax[1].text(5.25, 29, \"\\u2013\", ha = \"center\", va = \"center\", fontsize = 14, color = \"purple\")\nax[1].text(1.5, 29.5, f\"   : FNR\\n   : FPR\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(1.25, 29.9, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax[1].text(1.25, 29.2, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\nfig.suptitle(\"Model Accuracy and FPR/FNR by Racial Group\", fontsize = 20)\nfig.text(0.5, 0.005, \"Racial Group (Abbreviated Category Names)\", ha = \"center\", va = \"center\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above uses the data frame constructed in the previous chunk to plot the overall model accuracy and FPR/FNR by racial group\nFigure 5\nThe plots above provide a visual accompaniment to the information displayed in Table 1. As shown in overall accuracy plot (left), the model’s accuracy for the majority of racial groups is lower than the general accuracy. The largest disparity in general accuracy to the accuracy for a specific racial group is found for individuals identifying as Native Hawaiian & Other Pacific Islander. Regarding FPR anf FNR, the model’s error rates across the racial groups appear to differ considerably both from one another and also from the general FPR and FNR.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize = (20, 10))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# TPR & TNR by Race\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"TPR\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], ax = ax[0])\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"TNR\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], alpha = 0.5, ax = ax[0], edgecolor = \"black\")\nax[0].axhline(g_tpr * 100, color = \"purple\", linestyle = \"--\")\nax[0].axhline(g_tnr * 100, color = \"black\", linestyle = \"--\")\nax[0].set_ylim(60, 105)\nax[0].set_title(\"TPR & TNR\", fontsize = 16)\nax[0].set_xlabel(\"\")\nax[0].set_ylabel(\"TPR & TNR\", fontsize = 14)\nax[0].set_xticks(range(9))\nax[0].set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax[0].set_yticks(range(0, 110, 10)[6:])\nax[0].set_yticklabels([f\"{i}%\" for i in (range(0, 110, 10)[6:])], fontsize = 12)\nax[0].text(6.427, 102.5, f\"\\u2013 Overall Model TNR: {g_tnr * 100: .2f}%\\n\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(6.427, 102, f\"   Overall Model TPR: {g_tpr * 100: .2f}%\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\")\nax[0].text(5.17, 102, \"\\u2013\", ha = \"center\", va = \"center\", fontsize = 14, color = \"purple\")\nax[0].text(1.5, 102.5, f\"   : TNR\\n   : TPR\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(1.25, 103.1, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax[0].text(1.25, 102.1, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\n# PPV & NPV by Race\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"PPV\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], ax = ax[1])\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"NPV\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], alpha = 0.5, ax = ax[1], edgecolor = \"black\")\nax[1].axhline(g_npv * 100, color = \"purple\", linestyle = \"--\")\nax[1].axhline(g_ppv * 100, color = \"black\", linestyle = \"--\")\nax[1].set_ylim(60, 105)\nax[1].set_title(\"PPV & NPV\", fontsize = 16)\nax[1].set_xlabel(\"\")\nax[1].set_ylabel(\"PPV & NPV\", fontsize = 14)\nax[1].set_xticks(range(9))\nax[1].set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax[1].set_yticks(range(0, 110, 10)[6:])\nax[1].set_yticklabels([f\"{i}%\" for i in (range(0, 110, 10)[6:])], fontsize = 12)\nax[1].text(6.427, 102.5, f\"\\u2013 Overall Model PPV: {g_ppv * 100: .2f}%\\n\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(6.427, 102, f\"   Overall Model NPV: {g_npv * 100: .2f}%\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\")\nax[1].text(5.17, 102, \"\\u2013\", ha = \"center\", va = \"center\", fontsize = 14, color = \"purple\")\nax[1].text(1.5, 102.5, f\"   : NPV\\n   : PPV\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(1.25, 103.1, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax[1].text(1.25, 102.1, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\nfig.suptitle(\"Model Accuracy and Error Rates by Racial Group\", fontsize = 20)\nfig.text(0.5, 0.005, \"Racial Group (Abbreviated Category Names)\", ha = \"center\", va = \"center\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above uses the data frame constructed in the previous chunk to plot the and TPR/TNR and PPV/NPV by racial group\nFigure 6\nThe plots above provide additional visual accompaniment for the accuracy rates and predictive values displayed in Table 1. As shown in the plot of TPR/TNR for each racial group, the TPR and TNR tend to deviate notably both from one another and from the model’s general TPR/TNR. For some racial groups though, such as white individuals in the data, the TPR and TNR are very close to the general TPR/TNR. As shown on the right, the the PPV and NPV metrics for each racial group are generally not majorly far from the overall PPV and NPV. However, note that this trend does not extend to all racial groups, such as individuals in the “A.I./A.N. Tribe Specified” or Native Hawaiian & Other Pacific Islander groups, who have a notably lower NPV values than the general NPV.\nBased on the information depicted in the Table 1 and the plots from Figure 7 and Figure 8, it is clear that the model’s accuracy and error rates are not entirely consistent across all racial groups. This inconsistency is particularly concerning for individuals identifying as A.I./A.N. Tribe Specified \\(^1\\), Asian, or Native Hawaiian & Other Pacific Islander. That is, for individuals associating with these racial groups, it appears that the model is not as good at making accurate predictions of employment status as it is for predicting the employment status of other racial groups. This analysis suggests that the model may be perpetuating (through the cost of its misclassifications) systemic biases and past injustices that effect certain individuals differently on the bases of racial identity.\n\n\n\n\n\n\n\nCode\n# Examining the calibration of the model\n## Calculating the probability that a prediction is positive for each racial group\ncalibration = test.groupby(\"race\").aggregate({\"pred\": \"mean\"})\ncalibration[\"pred\"] = calibration[\"pred\"] * 100\ncalibration[\"pred_prob_diff\"] = np.abs(calibration[\"pred\"] - ((test[\"pred\"]).mean() * 100))\ncalibration[\"emp_status_rate\"] = train.groupby(\"race\").aggregate({\"emp_status\": \"mean\"}) * 100\n\ncalibration\n\n# # Plotting these probabilities for each racial group\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nsns.barplot(calibration, x = \"race\", y = \"pred\", hue = \"race\", palette = sns.color_palette()[:-2], ax = ax, legend = False)\nsns.barplot(calibration, x = \"race\", y = \"emp_status_rate\", hue = \"race\", palette = sns.color_palette()[:-2], ax = ax, legend = False, alpha = 0.5, edgecolor = \"black\")\nax.set_title(\"Model Calibration by Racial Group\", fontsize = 16)\nax.set_xlabel(\"Racial Group (Abbreviated Category Names)\", fontsize = 14)\nax.set_xticks(range(8))\nax.set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax.set_ylabel(\"Probability of Positive Prediction\", fontsize = 14)\nax.set_yticks([20 * i for i in range(6)])\nax.set_yticklabels([f\"{20 * i}%\" for i in range(6)], fontsize = 12)\nax.text(0, 95, f\"   : Prob. Pos. Outcome\\n   : Prob. Pos. Pred.\", ha = \"left\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax.text(0.075, 97, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax.text(0.075, 93.5, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above calculates the probability of the model predicting an individual to be employed for each racial group and plots these probabilities together.\nFigure 7\nAs shown in the calibration plot above, the model is generally well-calibrated for the majority of racial groups. The model’s predicted probabilities of employment are generally equal to the actual probability of employment irrespective of racial group in that for most groups. However, for some racial groups, such as A.I./A.N.Specified \\(^1\\) and Asian individuals, the probability of a positive prediction is notably different from the probability of employment for these groups respectively. This suggests that the model is well calibrated for most racial groups (for individuals identifying as white, black or African American, American Indian, Alaska Native, Asian, some other race, or two or more races), but not necessarily well-calibrated across all racial groups.\n\n\n\n\n\nCode\n# Comparing FPR and TPR across each racial group\nerb[[\"Racial Group (Abbr. Cats.)\", \"FPR\", \"TPR\"]]\n\n\n\n\n\n\n\n\n\nRacial Group (Abbr. Cats.)\nFPR\nTPR\n\n\n\n\n1\nWhite\n18.2 %\n82.1 %\n\n\n2\nAmerican Indian\n11.1 %\n87.5 %\n\n\n3\nBlack or African American\n13.8 %\n82.3 %\n\n\n4\nAlaska Native\nNA\nNA\n\n\n5\nA.I./A.N. Tribe Specified¹\n0.0 %\n66.7 %\n\n\n6\nAsian\n28.2 %\n85.8 %\n\n\n7\nNative Hawaiian and Other Pacific Islander\n33.3 %\n83.3 %\n\n\n8\nSome Other Race\n16.8 %\n75.9 %\n\n\n9\nTwo or More Races\n14.0 %\n73.2 %\n\n\n\n\n\n\n\nTable 2\nThe table above revisits a subset of information presented in Table 1. As depicted in this table, the FPR for the majority of racial groups (white, black or African American, American Indian, Asian, some other race, two or more races) are generally similar values with the largest disparity being \\(&lt; 10\\%\\). The same general property can be observed in TPR across racial groups. However, while for a majority of racial groups, the FPR and TPR appear to be mostly balanced, this is certainly not the case for FPR anf TPR across all racial groups. E.G, the FPR and TPR for for individuals of the A.I./A.N. Tribe Specified \\(^1\\) group is significantly lower than for all other groups, suggesting that the model correctly predicts A.I./A.N. Tribe Specified \\(^1\\) as employed with less accuracy than for other groups (the opposite observation can be made for Native Hawaiian & Other Pacific Islander individuals as well). That is, this model does not strictly satisfy true error-rate balance.\n\n\n\n\n\nCode\n# Calculating the absolute difference of the probabilities of a positive prediction for each racial group to assess statistical parity\nspd = calibration[\"pred\"].max() - calibration[\"pred\"].min()\nprint(f\"Statistical Parity (ABS.) Difference: {spd: .2f}%\")\n\n# Calculating an adjusted SPD (excluding the group with a 100% positive prediction probability)\nspd_adj = calibration[calibration[\"pred\"] &lt; 100][\"pred\"].max() - calibration[calibration[\"pred\"] &lt; 100][\"pred\"].min()\nprint(f\"Adjusted Statistical Parity (ABS.) Difference: {spd_adj: .2f}%\")\n\n\nStatistical Parity (ABS.) Difference:  28.37%\nAdjusted Statistical Parity (ABS.) Difference:  28.37%\n\n\nTo assess statistical parity, the absolute difference between the maximum probability of a positive prediction and the minimum probability of a positive prediction across all racial groups is calculated.\n\\[SPD = \\max_{r \\in R} P(\\hat{Y} = 1 | R = r) - \\min_{r \\in R} P(\\hat{Y} = 1 | R = r)\\]\nWhere \\(R\\) is the set of all racial groups in the data and \\(\\hat{Y}\\) is the model’s predicted employment status. The SPD metric is a measure of the difference in the model’s predictions across racial groups. Only a value of \\(0\\) indicates that the model’s predictions are consistent across all racial groups. The model’s statistical parity difference for each racial group is roughly \\(38\\%\\). This metric lies well above the accepted ABS. SPD (\\(0\\)) value associated with true fairness found here, indicating that the model likely exhibits biases in its predictions across racial groups and therefore does not satisfy statistical parity.\n\n\n\n\n\nCode\n# Defining FPR as linear function of FNR when PPV and P are held constant for white and black or African American individuals in the data\n## Calculating P for white and black or African American individuals in the data\np_w = train[train[\"race\"] == 1.0][\"emp_status\"].sum() / train[train[\"race\"] == 1.0].shape[0]\np_b = train[train[\"race\"] == 2.0][\"emp_status\"].sum() / train[train[\"race\"] == 2.0].shape[0]\n\n# Pulling PPV for white and black or African American individuals from Table 1\nppv_min = min((race_acc[\"PPV\"].iloc[0] / 100), (race_acc[\"PPV\"].iloc[2] / 100))\n\n# Storing the relevant information in a data frame\nfpr_by_fnr = pd.DataFrame(columns = [\"FNRW\", \"FPRW\", \"FNRB\", \"FPRB\"])\nfpr_by_fnr[\"FNRB\"] = np.linspace(0, 1, 100)\nfpr_by_fnr[\"FNRW\"] = np.linspace(0, 1, 100)\nfpr_by_fnr[\"FPRB\"] = (p_b / (1 - p_b)) * ((1 - ppv_min) / ppv_min) * (1 - fpr_by_fnr[\"FNRB\"])\nfpr_by_fnr[\"FPRW\"] = (p_w / (1 - p_w)) * ((1 - ppv_min) / ppv_min) * (1 - fpr_by_fnr[\"FNRW\"])\n\n# # Plotting the FPR as a linear function of FNR\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\nsns.lineplot(fpr_by_fnr, x = fpr_by_fnr[\"FNRW\"], y = fpr_by_fnr[\"FPRW\"], ax = ax, color = \"darkorange\", label = \"White Individuals\")\nsns.lineplot(fpr_by_fnr, x = fpr_by_fnr[\"FNRB\"], y = fpr_by_fnr[\"FPRB\"], ax = ax, color = \"darkgrey\", label = \"Black or African American Individuals\")\nax.scatter(race_acc[\"FNR\"].iloc[2] / 100, race_acc[\"FPR\"].iloc[2] / 100, color = \"darkgray\", s = 100)\nax.scatter(race_acc[\"FNR\"].iloc[0] / 100, race_acc[\"FPR\"].iloc[0] / 100, color = \"darkorange\", s = 100)\nax.set_title(\"Feasible (FNR, FPR) Combinations\", fontsize = 16)\nax.set_xlabel(\"False Negative Rate\", fontsize = 14)\nax.set_ylabel(\"False Positive Rate\", fontsize = 14)\nax.set_xticks([(.25 * i) for i in range(5)])\n\nplt.tight_layout()\n\nprint(race_acc[\"FNR\"].iloc[0])\n\n\n17.9\n\n\n\n\n\n\n\n\n\nCode above plots FPR as a function of FNR, PPV, and prevalence for individuals identifying as white and those identifying as black or African American.\nFigure 8\nThe plot above is a reproduction of Figure 5 from Chouldechova’s 2017 publication “Fair prediction with disparate impact: A study of bias in recidivism prediction instruments”. For white and black or African American individuals in the data, the plot displays the model’s FPR as a linear function of FNR when the prevalence \\(p\\) and PPV are held constant. Specifically, the function(s) plotted above are:\n\\[FPR = \\frac{p}{1 - p} * \\frac{1 - PPV}{PPV} * (1 - FNR)\\]\nBased on this figure, to tune the refined model to yield equal FPR values for white and black or African American individuals, the model’s FNR for white individuals would need to be increased by around 10 percentage points (increasing from what appears to be about \\(1\\%\\) to about \\(27\\%\\)). This indicates that equalizing FPR values for white and black or African American individuals would require a decrease in the model’s accuracy for white individuals."
  },
  {
    "objectID": "posts/post_3/index.html#accessing-the-data",
    "href": "posts/post_3/index.html#accessing-the-data",
    "title": "Post 3 - Auditing Bias in Machine Learning Models",
    "section": "",
    "text": "Code\n# Includuing all additional imports\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom folktables import ACSDataSource, BasicProblem\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.pipeline import make_pipeline\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import SVC\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Downloading ACS PUMS data for the state of Massachusetts - Code provided by Prof. Chodrow\nSTATE = \"MA\"\nds = ACSDataSource(survey_year = \"2018\", \n                            horizon = \"1-Year\", \n                            survey = \"person\")\nma = ds.get_data(states = [STATE], download = True)\n\n\nIncluding all additional imports\nThe data for this study comes from an ACS (American Community Survey) data set. Specifically, the data is from a 2018 PUMS (Public Use Microdata Sample) survey for the state of Massachusetts. Each observation from this data set is an individual massachusetts resident who completed the 2018 PUMS survey. The data is downloaded using the ACSDataSource class from the folktables package.\n\n\n\n\nCode\nvars = [\"AGEP\", \"SCHL\", \"MAR\", \"RELP\", \"DIS\", \"ESP\", \"CIT\", \"MIG\", \"MIL\", \"ANC\", \"NATIVITY\", \"DEAR\", \"DEYE\", \"DREM\", \"SEX\", \"RAC1P\", \"ESR\"]\n\n\nUntouched, the data contains nearly 300 different features. However, for this study, I will only be using a subset of the columns:\n\nAGEP: Age (Range 0 - 99)\nSCHL: Educational Attainment (Range 1 - 24)\nMAR: Marital Status (Range 1 - 5)\nRELP: Relationship to Householder (Range 1 - 17)\nDIS: Disability Present (Binary)\nESP: Employment Status of Parents (Range 1 - 8)\nCIT: Citizenship Status (Range 1 - 5)\nMIG: Mobility Status - Lived in Specified Location 1 Year Ago (Range 1 - 3)\nMIL: Military Service (Range 1 - 5)\nANC: Ancestry (Range 1 - 4, 8)\nNATIVITY: Nativity (Binary)\nDEAR: Hearing Difficulty (Binary)\nDEYE: Vision Difficulty (Binary)\nDREM: Cognitive Difficulty (Binary)\nSEX: Sex (Binary)\nRAC1P: Race (Range 1 - 9)\nESR: Employment Status (Range 1 - 6)\nPINCP: Total Person Income (Integer Range of Income in US Dollars: -19997 - 4209995)"
  },
  {
    "objectID": "posts/post_3/index.html#model-1---predicting-employment-status",
    "href": "posts/post_3/index.html#model-1---predicting-employment-status",
    "title": "Post 3 - Auditing Bias in Machine Learning Models",
    "section": "",
    "text": "Code\n# Filtering out employment status (target) and race variables\nvars1 = [v for v in vars if v not in [\"ESR\", \"RAC1P\"]]\n\n# Defining the predictive modeling task - Code provided by Prof. Chodrow\nEmploymentProblem = BasicProblem(\n    features = vars1,\n    target = \"ESR\",\n    target_transform = lambda x: x == 1,\n    group = \"RAC1P\",\n    preprocess = lambda x: x,\n    postprocess = lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = EmploymentProblem.df_to_numpy(ma)\n\n# Test, train split procedure\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features, label, group, test_size = 0.2, random_state = 69)\n\n\nCode above defines the predictive modeling task and constructs the training data for the employment prediction model.\nBefore constructing the first predictive model, the predictive modeling task is defined. Afterwards, a test-train-split procedure is conducted on the data to prepare it for model fitting.\n\n\n\n\n\nCode\n# Observing some general descriptives in the training data\ntrain = pd.DataFrame(X_train, columns = vars1)\ntrain[\"emp_status\"] = y_train.astype(int)\ntrain[\"race\"] = group_train\ntrain.dropna(inplace = True)\n\n# Total number of individuals in the training data\nn = train.shape[0]\n\n# Proportion of employed individuals in the training data\nemp_prop = train[\"emp_status\"].mean()\n\n# Proportion of employed individuals of each race\ntot_emp = (train[\"emp_status\"] == 1).sum()\nrace_prop_emp_tot = (train.groupby(\"race\")[\"emp_status\"].sum() / tot_emp) * 100\n\n# Proportion of employed individuals within each race\nrace_prop_emp = train.groupby(\"race\").aggregate({\"emp_status\": \"mean\"})\nrace_prop_emp[\"u_emp\"] = 1 - race_prop_emp[\"emp_status\"]\n\n# Plotting proportion of employed individuals (total)\n## Creating dataframe for plotting\nemp_prop_tot = pd.DataFrame({\n    \"emp\": ['Unemployed', 'Employed'],\n    \"prop\": [1 - emp_prop, emp_prop]\n})\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nsns.barplot(emp_prop_tot, x = \"prop\", y = \"emp\", hue = \"prop\", palette = [\"#D8BFD8\", \"#5D3A6D\"], legend = False, width = 0.6, ax = ax)\nax.set_title(\"Overall Proportion of Employed Individuals\", fontsize = 16)\nax.set_yticks([0, 1])\nax.set_yticklabels([\"Unemployed\", \"Employed\"], fontsize = 12, rotation = 60)\nax.set_xlim(0.25, 0.55)\nax.set_xticks([i / 10 for i in range(3, 6)])\nax.set_xticklabels([f\"{i * 10}%\" for i in range(3, 6)], fontsize = 12)\nax.set_xlabel(\"Proportion to all Individuals\", fontsize = 14)\nax.set_ylabel(\"Employment Status\", fontsize = 14)\nax.text((1 - emp_prop) + 0.01, 0, f\"{round((1 - emp_prop) * 100, 2)}%\", ha = \"center\", va = \"center\", fontsize = 12, color = \"black\", rotation = 60)\nax.text(emp_prop + 0.01, 1, f\"{round( emp_prop * 100, 2)}%\", ha = \"center\", va = \"center\", fontsize = 12, color = \"black\", rotation = 60)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCode above calculates three proportions of employed individuals in the data: 1. General proportion of employment across all individuals 2. Proportion of each race in all employed individuals 3. Proportion of employed individuals within each race.\nFigure 1\nPrior to constructing and fitting the predictive model, it’s useful to explore some general descriptives of the data. In total, there are 56104 individuals in the training data. Approximately \\(50\\%\\) of individuals from the data are employed (as of their filling out the survey in 2018).\n\n\nCode\n# Plotting proportion of employed individuals across each race\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nsns.barplot(race_prop_emp, x = \"race\", y = \"emp_status\", hue = \"race\", palette = sns.color_palette()[:-1], width = 0.9, ax = ax, legend = False)\nsns.barplot(race_prop_emp, x = \"race\", y = \"u_emp\", hue = \"race\", palette = sns.color_palette()[:-1], width = 0.9, ax = ax, legend = False, alpha = 0.5, edgecolor = \"black\")\n\n# Adding space between bars\nfor patch in ax.patches:\n    patch.set_width(patch.get_width() * 0.8)\nax.set_title(\"Proportion of Employed/Unemployed Individuals Within Each Race Category\", fontsize = 16)\nax.set_xticks(range(9))\nax.set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax.set_xlabel(\"Racial Groups\", fontsize = 14)\nax.set_ylabel(\"Proportion of Employed Individuals\", fontsize = 14)\nax.set_yticks([(0.2 * i) for i in range(6)])\nax.set_yticklabels([f\"{(0.2 * i) * 100: .1f}%\" for i in range(6)], fontsize = 12)\nax.text(1, 0.8, f\"   : Unemployed\\n   : Employed\", ha = \"left\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax.text(1.075, 0.82, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax.text(1.075, 0.78, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCode above plots the proportions calculated above\nFigure 2\nOf the individuals across each racial group:\n\nAbout \\(51.5\\%\\) of white individuals are employed\nAbout \\(46\\%\\) of black or African American individuals are employed\nAbout \\(47\\%\\) of American Indian individuals are employed\nSeemingly all (\\(100\\%\\)) of Alaska Native individuals are employed\n\\(^1\\) About \\(47\\%\\) of individuals identifying as American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, or not specified and of no other races are employed\nAbout \\(50\\%\\) of Asian individuals are employed\nAbout \\(61\\%\\) of Native Hawaiian and Other Pacific Islander individuals are employed\nAbout \\(47.3\\%\\) of individuals identifying as some other race are employed\nAbout \\(38\\%\\) of individuals identifying as two or more races are employed\n\n\n\nCode\n# Race and Sex intersection\nsex_recode = {1.0: \"Male\", 2.0: \"Female\"}\ntrain[\"SEX_rc\"] = train[\"SEX\"].replace(sex_recode)\nixn_data = train.groupby([\"race\", \"SEX_rc\"]).agg({\"emp_status\": \"mean\"})\n\n# Barplot to display the intersection of race and gender in employment status\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nsns.barplot(ixn_data, x = \"race\", y = \"emp_status\", hue = \"SEX_rc\", palette = [\"#FFDAB9\", \"darkorange\"], ax = ax)\nax.set_title(\"Proportion of Employed Individuals for Each Race Category by Sex\", fontsize = 16)\nax.set_ylabel(\"Proportion of Employed Individuals\", fontsize = 14)\nax.set_xlabel(\"Race (Abbreviated Category Names)\", fontsize = 14)\nax.set_xticks(range(0, 9))\nax.set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], \n                   fontsize = 12, rotation = 15)\nax.set_yticks([(0.2 * i) for i in range(6)])\nax.set_yticklabels([f\"{(0.2 * i) * 100: .1f}%\" for i in range(6)], fontsize = 12)\nax.legend(frameon = True, fontsize = 12, title = \"Sex\", title_fontsize = 14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCode above calculates and plots the proportions of employment within each racial group by sex.\n\\(^1\\)“A.I./A.N.Specified”: Individuals who are American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, or not specified and of no other races.\nFigure 3\nThe figure above displays the possible intersectionality between race and sex among employed individuals. For five of the nine race categories (all but American Indian, Alaska Native, Asian, and Native Hawaiian & Other Pacific Islander) found in this data set, there does not appear to be a significant difference (\\(\\leq5\\%\\)) in employment rates between males and females. However, for the remaining racial groups, there does appear to be a noteworthy difference (about \\(\\geq10\\%\\)) in employment rates between males and females. Therefore, based on this figure, there generally does not appear to be a significant intersectional effect of race and sex on employment rates across all racial groups, but for some groups, an intersectional effect may be present. Please note that this finding does not dismiss the presence of intersectional biases and systemic injustices that may impact certain individuals from this data set. The existence of such intersectionality may very well be present in the data and but just not captured by this figure. Observing the intersectionality of race and gender relating to employment may be more clearly observed through other statistical processes.\n\n\n\n\n\nCode\n# Constructing and fitting the model\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier())\nmodel.fit(X_train, y_train)\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier', DecisionTreeClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier', DecisionTreeClassifier())]) StandardScaler?Documentation for StandardScalerStandardScaler() DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n\nCode\n# Refining the model\n## Identifying the optimal max_depth value - Model Complexity parameter for DecisionTreeCLassifier model - though iterative process\n# #Updated variables\nbest_max_depth = None\nbest_avg_score = 0\nfor md in np.random.randint(1, 101, size = 10):\n    model.set_params(decisiontreeclassifier__max_depth = md)\n    avg_score = cross_val_score(model, X_train, y_train, cv = 5).mean()\n    if avg_score &gt; best_avg_score:\n        best_avg_score = avg_score\n        best_max_depth = md\n\n# Fitting the model with the optimal max_depth value\ndtc_refined = model.set_params(decisiontreeclassifier__max_depth = best_max_depth)\ndtc_refined.fit(X_train, y_train)\ngen_score = dtc_refined.score(X_train, y_train)\nprint(f\"Refined Model Overall Accuracy: {gen_score * 100: .3f}%\")\n\n\nRefined Model Overall Accuracy:  81.475%\n\n\nCode above uses the make_pipeline method from the sklearn package, the training data is first standardized and then used to fit a DecisionTreeClassifier model (also from sklearn). The max_depth parameter for the model is tuned iteratively with corss-validation to set the tree depth that maximizes overall accuracy while mitigating model over-fitting.\nTo refine the model, an iterative process tuning the model complexity (using the max_depth parameter) with cross-validation is conducted. This process aims at model maximizing trining accuracy while reducing the presence of model over-fitting. Following this tuning procedure, the model has an overall accuracy of approximately \\(83.6\\%\\) – as an initial metric, this indicates a considerable accurate model.\n\n\n\n\n\nCode\n# Calculating various accuracy metrics\n## Overall Test data accuracy\nacc_t = dtc_refined.score(X_test, y_test)\ny_preds_t = dtc_refined.predict(X_test).astype(int)\nacc_t = (y_preds_t == y_test).mean()\n\n# Confusion matrix for refined model\nC = confusion_matrix(y_test, y_preds_t)\nemp_status = [\"Unemployed\", \"Employed\"]\n\n# Creating a heatmap for better confusion matrix visualization\nfig, ax = plt.subplots(1, 1, figsize = (5, 5))\nsns.heatmap(C, annot = True, annot_kws = {\"size\": 12}, fmt = \"d\", cmap = \"Reds\", cbar = False, ax = ax)\n\n# Plot styling\nax.set_xlabel(\"Predicted Employment Status\", fontsize = 14)\nax.set_ylabel(\"True Employment Status\", fontsize = 14)\nax.set_xticklabels(emp_status, fontsize = 12)\nax.set_yticklabels(emp_status, fontsize = 12)\nax.set_title(\"Employment Status Classification Confusion Matrix\", fontsize = 16)\n\nplt.show()\n\n# Printing confusion matrix results\nprint(f\"There were {C[0, 0]} unemployed individuals that were predicted to be unemployed.\")\nprint(f\"There were {C[0, 1]} unemployed individuals that were predicted to be employed.\")\nprint(f\"There were {C[1, 0]} employed individuals that were predicted to be unemployed.\")\nprint(f\"There were {C[1, 1]} unemployed individuals that were predicted to be employed.\")\n\n# Calculating various accuracies and error rates\ng_fnr = C[1, 0] / (C[1, 0] + C[1, 1])\ng_tnr = C[0, 0] / (C[0, 0] + C[0, 1])\ng_fpr = 1 - g_tnr\ng_tpr = 1 - g_fnr\ng_ppv = C[1, 1] / (C[1, 1] + C[0, 1])\ng_npv = C[0, 0] / (C[0, 0] + C[1, 0])\n\nprint(f\"Overall Testing Accuracy: {acc_t * 100: .3f}%\")\nprint(f\"General False Negative Rate: {g_fnr * 100: .3f}%\")\nprint(f\"General True Negative Rate: {g_tnr * 100: .3f}%\")\nprint(f\"General False Positive Rate: {g_fpr * 100: .3f}%\")\nprint(f\"General True Positive Rate: {g_tpr * 100: .3f}%\")\nprint(f\"General Positive Predictive Value: {g_ppv * 100: .3f}%\")\nprint(f\"General Negative Predictive Value: {g_npv * 100: .3f}%\")\n\n\n\n\n\n\n\n\n\nThere were 5690 unemployed individuals that were predicted to be unemployed.\nThere were 1284 unemployed individuals that were predicted to be employed.\nThere were 1271 employed individuals that were predicted to be unemployed.\nThere were 5782 unemployed individuals that were predicted to be employed.\nOverall Testing Accuracy:  81.785%\nGeneral False Negative Rate:  18.021%\nGeneral True Negative Rate:  81.589%\nGeneral False Positive Rate:  18.411%\nGeneral True Positive Rate:  81.979%\nGeneral Positive Predictive Value:  81.828%\nGeneral Negative Predictive Value:  81.741%\n\n\nCode above constructs the general confusion matrix for the refined model and calculates the typical accuracy/error rates: FPR, FNR, TPR, TNR, PPV, and NPV\nFigure 4\nSome key overall accuracy and error rates of the refined model are:\n\nOverall Test Accuracy: About \\(83\\%\\)\nFNR: About \\(13\\%\\) of truly employed individuals were misclassified as unemployed.\nTNR: About \\(79\\%\\) of truly unemployed individuals were correctly classified as unemployed.\nFPR: About \\(21\\%\\) of truly unemployed individuals were misclassified as employed.\nTPR: About \\(87\\%\\) of truly employed individuals were correctly classified as employed\nPPV: About \\(81\\%\\) of “employed” model predictions are correct while only about \\(20\\%\\) are incorrect.\nNPV: About \\(86\\%\\) of “unemployed” model predictions are correct while only about \\(14\\%\\) are incorrect.\n\nThe general accuracy and error rates outlined above indicate that the refined model is approximately similarly but not equally accurate in predicting that truly unemployed individuals are unemployed versus predicting that truly employed individuals are employed. Additionally, the refined misclassifies truly unemployed individuals and truly employed individuals at similar but notably unequal rates. The refined model yields a PPV and NPV that are close in value (with a difference of \\(&lt;7\\%\\)). This suggests that, for any given individual, the refined model is generally similarly good at predicting if this individual is employed as it is at predicting if this individual is unemployed.\nWhile the overall accuracy of the refined model is appears decently high (\\(83\\%\\)), it is crucial to investigate the same accuracy rates across different groups of observations found in the data. In this context, the prediction task is to identify if an individual is employed or not. In the data, the race of each individual is provided, and not only is race undoubtedly related to employment in the US but race is also a factor by which systemic biases and systematic discrimination occurs in the American workforce. Although the model was not trained using this variable (race), there is no guarantee that the model does not rely on variables highly related to race or that could stand as proxies for race. Thus, it is critically important to examine if the model perpetuates systemic biases and past injustices effecting certain individuals differently on the bases of racial identity."
  },
  {
    "objectID": "posts/post_3/index.html#auditing-the-model-for-racial-bias",
    "href": "posts/post_3/index.html#auditing-the-model-for-racial-bias",
    "title": "Post 3 - Auditing Bias in Machine Learning Models",
    "section": "",
    "text": "Code\n# Adding a column for indicating correct predictions\ntest = pd.DataFrame(X_test, columns = vars1)\ntest[\"emp_status\"] = y_test\ntest[\"pred\"] = y_preds_t\ntest[\"correct_pred\"] = (y_preds_t == y_test).astype(int)\n\n# Recoding race variable for easier visualization\nrace_recode = {\n    1.0: \"White\",\n    2.0: \"Black or African American\",\n    3.0: \"American Indian\",\n    4.0: \"Alaska Native\",\n    5.0: f\"A.I./A.N. Tribe Specified\\u00B9\",\n    6.0: \"Asian\",\n    7.0: \" Native Hawaiian and Other Pacific Islander\",\n    8.0: \"Some Other Race\",\n    9.0: \" Two or More Races\"\n}\ntest[\"race\"] = group_test\ntest[\"Racial Group (Abbreviated Categories)\"] = test[\"race\"].map(race_recode)\n\n# Helper functions to calculate various accuracy and error rate metrics\ndef acc(x):\n    if (x.shape[0] == 0):\n        return \"NA\"\n    \n    return str(round(x[\"correct_pred\"].mean() * 100, 1)) + \" %\"\n\ndef prop(x):\n    return str(round((x.shape[0] / train.shape[0]) * 100, 1)) + \" %\"\n\ndef fpr(x):\n    fp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 0)).sum()\n    tn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 0)).sum()\n    if ((fp + tn) == 0):\n        return \"NA\"\n    fpr = fp / (fp + tn)\n    \n    return str(round(fpr * 100, 1)) + \" %\"\n\ndef fnr(x):\n    fn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 1)).sum()\n    tp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 1)).sum()\n    if ((fn + tp) == 0):\n        return \"NA\"\n    fnr = fn / (fn + tp)\n\n    return str(round(fnr * 100, 1)) + \" %\"\n\ndef tpr(x):\n    fn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 1)).sum()\n    tp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 1)).sum()\n    if ((fn + tp) == 0):\n        return \"NA\"\n    tpr = tp / (fn + tp)\n\n    return str(round(tpr * 100, 1)) + \" %\"\n\ndef tnr(x):\n    fp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 0)).sum()\n    tn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 0)).sum()\n    if ((fp + tn) == 0):\n        return \"NA\"\n    tnr = tn / (fp + tn)\n    \n    return str(round(tnr * 100, 1)) + \" %\"\n\ndef ppv(x):\n    tp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 1)).sum()\n    fp = ((x[\"pred\"] == 1) & (x[\"emp_status\"] == 0)).sum()\n    if ((tp + fp) == 0):\n        return \"NA\"\n    ppv = tp / (tp + fp)\n    \n    return str(round(ppv * 100, 1)) + \" %\"\n\ndef npv(x):\n    tn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 0)).sum()\n    fn = ((x[\"pred\"] == 0) & (x[\"emp_status\"] == 1)).sum()\n    if ((tn + fn) == 0):\n        return \"NA\"\n    npv = tn / (tn + fn)\n\n    return str(round(npv * 100, 1)) + \" %\"\n\n# Initializing data frame to visualize model accuracy and error rates for each race\nrace_acc = pd.DataFrame(index = range(1, 10), columns = [\"Racial Group (Abbr. Cats.)\", \"Model Accuracy\", \"Proportion to Tot. (Aprx.)\", \"FPR\", \"FNR\", \"TPR\", \"TNR\", \"PPV\", \"NPV\"])\n\n# Subsetting training data into each race category\nr1 = test[test[\"race\"] == 1.0]\nr2 = test[test[\"race\"] == 2.0]\nr3 = test[test[\"race\"] == 3.0]\nr4 = test[test[\"race\"] == 4.0]\nr5 = test[test[\"race\"] == 5.0]\nr6 = test[test[\"race\"] == 6.0]\nr7 = test[test[\"race\"] == 7.0]\nr8 = test[test[\"race\"] == 8.0]\nr9 = test[test[\"race\"] == 9.0]\n\n# Populating the accuracy/error rates table\nrace_acc.loc[1] = [race_recode.get(1.0), acc(r1), prop(r1), fpr(r1), fnr(r1), tpr(r1), tnr(r1), ppv(r1), npv(r1)]\nrace_acc.loc[3] = [race_recode.get(2.0), acc(r2), prop(r2), fpr(r2), fnr(r2), tpr(r2), tnr(r2), ppv(r2), npv(r2)]\nrace_acc.loc[2] = [race_recode.get(3.0), acc(r3), prop(r3), fpr(r3), fnr(r3), tpr(r3), tnr(r3), ppv(r3), npv(r3)]\nrace_acc.loc[4] = [race_recode.get(4.0), acc(r4), prop(r4), fpr(r4), fnr(r4), tpr(r4), tnr(r4), ppv(r4), npv(r4)]\nrace_acc.loc[5] = [race_recode.get(5.0), acc(r5), prop(r5), fpr(r5), fnr(r5), tpr(r5), tnr(r5), ppv(r5), npv(r5)]\nrace_acc.loc[6] = [race_recode.get(6.0), acc(r6), prop(r6), fpr(r6), fnr(r6), tpr(r6), tnr(r6), ppv(r6), npv(r6)]\nrace_acc.loc[7] = [race_recode.get(7.0), acc(r7), prop(r7), fpr(r7), fnr(r7), tpr(r7), tnr(r7), ppv(r7), npv(r7)]\nrace_acc.loc[8] = [race_recode.get(8.0), acc(r8), prop(r8), fpr(r8), fnr(r8), tpr(r8), tnr(r8), ppv(r8), npv(r8)]\nrace_acc.loc[9] = [race_recode.get(9.0), acc(r9), prop(r9), fpr(r9), fnr(r9), tpr(r9), tnr(r9), ppv(r9), npv(r9)]\nrace_acc\n\n\n\n\n\n\n\n\n\nRacial Group (Abbr. Cats.)\nModel Accuracy\nProportion to Tot. (Aprx.)\nFPR\nFNR\nTPR\nTNR\nPPV\nNPV\n\n\n\n\n1\nWhite\n81.9 %\n20.2 %\n18.2 %\n17.9 %\n82.1 %\n81.8 %\n82.4 %\n81.5 %\n\n\n2\nAmerican Indian\n88.2 %\n0.0 %\n11.1 %\n12.5 %\n87.5 %\n88.9 %\n87.5 %\n88.9 %\n\n\n3\nBlack or African American\n84.4 %\n1.5 %\n13.8 %\n17.7 %\n82.3 %\n86.2 %\n84.0 %\n84.6 %\n\n\n4\nAlaska Native\nNA\n0.0 %\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n5\nA.I./A.N. Tribe Specified¹\n80.0 %\n0.0 %\n0.0 %\n33.3 %\n66.7 %\n100.0 %\n100.0 %\n66.7 %\n\n\n6\nAsian\n79.0 %\n1.7 %\n28.2 %\n14.2 %\n85.8 %\n71.8 %\n76.4 %\n82.7 %\n\n\n7\nNative Hawaiian and Other Pacific Islander\n77.8 %\n0.0 %\n33.3 %\n16.7 %\n83.3 %\n66.7 %\n83.3 %\n66.7 %\n\n\n8\nSome Other Race\n79.8 %\n0.8 %\n16.8 %\n24.1 %\n75.9 %\n83.2 %\n79.8 %\n79.8 %\n\n\n9\nTwo or More Races\n80.7 %\n0.7 %\n14.0 %\n26.8 %\n73.2 %\n86.0 %\n78.3 %\n82.2 %\n\n\n\n\n\n\n\nCode above subsets the training data into each race category and creates a table displaying the model’s accuracy and error rates in employment/unemployment predictions for each racial group.\n\\(^1\\)“A.I./A.N.Specified”: Individuals who are American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, or not specified and of no other races.\nTable 1\nThe table above depicts the refined model’s accuracy and error rates for employment prediction of individuals across each racial group in the training data. The general accuracies for each racial group are all largely similar – with values ranging from around \\(80\\%\\) to \\(89\\%\\) accuracy.\n\\[FPR = \\frac{FP}{FP + TN}\\] \\[FNR = \\frac{FN}{FN + TP}\\]\nThe FPR and FNR for each race are slightly less equal across all racial groups (FPR range from roughly \\(0\\%\\) to \\(33\\%\\) and FNR range also from \\(0\\%\\) to \\(33\\%\\)), but these error rates do not differ dramatically.\n\\[TPR = \\frac{TP}{TP + FN}\\] \\[TNR = \\frac{TN}{TN + FP}\\]\nA similar trend is generally observable for TPR across each racial group (majority TPR range from about \\(83\\%\\) to \\(100\\%\\)) - with the exception of the “American Indian and Alaska Native tribes specified” group with a notably lower TPR: \\(67\\%\\)). As for TNR, there is a larger spread in values across all racial groups with a general TNR range of \\(67\\%\\) to \\(100\\%\\).\n\\[PPV = \\frac{TP}{TP + FP}\\] \\[NPV = \\frac{TN}{TN + FN}\\]\nFurther, the PPV and NPV for each racial group are primarily similar and display a smaller spread of values – with PPV metrics ranging from approximately \\(77\\%\\) to \\(100\\%\\) and NPV values ranging from approximately \\(67\\%\\) to \\(100\\%\\).\n\n\nCode\n# Changing column types for easier visualization\n## Saving a data frame for late use\nerb = race_acc.copy()\nfor col in race_acc.columns[1:]:\n    if (race_acc[col].dtype == \"object\"):\n        race_acc[col] = pd.to_numeric(race_acc[col].str.replace(\"%\", \"\").replace(\"NA\", np.nan))\n\n# Plotting the information described in the table above for easier visualization\nfig, ax = plt.subplots(1, 2, figsize = (20, 10))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Accuracy by Race\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"Model Accuracy\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], ax = ax[0])\nax[0].axhline(gen_score * 100, color = \"black\", linestyle = \"--\")\nax[0].set_ylim(40, 95)\nax[0].set_title(\"Model Accuracy\", fontsize = 16)\nax[0].set_xlabel(\"\")\nax[0].set_ylabel(\"Model Accuracy\", fontsize = 14)\nax[0].set_xticks(range(9))\nax[0].set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax[0].set_yticks([(10 * i) for i in range(4, 10)])\nax[0].set_yticklabels([f\"{10 * i}%\" for i in range(4, 10)], fontsize = 12)\nax[0].text(6.15, 92, f\"\\u2013 Overall Model Accuracy: {gen_score * 100: .2f}%\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\n\n# FPR & FNR by Race\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"FPR\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], ax = ax[1])\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"FNR\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], alpha = 0.5, ax = ax[1], edgecolor = \"black\")\nax[1].axhline(g_fpr * 100, color = \"purple\", linestyle = \"--\")\nax[1].axhline(g_fnr * 100, color = \"black\", linestyle = \"--\")\nax[1].set_title(\"FPR & FNR\", fontsize = 16)\nax[1].set_xlabel(\"\")\nax[1].set_ylabel(\"FPR & FNR\", fontsize = 14)\nax[1].set_xticks(range(9))\nax[1].set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax[1].set_yticks([i for i in range(0, 35, 5)])\nax[1].set_yticklabels([f\"{i}%\" for i in range(0, 35, 5)], fontsize = 12)\nax[1].text(6.5, 29.5, f\"\\u2013 Overall Model FNR: {g_fnr * 100: .2f}%\\n\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(6.5, 29, f\"   Overall Model FPR: {g_fpr * 100: .2f}%\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\")\nax[1].text(5.25, 29, \"\\u2013\", ha = \"center\", va = \"center\", fontsize = 14, color = \"purple\")\nax[1].text(1.5, 29.5, f\"   : FNR\\n   : FPR\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(1.25, 29.9, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax[1].text(1.25, 29.2, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\nfig.suptitle(\"Model Accuracy and FPR/FNR by Racial Group\", fontsize = 20)\nfig.text(0.5, 0.005, \"Racial Group (Abbreviated Category Names)\", ha = \"center\", va = \"center\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above uses the data frame constructed in the previous chunk to plot the overall model accuracy and FPR/FNR by racial group\nFigure 5\nThe plots above provide a visual accompaniment to the information displayed in Table 1. As shown in overall accuracy plot (left), the model’s accuracy for the majority of racial groups is lower than the general accuracy. The largest disparity in general accuracy to the accuracy for a specific racial group is found for individuals identifying as Native Hawaiian & Other Pacific Islander. Regarding FPR anf FNR, the model’s error rates across the racial groups appear to differ considerably both from one another and also from the general FPR and FNR.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize = (20, 10))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# TPR & TNR by Race\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"TPR\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], ax = ax[0])\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"TNR\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], alpha = 0.5, ax = ax[0], edgecolor = \"black\")\nax[0].axhline(g_tpr * 100, color = \"purple\", linestyle = \"--\")\nax[0].axhline(g_tnr * 100, color = \"black\", linestyle = \"--\")\nax[0].set_ylim(60, 105)\nax[0].set_title(\"TPR & TNR\", fontsize = 16)\nax[0].set_xlabel(\"\")\nax[0].set_ylabel(\"TPR & TNR\", fontsize = 14)\nax[0].set_xticks(range(9))\nax[0].set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax[0].set_yticks(range(0, 110, 10)[6:])\nax[0].set_yticklabels([f\"{i}%\" for i in (range(0, 110, 10)[6:])], fontsize = 12)\nax[0].text(6.427, 102.5, f\"\\u2013 Overall Model TNR: {g_tnr * 100: .2f}%\\n\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(6.427, 102, f\"   Overall Model TPR: {g_tpr * 100: .2f}%\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\")\nax[0].text(5.17, 102, \"\\u2013\", ha = \"center\", va = \"center\", fontsize = 14, color = \"purple\")\nax[0].text(1.5, 102.5, f\"   : TNR\\n   : TPR\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(1.25, 103.1, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax[0].text(1.25, 102.1, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\n# PPV & NPV by Race\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"PPV\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], ax = ax[1])\nsns.barplot(race_acc, x = \"Racial Group (Abbr. Cats.)\", y = \"NPV\", hue = \"Racial Group (Abbr. Cats.)\", palette = sns.color_palette()[:-1], alpha = 0.5, ax = ax[1], edgecolor = \"black\")\nax[1].axhline(g_npv * 100, color = \"purple\", linestyle = \"--\")\nax[1].axhline(g_ppv * 100, color = \"black\", linestyle = \"--\")\nax[1].set_ylim(60, 105)\nax[1].set_title(\"PPV & NPV\", fontsize = 16)\nax[1].set_xlabel(\"\")\nax[1].set_ylabel(\"PPV & NPV\", fontsize = 14)\nax[1].set_xticks(range(9))\nax[1].set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"Alaska\\nNative\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax[1].set_yticks(range(0, 110, 10)[6:])\nax[1].set_yticklabels([f\"{i}%\" for i in (range(0, 110, 10)[6:])], fontsize = 12)\nax[1].text(6.427, 102.5, f\"\\u2013 Overall Model PPV: {g_ppv * 100: .2f}%\\n\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(6.427, 102, f\"   Overall Model NPV: {g_npv * 100: .2f}%\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\")\nax[1].text(5.17, 102, \"\\u2013\", ha = \"center\", va = \"center\", fontsize = 14, color = \"purple\")\nax[1].text(1.5, 102.5, f\"   : NPV\\n   : PPV\", ha = \"center\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(1.25, 103.1, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax[1].text(1.25, 102.1, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\nfig.suptitle(\"Model Accuracy and Error Rates by Racial Group\", fontsize = 20)\nfig.text(0.5, 0.005, \"Racial Group (Abbreviated Category Names)\", ha = \"center\", va = \"center\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above uses the data frame constructed in the previous chunk to plot the and TPR/TNR and PPV/NPV by racial group\nFigure 6\nThe plots above provide additional visual accompaniment for the accuracy rates and predictive values displayed in Table 1. As shown in the plot of TPR/TNR for each racial group, the TPR and TNR tend to deviate notably both from one another and from the model’s general TPR/TNR. For some racial groups though, such as white individuals in the data, the TPR and TNR are very close to the general TPR/TNR. As shown on the right, the the PPV and NPV metrics for each racial group are generally not majorly far from the overall PPV and NPV. However, note that this trend does not extend to all racial groups, such as individuals in the “A.I./A.N. Tribe Specified” or Native Hawaiian & Other Pacific Islander groups, who have a notably lower NPV values than the general NPV.\nBased on the information depicted in the Table 1 and the plots from Figure 7 and Figure 8, it is clear that the model’s accuracy and error rates are not entirely consistent across all racial groups. This inconsistency is particularly concerning for individuals identifying as A.I./A.N. Tribe Specified \\(^1\\), Asian, or Native Hawaiian & Other Pacific Islander. That is, for individuals associating with these racial groups, it appears that the model is not as good at making accurate predictions of employment status as it is for predicting the employment status of other racial groups. This analysis suggests that the model may be perpetuating (through the cost of its misclassifications) systemic biases and past injustices that effect certain individuals differently on the bases of racial identity.\n\n\n\n\n\n\n\nCode\n# Examining the calibration of the model\n## Calculating the probability that a prediction is positive for each racial group\ncalibration = test.groupby(\"race\").aggregate({\"pred\": \"mean\"})\ncalibration[\"pred\"] = calibration[\"pred\"] * 100\ncalibration[\"pred_prob_diff\"] = np.abs(calibration[\"pred\"] - ((test[\"pred\"]).mean() * 100))\ncalibration[\"emp_status_rate\"] = train.groupby(\"race\").aggregate({\"emp_status\": \"mean\"}) * 100\n\ncalibration\n\n# # Plotting these probabilities for each racial group\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nsns.barplot(calibration, x = \"race\", y = \"pred\", hue = \"race\", palette = sns.color_palette()[:-2], ax = ax, legend = False)\nsns.barplot(calibration, x = \"race\", y = \"emp_status_rate\", hue = \"race\", palette = sns.color_palette()[:-2], ax = ax, legend = False, alpha = 0.5, edgecolor = \"black\")\nax.set_title(\"Model Calibration by Racial Group\", fontsize = 16)\nax.set_xlabel(\"Racial Group (Abbreviated Category Names)\", fontsize = 14)\nax.set_xticks(range(8))\nax.set_xticklabels([\"White\", \"Black or African\\nAmerican\", \"American\\nIndian\", \"A.I./A.N.\\nTribe Specified$^1$\", \"Asian\", \"Native Hawaiian\\n& Other Pacific\\nIslander\", \"Other\\nRace\", \"Two or more\\nRaces\"], fontsize = 12, rotation = 15)\nax.set_ylabel(\"Probability of Positive Prediction\", fontsize = 14)\nax.set_yticks([20 * i for i in range(6)])\nax.set_yticklabels([f\"{20 * i}%\" for i in range(6)], fontsize = 12)\nax.text(0, 95, f\"   : Prob. Pos. Outcome\\n   : Prob. Pos. Pred.\", ha = \"left\", va = \"center\", fontsize = 14, color = \"black\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax.text(0.075, 97, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\", alpha = 0.5)\nax.text(0.075, 93.5, \"\\u25A0\", ha = \"center\", va = \"center\", fontsize = 16, color = \"red\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above calculates the probability of the model predicting an individual to be employed for each racial group and plots these probabilities together.\nFigure 7\nAs shown in the calibration plot above, the model is generally well-calibrated for the majority of racial groups. The model’s predicted probabilities of employment are generally equal to the actual probability of employment irrespective of racial group in that for most groups. However, for some racial groups, such as A.I./A.N.Specified \\(^1\\) and Asian individuals, the probability of a positive prediction is notably different from the probability of employment for these groups respectively. This suggests that the model is well calibrated for most racial groups (for individuals identifying as white, black or African American, American Indian, Alaska Native, Asian, some other race, or two or more races), but not necessarily well-calibrated across all racial groups.\n\n\n\n\n\nCode\n# Comparing FPR and TPR across each racial group\nerb[[\"Racial Group (Abbr. Cats.)\", \"FPR\", \"TPR\"]]\n\n\n\n\n\n\n\n\n\nRacial Group (Abbr. Cats.)\nFPR\nTPR\n\n\n\n\n1\nWhite\n18.2 %\n82.1 %\n\n\n2\nAmerican Indian\n11.1 %\n87.5 %\n\n\n3\nBlack or African American\n13.8 %\n82.3 %\n\n\n4\nAlaska Native\nNA\nNA\n\n\n5\nA.I./A.N. Tribe Specified¹\n0.0 %\n66.7 %\n\n\n6\nAsian\n28.2 %\n85.8 %\n\n\n7\nNative Hawaiian and Other Pacific Islander\n33.3 %\n83.3 %\n\n\n8\nSome Other Race\n16.8 %\n75.9 %\n\n\n9\nTwo or More Races\n14.0 %\n73.2 %\n\n\n\n\n\n\n\nTable 2\nThe table above revisits a subset of information presented in Table 1. As depicted in this table, the FPR for the majority of racial groups (white, black or African American, American Indian, Asian, some other race, two or more races) are generally similar values with the largest disparity being \\(&lt; 10\\%\\). The same general property can be observed in TPR across racial groups. However, while for a majority of racial groups, the FPR and TPR appear to be mostly balanced, this is certainly not the case for FPR anf TPR across all racial groups. E.G, the FPR and TPR for for individuals of the A.I./A.N. Tribe Specified \\(^1\\) group is significantly lower than for all other groups, suggesting that the model correctly predicts A.I./A.N. Tribe Specified \\(^1\\) as employed with less accuracy than for other groups (the opposite observation can be made for Native Hawaiian & Other Pacific Islander individuals as well). That is, this model does not strictly satisfy true error-rate balance.\n\n\n\n\n\nCode\n# Calculating the absolute difference of the probabilities of a positive prediction for each racial group to assess statistical parity\nspd = calibration[\"pred\"].max() - calibration[\"pred\"].min()\nprint(f\"Statistical Parity (ABS.) Difference: {spd: .2f}%\")\n\n# Calculating an adjusted SPD (excluding the group with a 100% positive prediction probability)\nspd_adj = calibration[calibration[\"pred\"] &lt; 100][\"pred\"].max() - calibration[calibration[\"pred\"] &lt; 100][\"pred\"].min()\nprint(f\"Adjusted Statistical Parity (ABS.) Difference: {spd_adj: .2f}%\")\n\n\nStatistical Parity (ABS.) Difference:  28.37%\nAdjusted Statistical Parity (ABS.) Difference:  28.37%\n\n\nTo assess statistical parity, the absolute difference between the maximum probability of a positive prediction and the minimum probability of a positive prediction across all racial groups is calculated.\n\\[SPD = \\max_{r \\in R} P(\\hat{Y} = 1 | R = r) - \\min_{r \\in R} P(\\hat{Y} = 1 | R = r)\\]\nWhere \\(R\\) is the set of all racial groups in the data and \\(\\hat{Y}\\) is the model’s predicted employment status. The SPD metric is a measure of the difference in the model’s predictions across racial groups. Only a value of \\(0\\) indicates that the model’s predictions are consistent across all racial groups. The model’s statistical parity difference for each racial group is roughly \\(38\\%\\). This metric lies well above the accepted ABS. SPD (\\(0\\)) value associated with true fairness found here, indicating that the model likely exhibits biases in its predictions across racial groups and therefore does not satisfy statistical parity.\n\n\n\n\n\nCode\n# Defining FPR as linear function of FNR when PPV and P are held constant for white and black or African American individuals in the data\n## Calculating P for white and black or African American individuals in the data\np_w = train[train[\"race\"] == 1.0][\"emp_status\"].sum() / train[train[\"race\"] == 1.0].shape[0]\np_b = train[train[\"race\"] == 2.0][\"emp_status\"].sum() / train[train[\"race\"] == 2.0].shape[0]\n\n# Pulling PPV for white and black or African American individuals from Table 1\nppv_min = min((race_acc[\"PPV\"].iloc[0] / 100), (race_acc[\"PPV\"].iloc[2] / 100))\n\n# Storing the relevant information in a data frame\nfpr_by_fnr = pd.DataFrame(columns = [\"FNRW\", \"FPRW\", \"FNRB\", \"FPRB\"])\nfpr_by_fnr[\"FNRB\"] = np.linspace(0, 1, 100)\nfpr_by_fnr[\"FNRW\"] = np.linspace(0, 1, 100)\nfpr_by_fnr[\"FPRB\"] = (p_b / (1 - p_b)) * ((1 - ppv_min) / ppv_min) * (1 - fpr_by_fnr[\"FNRB\"])\nfpr_by_fnr[\"FPRW\"] = (p_w / (1 - p_w)) * ((1 - ppv_min) / ppv_min) * (1 - fpr_by_fnr[\"FNRW\"])\n\n# # Plotting the FPR as a linear function of FNR\nfig, ax = plt.subplots(1, 1, figsize = (12.5, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\nsns.lineplot(fpr_by_fnr, x = fpr_by_fnr[\"FNRW\"], y = fpr_by_fnr[\"FPRW\"], ax = ax, color = \"darkorange\", label = \"White Individuals\")\nsns.lineplot(fpr_by_fnr, x = fpr_by_fnr[\"FNRB\"], y = fpr_by_fnr[\"FPRB\"], ax = ax, color = \"darkgrey\", label = \"Black or African American Individuals\")\nax.scatter(race_acc[\"FNR\"].iloc[2] / 100, race_acc[\"FPR\"].iloc[2] / 100, color = \"darkgray\", s = 100)\nax.scatter(race_acc[\"FNR\"].iloc[0] / 100, race_acc[\"FPR\"].iloc[0] / 100, color = \"darkorange\", s = 100)\nax.set_title(\"Feasible (FNR, FPR) Combinations\", fontsize = 16)\nax.set_xlabel(\"False Negative Rate\", fontsize = 14)\nax.set_ylabel(\"False Positive Rate\", fontsize = 14)\nax.set_xticks([(.25 * i) for i in range(5)])\n\nplt.tight_layout()\n\nprint(race_acc[\"FNR\"].iloc[0])\n\n\n17.9\n\n\n\n\n\n\n\n\n\nCode above plots FPR as a function of FNR, PPV, and prevalence for individuals identifying as white and those identifying as black or African American.\nFigure 8\nThe plot above is a reproduction of Figure 5 from Chouldechova’s 2017 publication “Fair prediction with disparate impact: A study of bias in recidivism prediction instruments”. For white and black or African American individuals in the data, the plot displays the model’s FPR as a linear function of FNR when the prevalence \\(p\\) and PPV are held constant. Specifically, the function(s) plotted above are:\n\\[FPR = \\frac{p}{1 - p} * \\frac{1 - PPV}{PPV} * (1 - FNR)\\]\nBased on this figure, to tune the refined model to yield equal FPR values for white and black or African American individuals, the model’s FNR for white individuals would need to be increased by around 10 percentage points (increasing from what appears to be about \\(1\\%\\) to about \\(27\\%\\)). This indicates that equalizing FPR values for white and black or African American individuals would require a decrease in the model’s accuracy for white individuals."
  },
  {
    "objectID": "posts/project/index.html",
    "href": "posts/project/index.html",
    "title": "Project Solar-Searcher",
    "section": "",
    "text": "1. Abstract\nAs the severity of climate change continues to increase and the consumption of energy in the US remains massive, a critical focus of today’s power industry is establishing sources of renewable energy to replace less climate-friendly alternatives. Focusing on solar energy systems, we aim to address the problem of locating optimal regions throughout the US to build solar farms for cleaner energy production. Our general approach to this problem is to determine the predicted energy output of installing a solar system at a given coordinate location in the continental US. Leveraging several machine learning techniques to assist our success, we chose to frame this problem as a standard regression task performed by a predictive model. For training and informing the design of our predictive models, we collected, cleaned, and combined location-specific data from several sources on weather, elevation, and solar statistics throughout the continental US. Our modeling approach involves the use of a custom neural network and experimentation with different network layers and nonlinear activation functions. In addition to constructing our custom models, we fit and evaluated a standard linear regression and polynomial regression model from the sklearn library to compare predictive performance across several model architectures (performance assessed via Mean-Squared-Error loss minimization). From our model tuning and comparison procedures, we found that a linear regression model using the ReLU activation function offers the second most accurate solar energy predictions, falling short only to the prebuilt polynomial regression model from sklearn. To visualize and interpret our findings, we developed an interactive map of the continental US, displaying predicted solar energy output (in kWh/kWp \\(-\\) kilowatt-hour per kilowatt-peak) at a resolution of approximately \\(4\\) km \\(^2\\). We intend for our potential solar energy map to stand as a practical tool for guiding future solar power system development and planning.\nThe entirety of our work is accessible in the following public repository: Solar-Searcher\n\n\n2. Introduction\n\n2.1 Motivation\nAs acknowledged above, climate change and global warming continue to rapidly intensify, driving adversity towards billions of people and countless species of wildlife across the planet. In a 2023 report from the World Health Organization, it was conservatively projected that climate change will cause an additional \\(250,000\\) annual deaths by 2030 (World Health Organization 2023). Along similar and possibly more severe lines, the World Wide Fund for Nature reports a “catastrophic \\(73\\%\\) decline in the average size of monitored wildlife populations” over the past 50 years (World Wildlife Fund 2024). Perhaps the most significant threat posed by climate change and global warming is the irreparable and irreversible alteration of the planet’s biosphere. Some experts even fear that the “point of no return” has already been passed. Yet, despite the daunting effects and implications of the current climate circumstances, it remains crucial to direct worldwide attention, technology, and resources towards establishing climate-conscious societies. It is widely recognized that some of the largest contributors to the current climate crisis are carbon-intensive, non-renewable energy production systems. Consequently, large actors of the global power industry, including governmental bodies and officials throughout many nations, are drawing their attention toward the development of planet-supporting energy solutions. Such solutions involve the replacement of preexisting, harmful energy systems with renewable alternatives. Simply put, establishing renewable and regenerative energy sources is an irrefutably necessary step in combating global warming.\nThree of the most common avenues for renewable energy systems are in solar-, hydro-, and wind-powered generators. While these clean-energy sources are proven to be not only highly effective but also regenerative (or supporting of regeneration), they often require a complex set of specific circumstances for construction. Significant renewable energy system limitations include geographical/topological elements and the consistency of weather conditions of a given power production location. To function at peak capacity and comparably to traditional, environmentally abrasive methods, developers aim to install renewable energy systems in regions with the optimal set of conditions. Consequently, the task of identifying these optimal installation locations becomes a primary concern of renewable energy development. As one of the most abundant sources of renewable energy in the continental US, we direct our attention in this study to the development of solar energy systems. Specifically, we intend to address several key concerns of photovoltaic (PV) system implementation including: What factors should solar energy developers take into consideration when initiating installation projects? Based on the relevant factors, which regions are optimal locations for PV system development? At a given location, how can the quality of installing a solar energy system be quantified and compared to other neighboring areas? Driven by these questions, our study offers an approach to solar energy forecasting in the continental US.\n\n\n2.2 Related Work\nSolar energy forecasting has been widely addressed with the use of machine learning tools and techniques. Previous research in this area includes the incorporation of neural network-based models, random forest classifiers, and linear regression techniques \\(-\\) as well as thorough collection and preprocessing procedures of meteorological data \\(-\\) to provide informative insight for predicting solar power production. A 2020 study focusing on solar energy production in Hawaii found a gradient-boosted regression model enhanced with a standard PCA data process to yield strong predictive results for solar power forecasting (Munawar and Wang 2020). Similarly, a 2021 article describes another forecasting framework comparing linear and nonlinear regression models, experimenting with artificial neural network architectures, for predicting solar energy production in Morocco (Jebli et al. 2021). Alternatively, a related 2022 study on solar energy forecasting found compelling results using a modeling approach featuring a machine learning ensemble-to-classifier pipeline operating on standard weather data (Alzubaidi et al. 2022). In our study, we aim to extend the previous research in this field and develop a comprehensible, accessible PV energy prediction framework oriented for the continental US.\n\n\n2.3 Study Overview\nIn the sections below, we present our data collection and cleaning protocols; our model designs, neural network architecture, and model comparisons; our solar forecasting results; and an accompanying discussion addressing our progress and potential future work. Additionally, as a preface to the technical content explored in our study, we offer a brief acknowledgement of the societal implications and impact of our work.\n\n\n\n3. Values Statement\n\n3.1 Affected Parties, Benefits, and Harms\nWe view the potential users of our project to be a fairly wide range, including governmental initiatives, commercial enterprises, and possibly individuals looking to invest in small-scale solar farming. Essentially, any party with potential interest in making use of solar energy has the potential to use our project.\nUnder the assumption that our model will be used to inform solar development, potentially affected parties are the aforementioned end users, who may be benefitted by accurate results informing optimal solar energy development, or harmed if the results are misleading and do not accurately represent the theoretical output. Overall, local communities also have the potential to be impacted, as solar farms do take up a reasonable amount of property, which requires zoning and adequate space (Igini 2023). One other concern is that our application may unfairly harm underrepresented or marginalized groups, as solar panels are expensive to develop, and our model may prioritize more privileged areas. Lastly, it is important to recognize that while our project aims to optimize renewable energy generation, machine learning models require a significant amount of energy to train. While our model specifically only had to undergo training a limited number of times, it is still worth recognizing that in developing cleaner energy, we do have to make use of existing power.\nTo summarize, given the increasing global need for sustainable forms of energy, we believe that our model has the potential to be beneficial to the planet overall if it is used to promote development of solar energy. However, we also recognize the potential harms associated with inaccuracies in the model or over prioritization of certain areas over others.\n\n\n3.2 Personal Investment\nFrom a personal perspective, we all are invested in this project because we see its potential to improve the efficiency of new solar developments, and optimize implementations of renewable energy. We are passionate about increasing the amount of renewable energy used in our ecosystem, and view solar energy as one of the most widely applicable sources. We also believe that leveraging the now widely available data on solar irradiance, weather, and other geological features is a sensible step in the right direction for solar development.\nCommon criticisms of solar energy include its high initial cost, variability with weather, and the requirement of land and materials to make use of this technology (Igini 2023). Given these difficulties, we find that leveraging data to find patterns of efficiency in solar power generation has the potential to make solar development a more viable source of clean, renewable energy. Using weather data as a feature in our model allows us to factor in weather variability by location, and more closely study the correlations between certain weather features and photovoltaic output. By determining the optimal locations for solar farms, we can also minimize the impacts of high initial costs and the land requirements, as fewer farms will be needed to achieve the same end results.\n\n\n3.3 Overall Reflection\nBased on this reflection, our technology has the potential to make the world a more sustainable place, as solar power has the potential to replace harmful fossil fuels in the energy ecosystem. This outcome also has the potential to make the world a more equitable place, as developing solar energy has the potential to make energy more available in the long term. While there are potential harms associated with predicting photovoltaic output by location, we find that on balance, the potential benefits are strong, and the harms can be minimized through thorough evaluation of our models.\n\n\n\n4. Materials and Methods\n\n4.1 Data Collection\nThe data used in our study was gathered from a collection of publicly available sources from national and international organizations. For solar irradiance and photovoltaic information, we collected data from the National Renewable Energy Laboratory (NREL) (National Renewable Energy Laboratory 2025) and the Global Solar Atlas (GSA) from The World Bank (The World Bank Group, ESMAP, and Solargis 2019). For weather and elevation statistics, we pulled data from the Copernicus Climate Change Service (C3S) (Hersbach et al. 2023) and the Google Maps API. Outlined below are more detailed descriptions of the specific data sources used in our study:\n\nPhotovoltaic Output (PVO)\nPhotovoltaic output (measured in kWh/kWp \\(-\\) kilowatt-hour per kilowatt-peak) is a quantitative measurement of the amount of producible energy from a solar/PV power system. Specifically, PVO represents the amount of power generated per unit of a given solar energy installation over the long term. Defined in kWh/kWp, PVO describes the energy output in kilowatt-hours of a single PV unit operating at peak performance (according to standard testing conditions) over a designated, long-term period of time. In general, PVO provides a baseline metric for the energy production capacity of a given solar energy system. We collected PVO data from The World Bank’s Global Solar Atlas (The World Bank Group, ESMAP, and Solargis 2019). The PVO dataset we used contains monthly average “practical potential” PVO values from 1999-2018 for a \\(0.0083^\\circ\\)-latitude by \\(0.0083^\\circ\\)-longitude grid (~\\(1\\) km \\(^2\\)) of the entire US. Each potential PVO value is calculated using a multi-step modeling process combining satellite imagery, meteorological data, and PV system simulations developed by Solargis (Solargis, n.d.). For each month of the year (12 total data subsets), each row represents the potential PVO value at a given latitude-longitude coordinate location of the US. The figure below provides a visual representation of the PVO dataset used in our study.\n\n\n\nFigure 1: Visualizing the potential PVO data for the months of January, April, July, and October.\n\n\nRelating to our primary regression objective, we use PVO as the target variable for our predictive models (see sections below for a more detailed discussion). Considering the crucial role that the collected PVO data plays in our modeling approach, the limitations of this dataset should be addressed. Firstly, this dataset contains monthly average values over a ~20 year span, which may disregard or underrepresent historically significant spikes and drops in PVO. Additionally, the values in this dataset are defined in terms of a single PV unit installed at the optimal panel tilt angle. The size of PV units and the optimal panel tilt can vary considerably from installation to installation. Thus, it should be noted that the values in this dataset may be overgeneralizing the projected PV energy yield for certain US regions. Further, the calculated PVO values in this data depend on numerous other relevant solar radiation and meteorological components, which may result in excessive variable correlations in the context of regression models (see sections below for further discussion on this).\n\n\nIrradiance (GHI)\nSolar irradiance (measured in W/m2) quantifies the instantaneous power of sunlight striking a surface. We used Global Horizontal Irradiance (GHI) – the sum of direct beam and diffuse sky radiation on a level plane – as the primary input for photovoltaic (PV) yield models, since PV output scales roughly linearly with incident irradiance (National Renewable Energy Laboratory (2025)). Our GHI data from the U.S. Department of Energy’s National Renewable Energy Laboratory (NREL), published as the Physical Solar Model version 3 (PSM v3) from 1998-2016. This dataset contains monthly and annual GHI averages covering 0.038-degree latitude by 0.038-degree longitude (roughly 4 km by 4 km). This data was produced by merging satellite cloud-detection with radiative transfer clear-sky (REST2) and cloudy-sky (FARMS) models. Each row corresponds to one grid cell’s location, and reports its twelve monthly mean GHI values. There are some limitations to consider upon using this dataset. Firstly, values are monthly means, meaning they omit potential important peaks or lows. Next, panel tilt – finding the optimal angle to capture GHI – is not captured nor discussed by NREL. Lastly, recent climatic trends are not reflected as this data spans only up until 2016; overall weather patterns and irradiance values may have fluctuated since then.\n\n\nWeather\nWe theorized that the climate of a given area may have a substantial impact on the PVO, as factors like cloud coverage and precipitation would influence the amount of sun available to solar farms. We sourced data from ERA5, the fifth iteration of climate reanalysis data from the European Centre for Medium-Range Weather Forecasts (ECMFW), which combines observations with model data to create a physically accurate and consistent climate dataset (Hersbach et al. 2023). We pulled monthly data from 1999 to 2018 for features cvh (high vegetation cover as a fraction of the latitude-longitude grid), sd (snow depth in m of water), msl (sea level pressure in Pa), tcc (cloud cover as a fraction of the latitude-longitude grid), t2m (2 meter temperature K), u100 (100 meter latitudinal wind component in m/s), v100 (100 meter longitudinal wind component m/s), sf (snowfall in m of water), and tp (total precipitation in m of water). We collected this data for latitude and longitude values in the United States in 0.25 degree increments and took the mean over all months and years. We made heatmaps for vegetation cover and and total precipitation to observe how these variables changed with latitude and lonigitude, which are shown in Figures 3 and 4.\n\n\n\nFigure 2: Total Precipitation Heat Map.\n\n\n\n\n\nFigure 3: Vegetation Cover Heat Map.\n\n\nBecause this dataset is from a reputable source and is fairly comprehensive, we can trust that our data is reliable. However, it is not without its limitations, particularly because the changes in latitude and longitude are quite coarse with each latitude-longitude pair corresponding to a 17.25x17.25 mile grid. While we were able to achieve finer measurements for our other data sources, we needed to make them more coarse in order to be consistent with this data (see Combining the Data). The climate variables we selected were chosen based on what we thought would have the largest impact on PVO, but our decisions were not backed by any quantitative evidence. Should we seek to make a more thorough model, we likely would want to select more climate features (many of which are provided by ERA5) and determine through quantitative analysis (like a correlation plot) which features would be most effective.\n\n\nElevation\nLastly, in our data planning, we theorized that elevation may be correlated with photovoltaic output on the assumption that higher elevation would have more potential to capture the sun’s energy. Given the resolution challenges we were having with our other datasets, and the difficulty of finding matching data for elevation, we elected to use the Google Maps Elevation API to retrieve elevation data. With this approach, we can simply make API requests for latitude longitude pairs after all of the other data has been combined. The API accepts latitude and longitude, and returns the elevation of that location, as well as the resolution of the measurement, in meters (Google Developers 2024). The measurement is relative to sea level, so elevation values can be positive or negative. Google acquired this data using NASA’s shuttle radar program (Google Developers 2024)\nThe main limitation affecting this dataset is the variable resolution of the measurements. Areas with higher resolution may more accurately capture terrain features such as small hills or cliffs, while areas with lower resolution may be slightly misrepresented, or smoothed over. As a result, if our model finds that elevation is highly correlated with photovoltaic output, the resolution could be a source of bias in our results. A more practical limitation are the rate limits: we can only make 512 elevation requests at a time, and only a certain number of requests per day. Luckily, this was only a minor issue for our data collection, simply requiring us to write a batch request function to perform 512 requests at a time until we had the complete dataset.\nThe figure below shows the correlation between elevation and photovoltaic output in our final dataset, on a small representative sample.\n\n\n\nFigure 4: Elevation vs Photovoltaic Output.\n\n\nAs the figure shows, there is some positive correlation between elevation and photovoltaic output, but it is a noisy relationship, and much of the elevation data is clustered in the zero to five hundred meter range. As a result, elevation was not the most strongly correlated feature in our dataset, but the outlying areas of high elevation showed strong photovoltaic output.\n\n\n\n4.2 Dataset Compilation & Construction\nWith all of these data sources in place, we began our process of combining the datasets into a single csv file by reading our .tif image files. We wrote conversion functions which would read the data from the .tif files, construct a dataframe containing the relevant rows and columns, then export that dataframe to a csv. At that point, we had constructed the relevant csvs, and then needed to deal with the issue of resolution.\nUnfortunately, because our irradiance data was subject to a lower resolution than our photovoltaic output data, we were forced to downsample the resolution to two decimal places. To accomplish this, we simply created a temporary rounded column, then used an inner join on two dataframes using that rounded column, before dropping the mismatched columns and renaming the rounded column accordingly. This allowed us to combine our two key solar datasets, and though we did lose resolution, we were able to keep all of the relevant data points. However, merging this dataset with the weather dataset was a more complicated issue. If we used the same downsampling technique, we would lose a fair number of our data points, due to a mismatch in the latitude and longitude ranges between the solar and weather datasets. Because these datasets were collected separately from different sources, there were different ranges of latitude and longitude, though there was a great deal of overlap since both datasets pertain to the United States, of course. To account for this difficulty, we used a cKDtree, which allows a nearest-neighbor approach to merging the dataset. Essentially, this structure indexes the latitudes and longitudes in the solar dataset, then queries the weather dataset to find the nearest neighboring points, merging values within a certain range. By taking this approach, we were able to maintain over 400000 data points in our final csv, which is a dataset size that we were very happy to achieve.\nThe last step after this combination was to simply use the batch request function to query the Google Maps API for the relevant latitudes and longitudes, then export the finalized dataframe to a csv for ease of access between notebooks. With the full dataset in place, we constructed a correlation matrix for our features, which shows the strength and positive or negative value of correlation between each feature. The correlation matrix is shown below.\n\n\n\nFigure 5: Correlation Matrix.\n\n\nWhile there are many interesting trends to observe in this correlation matrix, we were mostly concerned with the correlations to photovoltaic output (pvo). The strongest positive correlation was with solar irradiance, which follows our expectations, given that irradiance refers to the strength of the solar beams in a given location. Interestingly, the strongest negative correlation with photovoltaic output was total cloud cover, having even more of an impact than irradiance. This exemplifies one of the primary challenges with solar energy, in that cloud cover blocking the sun can dramatically harm energy output. Other key correlations include elevation, total precipitation, vegetation cover height, and sea level pressure.\n\n\n4.3 Model Design\nWe constructed a sequential neural network using PyTorch to try and predict the PVO based on our collected data. We used three linear layers (decreasing by 2-4 neurons with each layer) with a non-linear activation function between each layer. We experimented with a model where we used ReLU functions and one where we used sigmoid activation functions. We evaluated the loss at each epoch and used the stochastic gradient descent optimizer with a learning rate of 0.01. We determined through trial and error that stochastic gradient descent was a more effective optimizer for this problem than Adam and that a learning rate of 0.01 was sufficient to accurately train the model without overfitting. The code for our LinearModel class is shown below. For the models that we used the sigmoid activation function, we replaced all ReLU() calls with a Sigmoid() call.\n    class LinearModel(nn.Module):\n        \n        def __init__(self, all_feats = False): # \"all_feats\" arg. passed from above\n            \n            # Initialize nn.Module object\n            super().__init__()\n\n            # Matrix alignment depending on if all features are used\n            if all_feats:\n                init_feats = 13\n            else:\n                init_feats = 12\n\n            # Basic linear model pipeline\n            self.pipeline = nn.Sequential(\n                nn.Linear(init_feats, 10),\n                ReLU(),\n                nn.Linear(10, 6),\n                ReLU(),\n                nn.Linear(6,2),\n                ReLU(),\n                nn.Linear(2,1)\n            )\n\n\n4.4 Model Training & Evaluation\nBefore feeding our data to the neural network, we rescaled it using a standard scaling and separated the PVO column into a separate target column. Using SciKit-Learn’s train_test_split() function, we divided the data into a training and testing dataset where 30% of the data was reserved for testing. We batched each dataset using the torch dataloader torch.utils.data.DataLoader() into batches of size 32.\nIn our preliminary data analysis, we found that irradiance had a strong correlation with PVO. To determine how impactful this would be on the accuracy of our model, we trained models both with and without the irradiance feature and compared their testing losses. The init_feats parameter in our model archietecture accounts for this, allowing the model to accept a dataset that may or may not contain the irradiance feature. We thus trained four models: Irradiance included with ReLU activation, Irradiance included with sigmoid activation, Irradiance excluded with ReLU activation, Irradiance excluded with sigmoid activation. We trained each for 100 epochs on the GPUs provided by the Middlebury College Physics Department research computers. The entire code for our model can be found here.\nTo compare our approach to existing models, we trained a linear regression model and polynomial regression model from SciKit-Learn on our data as well. We evaluated the testing loss using mean squared error, allowing us to directly compare the sklearn models to our own. We comapre each of these approaches in the results section.\n\n\n\n5. Results\nUsing a Mean Squared Error loss function, we tracked the progress of each of our models and then used the testing loss after the final epoch to evaluate how well our model performed when predicting PVO. We show the loss plots for our four models below.\n\n\n\nFigure 6: Training and Testing Loss.\n\n\nBased on these plots, it appears that the sigmoid mdoels had a much smoother improvement in loss, but ultimately converged to a slightly higher loss value than the ReLU models. We observe the best testing loss with the Irradiance ReLU model with a MSE of \\(0.0058\\) and the worst testing loss with the No Irradiance Sigmoid model with a MSE of \\(0.0079\\). The table below shows the testing loss for each model.\n\n\n\nModel\nMSE\n\n\n\n\nIrradiance ReLU\n\\(0.0058\\)\n\n\nIrradiance Sigmoid\n\\(0.0065\\)\n\n\nNo Irradiance ReLU\n\\(0.0071\\)\n\n\nNo Irradiance Sigmoid\n\\(0.0079\\)\n\n\nSklearn Linear Model\n\\(0.0104\\)\n\n\nSklearn Third Order Polynomial Model\n\\(0.0050\\)\n\n\n\nEach of our neural network models outperformed the standard SKlearn linear model, meaning the increase in parameters due to deep learning allowed for an improvement in our predictive abilities. However, our best model still underperformed the SKlearn polynomial regression model, meaning there is room for improvement in our deep learning models. We could potentially achieve this by training our models for more epochs, adjusting the parameters in each layer of our model, or implementing non-linear archietecture such as a convolutional neural netowrk to better train to the data. Despite the varying efficacy of these models, all are able to achieve very low loss values relative to the actual PVO values.\nUtilizing the Irradiance ReLU model, we created a map that comprehensively showcases our end results. The map visualizes predicted photovoltaic (PV) output across the United States, displaying both actual and predicted values for various locations. The interactive visualization uses color intensity to represent the predicted solar energy production potential, allowing for comparison between actual and model-predicted values throughout different geographic regions. The map is shown below.\n\n\n\n\n6. Discussion\nWe set out to answer a simple but high-stakes question: Can we predict the potential solar energy output across the United States? By fusing publicly available irradiance, climate, elevation, and historical PV-output data onto a unified 4-km grid, we trained and benchmarked several regression models, ultimately creating a model architecture that was highly accurate nationwide. The predictions are shown through an interactive map that anyone can easily explore. In short, our project turns scattered open data into an actionable siting tool, closing the gap between technical resource assessments and real-world solar deployment decisions.\nIn the end, we achieved the core objectives that we set at the start. Our minimum-viable goal was to turn the open data we collected into a working model that reliably predicts photovoltaic output across the U.S. and present those predictions in an easily digestible format. Despite initially exploring wind and hydro, we discovered that collecting, cleaning, and combining the solar-specific data alone was a substantial undertaking, thus we decided to only focus on solar, while still expressing interest in wind and hydro interest in potential future work (which will be discussed in more detail below).\nCompared with earlier regional studies, our models are competitive – even after scaling the problem to the entire continental United States. Munawar & Wang (Munawar and Wang 2020) reported a 4% normalized RMSE (nRMSE) for short-term PV forecasts in Hawai‘i using XGBoost + PCA, and Jebli et al.(( Jebli et al. 2021) achieved a 6% nRMSE with an ANN in semi-arid Morocco. By contrast, our best nationwide model – the Irradiance-ReLU network – delivers a 2%* nRMSE and cuts mean-squared error by 44% relative to a plain sklearn linear regression baseline (0.0058 vs 0.0104). That said, sklearn’s third-order polynomial regressor edges out the neural network (MSE = 0.0050), reminding us that good feature engineering can sometimes outperform added model depth. In short, we match or surpass the accuracy of the regional studies while covering a far larger, more diverse geography, and we do so with models that remain tractable for potential real-world use.\n**Normalized RMSE was computed by first converting MSE to RMSE, then normalizing by the average PVO (about 4 kWh/kWp).*\n\\[\\text{nRMSE}=\\frac{\\sqrt{\\text{MSE}}}{\\bar{\\text{PVO}}} =\\frac{\\sqrt{0.0058}}{4.0\\ \\text{kWh/kWp}^{-1}}\\approx0.019\\;(\\text{≈ 2 \\%})\\]\nWith additional time, more data, and stronger compute, we would push the project in 3 potential further directions. First, we’d incorporate other renewables – wind and hydro – by sourcing high-resolution wind‐resource and streamflow archives and integrating them into our 4 km grid. This would give much more context for potential renewable energy development, giving a more comprehensive view of which renewable source might be best suited for a certain location. Second, we’d move from a yearly climatology average to more time-specific data, potentially refining our data to utilize daily or hourly measurements. This would allow our models to capture swings in our features, shedding light on potentially crucial intricacies of solar farm developments, such as panel tilt optimization, shading and soil losses, and overall more time-specific weather fluctuations. Third, we’d overlay economic and land-use constraints – site cost, permitting zones, grid capacity – and compute the levelized cost of energy (LCOE) rather than just kWh/kWp. By combining predicted energy yields with spatial cost surfaces and zoning maps, we could pinpoint sites that minimize LCOE or maximize return on investment. On the modeling side, more compute would let us train deeper neural architectures, hypertune our loss functions and activations, and deploy active-learning loops that continually refine forecasts as new utility-scale production data arrive, allowing us to provide more real time predictions. Together, these enhancements would evolve our prototype into a production-grade platform for optimally siting clean energy systems at continental scale.\n\n\n7. Group Contribution Statement\nOverall, we worked together on the writing of this blog post (except, of course, for the personal reflection).\n\n7.1 Omar Armbruster\nOmar worked on collecting and compiling the weather data used in our dataset. Using the weather data, he was able to create some preliminary heatmaps to visualize the features in different regions of the U.S. He also built the foundation of the neural network (including features like model saving and loading), built the data processing pipeline, and wrote the scripts needed to train the model on the physics department computers remotely. He led the writing of the methods section and the weather data section of this blog post and wrote the loss discussion in the results section.\n\n\n7.2 Andrew Dean\nAndrew collected irradiance data from NREL and developed initial visualizations to explore spatial patterns across the U.S. He then collaborated with Col to merge this irradiance data with Photovoltaic Output (PVO) metrics, calculating annual averages for each feature at every coordinate point. To synthesize the results, Andrew produced an interactive U.S. map using plotly, standardizing the input features and applying Omar’s pre-trained neural network to generate predicted PV values. Each point on the map displays both actual and predicted PV outputs, enabling clear visual comparison. He led the writing of the irradiance and discussion sections of this blog post, and contributed to the results section.\n\n\n7.3 Col McDermott\nContributing to the source code, Col collected and cleaned the potential photovoltaic output data from the GSA. He then collaborated with Andrew to merge the GHI and PVO data into a single dataset of solar information, collapsing the combined data into the overall annual average (across all \\(12\\) months) of each feature for every coordinate data point. Additionally, Col worked alongside Andrew and Noah to combine the solar and weather/elevation data into a single curated dataset. Constructing the full dataset involved incorporating a cKDTree from the scipy library to merge datasets with different latitude-longitude resolutions while maintaining a sufficient number of data points and ensuring the absence of any null values. With the full dataset established, Col created some preliminary visualizations to observe potential trends and relationships between various features. Col also assisted with debugging the model design and improving the training scripts. For the project report, Col wrote the abstract and introduction sections (involving research on present-day climate events and some related work in the field) as well as the brief discussion on PVO data collection. In addition to contributing these components, Col worked on organizing and tidying the layout of the full written report.\n\n\n7.4 Noah Price\nNoah began the project working on collecting elevation data for our dataset, which involved establishing a connection to the Google Maps Elevation API using a private key system. He then worked on combining the data from each of our data sources, which involved rescaling the coarseness of the latitude and longitude for some features so that they could be joined with the climate data. With the data in place, he debugged and made improvements to the neural network architecture and infrastructure to optimize the model’s performance and created data visualizations for preliminary data analysis. He led the writing of the values section, combining the data section, and elevation data section of this blog post.\n\n\n\n8. Personal Reflection\nStand alone, this study is only one of several research endeavors I’ve contributed to throughout my academic career in computer science. However, this project has been one of the first pursuits in which I’ve had equally weighted roles in background research, source code development, and communicating our findings. While conducting research on current climate events and related past studies, I developed a more concrete understanding of how extreme global issues (like climate change) are discussed in academic language. In addition to this, examining the related work provided me with more insight into how the data collection → data cleaning → modeling pipeline is typically designed when applying machine learning methods to empirical data at a large scale. While working on the source code, I gained more valuable, hands-on experience in data collection/preprocessing and custom model design with a neural network. Dealing with data, I learned how to handle new types of data formats, such as .tif files, and how to merge geo-spatial datasets with different resolutions into combined datasets (using prebuilt “join” functions and nearest-neighbors data structures). Debugging our model design, I developed a better understanding of the neural network layer “funnel” and how the model architecture must align exactly with the shape of the data. Lastly, in communicating our results, I received more useful practice with expository writing, aiming to provide thorough explanations of our work while not bombarding readers with convoluted, impenetrable text.\nOverall, I am certainly satisfied with the progress my group mates and I achieved with this project. While we did not reach any ground-breaking discoveries, I believe that we provided some small-scale but useful work to an important area of present-day research. I feel that we built a relatively well-organized code repository and wrote a detailed, comprehensive blog post to accompany our technical work. Relating to the completion of our initial goals, I feel that we saw success in our primary aims but did not make as much progress on our extended ambitions. That is, we successfully established a thorough data collection and curation procedure, designed several regression models to predict solar power production, and visualized our results with an intuitive interactive map to provide information about the optimal installation locations for new solar energy systems. However, we did not meet our goals of implementing a cost-analysis model for PV system development (based on our interactive map) and extending our work to address similar guiding questions for other renewable energy solutions (such as hydro and wind power). Despite this, my group mates and I agree that our data collection and curation process took a significant amount of time and effort \\(-\\) compiling large amounts of data from four distinct sources and maintaining an organized file management system for four people to collaborate on \\(-\\) ultimately limiting the scope of our project achievements.\nLastly, as I approach the end of my undergraduate academic career, I can confidently say that the research, coding, and writing experience I’ve gained from this project has had a significant impact on my fundamental intellectual/academic skills. When consulting sources of current events and past research, it was considerably challenging to understand and sift through the technical language. Similarly, while writing about our own study, I found it very difficult at times to put our ideas into words. And, of course, working on the source code came with its own collection of head-scratching implementation problems and debugging difficulties. Yet, these challenges brought nothing less than excellent opportunities to develop each of the aforementioned skills. In the end, beyond the project-specific experience I received from elements like working with torch, merging datasets, reading through complex papers, and formalizing an abstract, I feel that the most important skills developed in this project come from the collaboration with my group mates. Whether brainstorming data cleaning ideas, programming together in .ipynb files (avoiding merge conflicts as best we can), or delegating the write-up process, this project has shown me first-hand the intricacies of working in a team.\n\n\n9. References\n\n\nAlzubaidi, Laith, Jinglan Zhang, Ali J. Humaidi, Abdulmuttalib Al-Dujaili, Yuxuan Duan, Omar Al-Shamma, Juan Santamaría, Mohammed A. Fadhel, Mohammed Al-Amidie, and Laith Farhan. 2022. “Review of Deep Learning: Concepts, CNN Architectures, Challenges, Applications, Future Directions.” Journal of Big Data 9 (1): 1–74. https://doi.org/10.1155/2022/7797488.\n\n\nGoogle Developers. 2024. “Elevation API Overview.” https://developers.google.com/maps/documentation/elevation/overview.\n\n\nHersbach, Hans, B. Bell, P. Berrisford, G. Biavati, A. Horányi, J. Muñoz Sabater, J. Nicolas, et al. 2023. “ERA5 Monthly Averaged Data on Single Levels from 1940 to Present.” Copernicus Climate Change Service (C3S) Climate Data Store (CDS). https://doi.org/10.24381/cds.f17050d7.\n\n\nIgini, Martina. 2023. “What Are the Advantages and Disadvantages of Solar Energy?” https://earth.org/what-are-the-advantages-and-disadvantages-of-solar-energy/.\n\n\nJebli, Imane, Fatima-Zahra Belouadha, Mohammed Issam Kabbaj, and Amine Tilioua. 2021. “Prediction of Solar Energy Guided by Pearson Correlation Using Machine Learning.” Energy 224: 120109. https://doi.org/https://doi.org/10.1016/j.energy.2021.120109.\n\n\nMunawar, Usman, and Zhifang Wang. 2020. “A Framework of Using Machine Learning Approaches for Short-Term Solar Power Forecasting.” Journal of Electrical Engineering & Technology 15 (2): 561–69. https://doi.org/10.1007/s42835-020-00346-4.\n\n\nNational Renewable Energy Laboratory. 2025. “Solar Resource Maps and Data Portal.” https://www.nrel.gov/gis/solar-resource-maps.\n\n\nSolargis. n.d. “Solar Radiation.” https://kb.solargis.com/docs/solar-radiation.\n\n\nThe World Bank Group, ESMAP, and Solargis. 2019. “Global Solar Atlas 2.0: LTAy_AvgDailyTotals (GeoTIFF) – USA Solar Resource Data.” https://globalsolaratlas.info/download/usa.\n\n\nWorld Health Organization. 2023. “Climate Change and Health.” https://www.who.int/news-room/fact-sheets/detail/climate-change-and-health.\n\n\nWorld Wildlife Fund. 2024. “Catastrophic 73% Decline in the Average Size of Global Wildlife Populations in Just 50 Years Reveals a System in Peril.” https://www.worldwildlife.org/press-releases/catastrophic-73-decline-in-the-average-size-of-global-wildlife-populations-in-just-50-years-reveals-a-system-in-peril."
  },
  {
    "objectID": "posts/post_6/index.html",
    "href": "posts/post_6/index.html",
    "title": "Post 6 - Investigating Overfitting, Overparameterization, and Double-Descent",
    "section": "",
    "text": "Classically, the standard approach towards constructing a supervised ML model begins with harnessing a dataset with an arbitrary number of data points each containing a uniform arbitrary number of features. The dataset is then split into at least two subsets: one for training the model and the other for testing the model. The underlying goals of model construction and refinement are to tweak the parameters of the model to produce desirable results for the given problem of focus (such as general classification or regression) with respect to the training data. Finally, the model attempts to generalize the patterns that it identified in the training data to the unseen testing data. Generally, the more complex (i.e. parameterized) a model is designed to be allows it to achieve higher training-data accuracy. To a point, the gains in training-data accuracy (or loss in training-data error) translate to test-data performance, but as a model is made too complex (i.e. overparameterized), it begins to regress in test-data performance. In general, it is believed that after a certain threshold of training-data improvement, the testing data performance will either plateau or continually decrease. Yet, this need not strictly be the case. While overfitting leads to decreasing generalized behavior at first, it can later observed to produce testing results better than the peak performance prior to overfitting: A phenomenon called Double-Descent.\nIn this short study, using a rudimentary, from-scratch linear regression model, I aim to investigate the results of overparameterization and they relate to initial overfitting followed by the double-descent phenomenon. To conduct my analysis of these topics, I will first explore the mathematical properties related to an overprameterized linear regression model. I will then construct an overparamterized model and evaluate its performance on generated, non-linear preprocessed data after applying a feature map. Finally, I will apply my overparamterized model to a more complex dataset of corrupted images and explore how the model performance (measured in ability to predict the number of corruptions in an image) changes with adding more features to the model, with the intention of uncovering an instance of double-descent.\nAs discussed further in the sections below, for the generated, nonlinear data, I found that a more complex model produced a more accurate regression line and lower MSE. Upon evaluating the model performance on the corrupted image dataset, I found that the model’s testing performance did indeed exhibit an instance of double-descent, and the model’s peak testing data performance occurred at a point of model complexity well beyond the interpolation threshold.\nFor more information on my overparameterized linear regression model and random feature map implementation, check out OPLinearRegression.py and RandFeatures.py\n\n\nCode\n# Including all additional imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_sample_images\nfrom matplotlib import pyplot as plt\nfrom scipy.ndimage import zoom\nimport torch as tch\nimport pandas as pd\nimport numpy as np\nimport sys\n\n# Porting over linear regression and feature map implementation\n%load_ext autoreload\n%autoreload 2\nfrom OPLinearRegression import LinearRegression, OPLinearRegressionOptimizer\nfrom RandFeatures import RandomFeatures\ntch.manual_seed(50) # For consistent data generation\nplt.style.use('seaborn-v0_8-whitegrid') # For consistent plotting\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\nPrior to constructing an overparameterized model, it is important to analyze the mathematical properties that prevent or need to be changed in order to allow for overparameterization to exist. When constructing a standard linear regression model, the weights vector \\(\\mathbf{w}\\) is optimized to minimize the standard, unregularized mean-squared-error loss function:\n\\[\nL(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^{n}(\\langle\\mathbf{w}, \\phi(x_i)\\rangle - y_i)^2\n\\]\nNote that \\(\\phi(\\mathbf{X})\\) represents the application of the feature map \\(\\phi\\) to the original feature matrix \\(\\mathbf{X}\\). The optimal weights vector \\(\\mathbf{w}\\) can be solved using \\(\\hat{\\mathbf{w}} = \\text{argmin}_{\\mathbf{w}}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|^2\\). This expression takes the closed form of:\n\\[\n\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\]\nOverparameterization occurs when the number of features in the model exceeds the number of data points. That is, when elements in \\(\\mathbf{w}\\) exceed the number of rows in \\(\\mathbf{X}\\), a model is overparameterized. Note that when this occurs, the above formula for the optimal \\(\\mathbf{w}\\) becomes mathematically impossible when the number of features \\(p\\) is greater than the number of data points \\(n\\) (given that \\(\\mathbf{X}: n \\times p\\)). The reason for this is that when \\(\\mathbf{X}: n \\times p\\) and \\(p &gt; n\\), the matrix \\(\\mathbf{X}\\) is strictly rank-deficient. That is, the columns of \\(\\mathbf{X}\\) cannot be linearly-independent. It follows from this that \\(\\mathbf{X}^T\\mathbf{X}\\) is also rank-deficient because the rank of \\(\\mathbf{X}^T\\mathbf{X}\\) is at most the rank of \\(\\mathbf{X}\\). Thus, \\(\\mathbf{X}^T\\mathbf{X}\\) is a singular matrix in this case and is therefore not invertible (i.e. \\(\\mathbf{X}^T\\mathbf{X})^{-1}\\) is undefined). This ultimately breaks down the above formula used to solve \\(\\mathbf{w}\\).\nHowever, the optimal weights vector \\(\\mathbf{w}\\) can still be found even when a model is overparameterized. The formula for finding the optimal \\(\\mathbf{w}\\) uses the Moore-Penrose pseudoinverse \\(\\mathbf{X}^+\\) of \\(\\mathbf{X}\\):\n\\[\n\\begin{align}\n\\hat{\\mathbf{w}} &= \\mathbf{X}^+\\mathbf{y} \\\\\n\\hat{\\mathbf{w}} &= \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}\n\\end{align}\n\\]\nThe above equation is used to find the optimal \\(\\mathbf{w}\\) for my implementation of an overparameterized linear regression model.\n\n\n\nMy implementation of an overparameterized linear model involves three class definitions: LinearModel, LinearRegression, and OPLinearRegressionOptimizer.\nLinearModel:\n\nself.w: An instance variable to store the weights vector \\(\\mathbf{w}\\) of a linear model.\nscore(X): A method to compute the score \\(s_i\\) for each data point in the feature matrix \\(X\\) using:\n\n\\[\ns_i = \\langle\\mathbf{w}, x_i\\rangle\n\\]\nLinearRegression (inherits from LinearModel):\n\npredict(X): A method that returns the vector of model predictions \\(\\mathbf{y}\\). The model predictions are simply the scores \\(\\mathbf{s}\\) from the linear model.\nloss(X, y): A method to compute the mean-squared-error (MSE) between the model scores \\(\\mathbf{s}\\) and the targets \\(\\mathbf{y}\\). The MSE is computed with (note that \\(\\phi(\\mathbf{X})\\) represents the application of the feature map \\(\\phi\\) to the original feature matrix \\(\\mathbf{X}\\)):\n\n\\[\nL(\\mathbf{w}) = \\mathbf{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(\\langle\\mathbf{w}, \\phi(x_i)\\rangle - y_i)^2\n\\]\nOPLinearRegressionOptimizer:\n\nself.lr: An instance variable of a LinearRegression object.\nfit(X, y): A method that computes the optimal weights vector \\(\\mathbf{w}\\) using the Moore-Penrose pseudoinverse of \\(\\mathbf{X}\\):\n\n\\[\n\\begin{align*}\n\\hat{\\mathbf{w}} &= \\mathbf{X}^+\\mathbf{y} \\\\\n\\hat{\\mathbf{w}} &= \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}\n\\end{align*}\n\\]\n\n\n\nIn order to create overparameterized versions of my linear regression model, I will be using a (slightly edited) random feature map implementation provided by Prof. Chodrow. This feature map implementation involves a single class definition: RandomFeatures. Upon constructing a RandomFeatures object, the number of features (n_features) present after data transformation transformation and the activation function can be manually adjusted. Note that the default activation function is the logistic sigmoid function (\\(1\\)), but this parameter can also be set to the simple squaring function (\\(2\\)):\n\\[\n\\begin{align}\n\\sigma(x) &= \\frac{1}{1 + e^{-x}} \\\\\ny &= x^2\n\\end{align}\n\\]\n\n\n\n\nTo evaluate the correctness and performance of my implementation, the overparameterized linear regressison model is fit to and tested on some basic, nonlinear generated data.\n\n\nCode\n# Generating data - code provided by Prof. Chodrow\nX = tch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = tch.float64)\ny = (X ** 4) - (4 * X) + tch.normal(0, 5, size=X.shape)\n\n# Initializing Linear Regression models\nLinR1 = LinearRegression()\nopt1 = OPLinearRegressionOptimizer(LinR1)\nLinR2 = LinearRegression()\nopt2 = OPLinearRegressionOptimizer(LinR2)\nLinR3 = LinearRegression()\nopt3 = OPLinearRegressionOptimizer(LinR3)\n\n# Applying random feature maps\nphi1 = RandomFeatures(5)\nphi1.fit(X)\nX_map1 = phi1.transform(X)\nopt1.fit(X_map1, y)\ny_hat1 = LinR1.predict(X_map1)\nmse1 = LinR1.loss(X_map1, y)\nphi2 = RandomFeatures(7)\nphi2.fit(X)\nX_map2 = phi2.transform(X)\nopt2.fit(X_map2, y)\ny_hat2 = LinR2.predict(X_map2)\nmse2 = LinR2.loss(X_map2, y)\nphi3 = RandomFeatures(9)\nphi3.fit(X)\nX_map3 = phi3.transform(X)\nopt3.fit(X_map3, y)\ny_hat3 = LinR3.predict(X_map3)\nmse3 = LinR3.loss(X_map3, y)\n\n# Plotting the data and regression lines\nfig, ax = plt.subplots(1, 3, figsize = (15, 7.5))\nax[0].scatter(X, y, color = \"darkcyan\", label = \"Data\", alpha = 0.75)\nax[0].set_xlabel(\"x\", fontsize = 14)\nax[0].set_ylabel(\"y\", fontsize = 14)\nax[0].set_yticks([-20, 0, 20, 40, 60, 80, 100])\nax[0].set_yticklabels([str(i) for i in [-20, 0, 20, 40, 60, 80, 100]], fontsize = 12)\nax[0].set_xticks([-3, -2, -1, 0, 1, 2, 3])\nax[0].set_xticklabels([str(i) for i in [-3, -2, -1, 0, 1, 2, 3]], fontsize = 12)\nax[0].plot(X, y_hat1, color = \"#A46AAE\", linestyle = \"--\")\nax[0].legend([\"Data\", \"Model Predictions\"], fontsize = 12, frameon = True)\nax[0].set_title(f\"5-Parameter Model (MSE = {round(mse1, 3)})\", fontsize = 16)\nax[1].scatter(X, y, color = \"darkcyan\", label = \"Data\", alpha = 0.75)\nax[1].set_xlabel(\"x\", fontsize = 14)\nax[1].set_ylabel(\"y\", fontsize = 14)\nax[1].set_yticks([-20, 0, 20, 40, 60, 80, 100])\nax[1].set_yticklabels([str(i) for i in [-20, 0, 20, 40, 60, 80, 100]], fontsize = 12)\nax[1].set_xticks([-3, -2, -1, 0, 1, 2, 3])\nax[1].set_xticklabels([str(i) for i in [-3, -2, -1, 0, 1, 2, 3]], fontsize = 12)\nax[1].plot(X, y_hat2, color = \"#A46AAE\", linestyle = \"--\")\nax[1].legend([\"Data\", \"Model Predictions\"], frameon = True, fontsize = 12)\nax[1].set_title(f\"7-Parameter Model (MSE = {round(mse2, 3)})\", fontsize = 16)\nax[2].scatter(X, y, color = \"darkcyan\", label = \"Data\", alpha = 0.75)\nax[2].set_xlabel(\"x\", fontsize = 14)\nax[2].set_ylabel(\"y\", fontsize = 14)\nax[2].set_yticks([-20, 0, 20, 40, 60, 80, 100])\nax[2].set_yticklabels([str(i) for i in [-20, 0, 20, 40, 60, 80, 100]], fontsize = 12)\nax[2].set_xticks([-3, -2, -1, 0, 1, 2, 3])\nax[2].set_xticklabels([str(i) for i in [-3, -2, -1, 0, 1, 2, 3]], fontsize = 12)\nax[2].plot(X, y_hat3, color = \"#A46AAE\", linestyle = \"--\")\nax[2].legend([\"Data\", \"Model Predictions\"], frameon = True, fontsize = 12)\nax[2].set_title(f\"9-Parameter Model (MSE = {round(mse3, 3)})\", fontsize = 16)\nfig.suptitle(\"Regression Data and Model Predictions (Regression Lines)\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above generates some 1D regression data, fits a 5-, 7-, and 9-parameter, LinearRegression model to the generated data, and plots the data along with the model predictions (some code provided by Prof. Chodrow).\nFigure 1\nAbove shows the plots of some generated 1D, nonlinear data along with the corresponding regression line for a \\(5\\)-, \\(7\\)-, and \\(9\\)-parameter linear regression model (initialized using feature-map transformed data with the logistic sigmoid activation function). Prior to plotting the model regression lines, the data is transformed with a random feature map of \\(5\\), \\(7\\), and \\(9\\) features. As expected, as the number of parameters per model increases, the corresponding regression line appears to follow the trend of the data more accurately. In support of this, the MSE strictly decreases as the number of model parameters increases in this simple example: the lowest MSE corresponds to the highest-parameter model. This aligns with the visual representation of the \\(9\\)-parameter model yielding the most accurate regression line.\n\n\n\n\n\nTo further evaluate the performance and properties of my implementation, the overperameterized linear regression model is fit and tested on a much more complex data set. In this case, the data is a greyscale pixelated image of a flower (from the sklearn.datasets library). The data, or image rather, will be “corrupted” (where contiguous groups of pixels are greyed out) to varying degrees, and the task of the model with given this data will be to predict the number of “corruptions” found in the given version of the image using just the image alone. Specifically, a random dataset of \\(300\\) corrupted images (including the corresponding number of corrections for each image) is created to fit and test the overparameterized regression model. Below are references to the original, uncorrupted image used in this experiment and an example of a corrupted version of the image:\n\n\nCode\n# Retrieving the image data for this experiment - code provided by Prof. Chodrow\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) # Decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = tch.tensor(X, dtype = tch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\n\nCode above retrieves and displays the image data for this experiment (code provided by Prof. Chodrow).\nImage 1\n\n\nCode\n# Code used to corrupt the image data with random groups of greyed-out pixels - code provided by prof. Chodrow\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = tch.round(mean_patches * tch.rand(1))\n    num_added = 0\n    X = im.clone()\n\n    for _ in tch.arange(num_pixels_to_corrupt.item()): \n        try: \n            x = tch.randint(0, n_pixels[0], (2,))\n            x = tch.randint(0, n_pixels[0], (1,))\n            y = tch.randint(0, n_pixels[1], (1,))\n            s = tch.randint(5, 10, (1,))\n            patch = tch.zeros((s.item(), s.item()), dtype = tch.float64) + 0.5\n\n            # Place patch in base image X\n            X[x: x + s.item(), y: y + s.item()] = patch\n            num_added += 1\n        except: \n            pass\n\n    return X, num_added\n\n# Providing an example of a corrupted version of the image - code provided by Prof. Chodrow\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} Patches\")\noff = plt.gca().axis(\"off\")\n\n# Generating a data set of random corrupted versions of the image - code provided by Prof. Chodrow\nn_samples = 300\nX = tch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = tch.float64)\ny = tch.zeros(n_samples, dtype = tch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n# Reshaping the data for training\nX = X.reshape(n_samples, -1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 50)\n\n\n\n\n\n\n\n\n\nCode above defines a function used to corrupt the image data by randomly creating patches of grey pixels and provides an example of corrupting an image using this function. Code above also generates a data set of 300 random corrupted images and conducts a train-test-split (\\(50\\%\\) each) on the data set(code provided by Prof. Chodrow).\nImage 2\n\n\n\nNow with a complex data set, the overparameterized linear regression model can be fit to and evaluated on its performance. Further, the evolution of the MSE for both the training and testing data can be observed as the number of parameters in the model is increased.\n\n\nCode\n# Assessing model performance across applying a few feature maps to the training data\nnum_feats = [0, 75, 150, 225]\n\n# Evaluating model performance at the 4 different feature map sizes\nfor num_feat in num_feats:\n    phi = RandomFeatures(num_feat, activation = 1)\n    phi.fit(X_train)\n    X_map_tr = phi.transform(X_train)\n    X_map_tst = phi.transform(X_test)\n    LinR = LinearRegression()\n    opt = OPLinearRegressionOptimizer(LinR)\n    opt.fit(X_map_tr, y_train)\n    mse_tr = LinR.loss(X_map_tr, y_train)\n    mse_tst = LinR.loss(X_map_tst, y_test)\n    if num_feat == 150:\n        print(\"----------\")\n        print(f\"Mean Squared Error (Training, {num_feat}-Feature Map Applied | Interpolation Threshold): {round(mse_tr, 3) :.3f}\")\n        print(f\"Mean Squared Error (Testing, {num_feat}-Feature Map Applied | Interpolation Threshold): {round(mse_tst, 3) :.3f}\")\n    else:    \n        print(\"----------\")\n        print(f\"Mean Squared Error (Training, {num_feat}-Feature Map Applied): {round(mse_tr, 3) :.3f}\")\n        print(f\"Mean Squared Error (Testing, {num_feat}-Feature Map Applied): {round(mse_tst, 3) :.3f}\")\n\n\n----------\nMean Squared Error (Training, 0-Feature Map Applied): 2474.420\nMean Squared Error (Testing, 0-Feature Map Applied): 2633.267\n----------\nMean Squared Error (Training, 75-Feature Map Applied): 65.882\nMean Squared Error (Testing, 75-Feature Map Applied): 455.639\n----------\nMean Squared Error (Training, 150-Feature Map Applied | Interpolation Threshold): 0.000\nMean Squared Error (Testing, 150-Feature Map Applied | Interpolation Threshold): 27062.819\n----------\nMean Squared Error (Training, 225-Feature Map Applied): 0.000\nMean Squared Error (Testing, 225-Feature Map Applied): 437.820\n\n\nCode above fits a LinearRegression model to the corrupted image data with several example feature maps applied (using the squaring activation function) and computes the MSE for both the training and testing data.\nThe output above displays the MSE for both the training and testing data when the model is fit to the corrupted image data with a 0-feature, 75-feature, 150-feature (this is the interpolation threshold), and 225-feature feature map applied. As expected, the MSE for the training data is lower than that of the testing data for each model, and the training data MSE strictly decreases. As expected the training data MSE becomes \\(0.0\\) once the model meets and exceeds the interpolation threshold. The testing data MSE is shown to decrease as the training data MSE decreases and before the model complexity meets the interpolation threshold. At the interpolation threshold, the testing data MSE significantly increases, strongly suggesting that the model notably overfits to the training data. However, when the model complexity exceeds the interpolation threshold, the testing data MSE appears to drop back down again. This is indicative of an instance of double-descent, suggesting that the minimal testing data MSE might be produced by an overparameterized model. The figure below investigates this double-descent phenomenon further.\n\n\nCode\n# Fitting many models with increasing feature sizes and assessing the training/testing performance\n## Book keeping arrays for future plotting\nmses_tr = []\nmses_tst = []\nnum_feats = []\nint_thrsh = X_train.size()[0]\nmin_mse_tr = sys.float_info.max\nbst_tr_feats = 0\nmin_mse_tst = sys.float_info.max\nbst_tst_feats = 0\n\n# Fitting models with increasing feature sizes\nfor i in range(301):\n    \n    # Applying the increasingly complex feature maps - using the squaring activation function\n    phi = RandomFeatures(i, activation = 1)\n    phi.fit(X_train)\n    X_map_tr = phi.transform(X_train)\n    X_map_tst = phi.transform(X_test)\n\n    # Initializing/fitting a model\n    LinR = LinearRegression()\n    opt = OPLinearRegressionOptimizer(LinR)\n    opt.fit(X_map_tr, y_train)\n    \n    # Computing the training/testing performance and updating the best training/testing MSE\n    mse_tr = LinR.loss(X_map_tr, y_train)\n    mse_tst = LinR.loss(X_map_tst, y_test)\n    if mse_tr &lt; min_mse_tr:\n        min_mse_tr = mse_tr\n        bst_tr_feats = i\n    if mse_tst &lt; min_mse_tst:\n        min_mse_tst = mse_tst\n        bst_tst_feats = i\n    \n    # Recording current performance\n    mses_tr.append(mse_tr)\n    mses_tst.append(mse_tst)\n    num_feats.append(i)\n\n# Plotting the training/testing performance of each model\nfig, ax = plt.subplots(1, 2, figsize = (12.5, 7.5))\n\n# Training performance\nax[0].scatter(num_feats, mses_tr, color = \"#A46AAE\", alpha = 0.75)\nax[0].scatter(num_feats[bst_tr_feats], min_mse_tr, color = \"black\", label = \"Min MSE\")\nax[0].set_yscale(\"log\")\nax[0].axvline(x = int_thrsh, color = \"black\", linestyle = \"--\", label = \"Interpolation Threshold\")\nax[0].set_xlabel(\"Number of Features\", fontsize = 14)\nax[0].set_ylabel(\"Mean Squared Error (Training)\", fontsize = 14)\nax[0].set_title(\"Training Performance\", fontsize = 16)\nax[0].legend(frameon = True)\n\n# Testing performance\nax[1].scatter(num_feats, mses_tst, color = \"darkcyan\", alpha = 0.75)\nax[1].scatter(num_feats[bst_tst_feats], min_mse_tst, color = \"black\", label = \"Min MSE\")\nax[1].set_yscale(\"log\")\nax[1].axvline(x = int_thrsh, color = \"black\", linestyle = \"--\", label = \"Interpolation Threshold\")\nax[1].set_xlabel(\"Number of Features\", fontsize = 14)\nax[1].set_ylabel(\"Mean Squared Error (Testing)\", fontsize = 14)\nax[1].set_title(\"Testing Performance\", fontsize = 16)\nax[1].legend(frameon = True)\n\nfig.suptitle(\"Training and Testing Performance on Corrupted Image Data\\nWith Increasing Model Complexity (0-300 Features)\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above evaluates and stores the performance of the overparameterized linear regression model on the training and testing corrupted image data as the number of model features is increased from \\([0, 300]\\). Code then plots the respective training and testing performances for each model, displaying the interpolation threshold and identifying the model/feature number corresponding to the minimum training and testing MSE (some code suggested by Prof. Chodrow).\nFigure 2\nThe plots above illustrate the evolution of the MSE for both the training and testing data as the overparameterized linear regression model complexity (i.e. number of features) increases. Prior to exceeding the interpolation threshold, the training data MSE appears to steadily drop, which is expected as the model is gradually becoming more complex. At the same time, the testing data MSE begins to drop, but soon starts to increase as the model complexity approaches the interpolation threshold. This is also expected as the model is beginning to overfit to the training data more. Once the number of features in the model exceeds the interpolation threshold, the model begins to notably overfit to the training data, ultimately bringing the training data MSE down to essentially \\(0.0\\). Aligning with this, the testing data MSE seems to be maximized when the model complexity is very close or equal to the interpolation threshold, indicating where the negative effects of training-data overfitting are most pronounced. However, as the model complexity continues past the interpolation threshold, the testing data MSE begins to decrease again. This is an instance of the double-descent phenomenon.\n\n\nCode\n# Displaying the peak testing performance and corresponding number of features\nprint(f\"Minimum MSE for Testing Data: {round(min_mse_tst, 3)} | Number of Features: {bst_tst_feats} (Interpolation Threshold: {int_thrsh})\")\nprint(f\"Minimum MSE for Training Data: {round(min_mse_tr, 3)} | Number of Features: {bst_tr_feats} (Interpolation Threshold: {int_thrsh})\")\n\n\nMinimum MSE for Testing Data: 252.059 | Number of Features: 257 (Interpolation Threshold: 150)\nMinimum MSE for Training Data: 0.0 | Number of Features: 282 (Interpolation Threshold: 150)\n\n\nCode above displays the minimum MSE and the number of features in the corresponding model for both the training and testing corrupted image data. The numbers of features in the corresponding models are compared to the interpolation threshold.\nThe output above depicts the minimum (best) MSE values for the training and testing data along with the number of features in the corresponding models and a comparison to the interpolation threshold. As expected, the minimum training data MSE is \\(0.0\\) and occurs when the model has \\(282\\) features; which is greater than the interpolation threshold at \\(150\\). The minimum testing data MSE occurs when the model has \\(257\\) features; which is also greater than the interpolation threshold at \\(150\\). Interestingly, the best training and testing performances are produced by models with distinctly different complexities. Yet overall, the information depicted above aligns with the double-descent occurrence shown in the previous plots."
  },
  {
    "objectID": "posts/post_6/index.html#implementing-an-overparameterized-linear-regression-model",
    "href": "posts/post_6/index.html#implementing-an-overparameterized-linear-regression-model",
    "title": "Post 6 - Investigating Overfitting, Overparameterization, and Double-Descent",
    "section": "",
    "text": "Prior to constructing an overparameterized model, it is important to analyze the mathematical properties that prevent or need to be changed in order to allow for overparameterization to exist. When constructing a standard linear regression model, the weights vector \\(\\mathbf{w}\\) is optimized to minimize the standard, unregularized mean-squared-error loss function:\n\\[\nL(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^{n}(\\langle\\mathbf{w}, \\phi(x_i)\\rangle - y_i)^2\n\\]\nNote that \\(\\phi(\\mathbf{X})\\) represents the application of the feature map \\(\\phi\\) to the original feature matrix \\(\\mathbf{X}\\). The optimal weights vector \\(\\mathbf{w}\\) can be solved using \\(\\hat{\\mathbf{w}} = \\text{argmin}_{\\mathbf{w}}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|^2\\). This expression takes the closed form of:\n\\[\n\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\]\nOverparameterization occurs when the number of features in the model exceeds the number of data points. That is, when elements in \\(\\mathbf{w}\\) exceed the number of rows in \\(\\mathbf{X}\\), a model is overparameterized. Note that when this occurs, the above formula for the optimal \\(\\mathbf{w}\\) becomes mathematically impossible when the number of features \\(p\\) is greater than the number of data points \\(n\\) (given that \\(\\mathbf{X}: n \\times p\\)). The reason for this is that when \\(\\mathbf{X}: n \\times p\\) and \\(p &gt; n\\), the matrix \\(\\mathbf{X}\\) is strictly rank-deficient. That is, the columns of \\(\\mathbf{X}\\) cannot be linearly-independent. It follows from this that \\(\\mathbf{X}^T\\mathbf{X}\\) is also rank-deficient because the rank of \\(\\mathbf{X}^T\\mathbf{X}\\) is at most the rank of \\(\\mathbf{X}\\). Thus, \\(\\mathbf{X}^T\\mathbf{X}\\) is a singular matrix in this case and is therefore not invertible (i.e. \\(\\mathbf{X}^T\\mathbf{X})^{-1}\\) is undefined). This ultimately breaks down the above formula used to solve \\(\\mathbf{w}\\).\nHowever, the optimal weights vector \\(\\mathbf{w}\\) can still be found even when a model is overparameterized. The formula for finding the optimal \\(\\mathbf{w}\\) uses the Moore-Penrose pseudoinverse \\(\\mathbf{X}^+\\) of \\(\\mathbf{X}\\):\n\\[\n\\begin{align}\n\\hat{\\mathbf{w}} &= \\mathbf{X}^+\\mathbf{y} \\\\\n\\hat{\\mathbf{w}} &= \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}\n\\end{align}\n\\]\nThe above equation is used to find the optimal \\(\\mathbf{w}\\) for my implementation of an overparameterized linear regression model.\n\n\n\nMy implementation of an overparameterized linear model involves three class definitions: LinearModel, LinearRegression, and OPLinearRegressionOptimizer.\nLinearModel:\n\nself.w: An instance variable to store the weights vector \\(\\mathbf{w}\\) of a linear model.\nscore(X): A method to compute the score \\(s_i\\) for each data point in the feature matrix \\(X\\) using:\n\n\\[\ns_i = \\langle\\mathbf{w}, x_i\\rangle\n\\]\nLinearRegression (inherits from LinearModel):\n\npredict(X): A method that returns the vector of model predictions \\(\\mathbf{y}\\). The model predictions are simply the scores \\(\\mathbf{s}\\) from the linear model.\nloss(X, y): A method to compute the mean-squared-error (MSE) between the model scores \\(\\mathbf{s}\\) and the targets \\(\\mathbf{y}\\). The MSE is computed with (note that \\(\\phi(\\mathbf{X})\\) represents the application of the feature map \\(\\phi\\) to the original feature matrix \\(\\mathbf{X}\\)):\n\n\\[\nL(\\mathbf{w}) = \\mathbf{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(\\langle\\mathbf{w}, \\phi(x_i)\\rangle - y_i)^2\n\\]\nOPLinearRegressionOptimizer:\n\nself.lr: An instance variable of a LinearRegression object.\nfit(X, y): A method that computes the optimal weights vector \\(\\mathbf{w}\\) using the Moore-Penrose pseudoinverse of \\(\\mathbf{X}\\):\n\n\\[\n\\begin{align*}\n\\hat{\\mathbf{w}} &= \\mathbf{X}^+\\mathbf{y} \\\\\n\\hat{\\mathbf{w}} &= \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}\n\\end{align*}\n\\]\n\n\n\nIn order to create overparameterized versions of my linear regression model, I will be using a (slightly edited) random feature map implementation provided by Prof. Chodrow. This feature map implementation involves a single class definition: RandomFeatures. Upon constructing a RandomFeatures object, the number of features (n_features) present after data transformation transformation and the activation function can be manually adjusted. Note that the default activation function is the logistic sigmoid function (\\(1\\)), but this parameter can also be set to the simple squaring function (\\(2\\)):\n\\[\n\\begin{align}\n\\sigma(x) &= \\frac{1}{1 + e^{-x}} \\\\\ny &= x^2\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/post_6/index.html#testing-the-overparameterized-linear-regression-model-on-simple-generated-data",
    "href": "posts/post_6/index.html#testing-the-overparameterized-linear-regression-model-on-simple-generated-data",
    "title": "Post 6 - Investigating Overfitting, Overparameterization, and Double-Descent",
    "section": "",
    "text": "To evaluate the correctness and performance of my implementation, the overparameterized linear regressison model is fit to and tested on some basic, nonlinear generated data.\n\n\nCode\n# Generating data - code provided by Prof. Chodrow\nX = tch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = tch.float64)\ny = (X ** 4) - (4 * X) + tch.normal(0, 5, size=X.shape)\n\n# Initializing Linear Regression models\nLinR1 = LinearRegression()\nopt1 = OPLinearRegressionOptimizer(LinR1)\nLinR2 = LinearRegression()\nopt2 = OPLinearRegressionOptimizer(LinR2)\nLinR3 = LinearRegression()\nopt3 = OPLinearRegressionOptimizer(LinR3)\n\n# Applying random feature maps\nphi1 = RandomFeatures(5)\nphi1.fit(X)\nX_map1 = phi1.transform(X)\nopt1.fit(X_map1, y)\ny_hat1 = LinR1.predict(X_map1)\nmse1 = LinR1.loss(X_map1, y)\nphi2 = RandomFeatures(7)\nphi2.fit(X)\nX_map2 = phi2.transform(X)\nopt2.fit(X_map2, y)\ny_hat2 = LinR2.predict(X_map2)\nmse2 = LinR2.loss(X_map2, y)\nphi3 = RandomFeatures(9)\nphi3.fit(X)\nX_map3 = phi3.transform(X)\nopt3.fit(X_map3, y)\ny_hat3 = LinR3.predict(X_map3)\nmse3 = LinR3.loss(X_map3, y)\n\n# Plotting the data and regression lines\nfig, ax = plt.subplots(1, 3, figsize = (15, 7.5))\nax[0].scatter(X, y, color = \"darkcyan\", label = \"Data\", alpha = 0.75)\nax[0].set_xlabel(\"x\", fontsize = 14)\nax[0].set_ylabel(\"y\", fontsize = 14)\nax[0].set_yticks([-20, 0, 20, 40, 60, 80, 100])\nax[0].set_yticklabels([str(i) for i in [-20, 0, 20, 40, 60, 80, 100]], fontsize = 12)\nax[0].set_xticks([-3, -2, -1, 0, 1, 2, 3])\nax[0].set_xticklabels([str(i) for i in [-3, -2, -1, 0, 1, 2, 3]], fontsize = 12)\nax[0].plot(X, y_hat1, color = \"#A46AAE\", linestyle = \"--\")\nax[0].legend([\"Data\", \"Model Predictions\"], fontsize = 12, frameon = True)\nax[0].set_title(f\"5-Parameter Model (MSE = {round(mse1, 3)})\", fontsize = 16)\nax[1].scatter(X, y, color = \"darkcyan\", label = \"Data\", alpha = 0.75)\nax[1].set_xlabel(\"x\", fontsize = 14)\nax[1].set_ylabel(\"y\", fontsize = 14)\nax[1].set_yticks([-20, 0, 20, 40, 60, 80, 100])\nax[1].set_yticklabels([str(i) for i in [-20, 0, 20, 40, 60, 80, 100]], fontsize = 12)\nax[1].set_xticks([-3, -2, -1, 0, 1, 2, 3])\nax[1].set_xticklabels([str(i) for i in [-3, -2, -1, 0, 1, 2, 3]], fontsize = 12)\nax[1].plot(X, y_hat2, color = \"#A46AAE\", linestyle = \"--\")\nax[1].legend([\"Data\", \"Model Predictions\"], frameon = True, fontsize = 12)\nax[1].set_title(f\"7-Parameter Model (MSE = {round(mse2, 3)})\", fontsize = 16)\nax[2].scatter(X, y, color = \"darkcyan\", label = \"Data\", alpha = 0.75)\nax[2].set_xlabel(\"x\", fontsize = 14)\nax[2].set_ylabel(\"y\", fontsize = 14)\nax[2].set_yticks([-20, 0, 20, 40, 60, 80, 100])\nax[2].set_yticklabels([str(i) for i in [-20, 0, 20, 40, 60, 80, 100]], fontsize = 12)\nax[2].set_xticks([-3, -2, -1, 0, 1, 2, 3])\nax[2].set_xticklabels([str(i) for i in [-3, -2, -1, 0, 1, 2, 3]], fontsize = 12)\nax[2].plot(X, y_hat3, color = \"#A46AAE\", linestyle = \"--\")\nax[2].legend([\"Data\", \"Model Predictions\"], frameon = True, fontsize = 12)\nax[2].set_title(f\"9-Parameter Model (MSE = {round(mse3, 3)})\", fontsize = 16)\nfig.suptitle(\"Regression Data and Model Predictions (Regression Lines)\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above generates some 1D regression data, fits a 5-, 7-, and 9-parameter, LinearRegression model to the generated data, and plots the data along with the model predictions (some code provided by Prof. Chodrow).\nFigure 1\nAbove shows the plots of some generated 1D, nonlinear data along with the corresponding regression line for a \\(5\\)-, \\(7\\)-, and \\(9\\)-parameter linear regression model (initialized using feature-map transformed data with the logistic sigmoid activation function). Prior to plotting the model regression lines, the data is transformed with a random feature map of \\(5\\), \\(7\\), and \\(9\\) features. As expected, as the number of parameters per model increases, the corresponding regression line appears to follow the trend of the data more accurately. In support of this, the MSE strictly decreases as the number of model parameters increases in this simple example: the lowest MSE corresponds to the highest-parameter model. This aligns with the visual representation of the \\(9\\)-parameter model yielding the most accurate regression line."
  },
  {
    "objectID": "posts/post_6/index.html#testing-the-overparameterized-linear-regression-model-on-a-more-complex-dataset---observing-double-descent",
    "href": "posts/post_6/index.html#testing-the-overparameterized-linear-regression-model-on-a-more-complex-dataset---observing-double-descent",
    "title": "Post 6 - Investigating Overfitting, Overparameterization, and Double-Descent",
    "section": "",
    "text": "To further evaluate the performance and properties of my implementation, the overperameterized linear regression model is fit and tested on a much more complex data set. In this case, the data is a greyscale pixelated image of a flower (from the sklearn.datasets library). The data, or image rather, will be “corrupted” (where contiguous groups of pixels are greyed out) to varying degrees, and the task of the model with given this data will be to predict the number of “corruptions” found in the given version of the image using just the image alone. Specifically, a random dataset of \\(300\\) corrupted images (including the corresponding number of corrections for each image) is created to fit and test the overparameterized regression model. Below are references to the original, uncorrupted image used in this experiment and an example of a corrupted version of the image:\n\n\nCode\n# Retrieving the image data for this experiment - code provided by Prof. Chodrow\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) # Decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = tch.tensor(X, dtype = tch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\n\nCode above retrieves and displays the image data for this experiment (code provided by Prof. Chodrow).\nImage 1\n\n\nCode\n# Code used to corrupt the image data with random groups of greyed-out pixels - code provided by prof. Chodrow\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = tch.round(mean_patches * tch.rand(1))\n    num_added = 0\n    X = im.clone()\n\n    for _ in tch.arange(num_pixels_to_corrupt.item()): \n        try: \n            x = tch.randint(0, n_pixels[0], (2,))\n            x = tch.randint(0, n_pixels[0], (1,))\n            y = tch.randint(0, n_pixels[1], (1,))\n            s = tch.randint(5, 10, (1,))\n            patch = tch.zeros((s.item(), s.item()), dtype = tch.float64) + 0.5\n\n            # Place patch in base image X\n            X[x: x + s.item(), y: y + s.item()] = patch\n            num_added += 1\n        except: \n            pass\n\n    return X, num_added\n\n# Providing an example of a corrupted version of the image - code provided by Prof. Chodrow\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} Patches\")\noff = plt.gca().axis(\"off\")\n\n# Generating a data set of random corrupted versions of the image - code provided by Prof. Chodrow\nn_samples = 300\nX = tch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = tch.float64)\ny = tch.zeros(n_samples, dtype = tch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n# Reshaping the data for training\nX = X.reshape(n_samples, -1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 50)\n\n\n\n\n\n\n\n\n\nCode above defines a function used to corrupt the image data by randomly creating patches of grey pixels and provides an example of corrupting an image using this function. Code above also generates a data set of 300 random corrupted images and conducts a train-test-split (\\(50\\%\\) each) on the data set(code provided by Prof. Chodrow).\nImage 2\n\n\n\nNow with a complex data set, the overparameterized linear regression model can be fit to and evaluated on its performance. Further, the evolution of the MSE for both the training and testing data can be observed as the number of parameters in the model is increased.\n\n\nCode\n# Assessing model performance across applying a few feature maps to the training data\nnum_feats = [0, 75, 150, 225]\n\n# Evaluating model performance at the 4 different feature map sizes\nfor num_feat in num_feats:\n    phi = RandomFeatures(num_feat, activation = 1)\n    phi.fit(X_train)\n    X_map_tr = phi.transform(X_train)\n    X_map_tst = phi.transform(X_test)\n    LinR = LinearRegression()\n    opt = OPLinearRegressionOptimizer(LinR)\n    opt.fit(X_map_tr, y_train)\n    mse_tr = LinR.loss(X_map_tr, y_train)\n    mse_tst = LinR.loss(X_map_tst, y_test)\n    if num_feat == 150:\n        print(\"----------\")\n        print(f\"Mean Squared Error (Training, {num_feat}-Feature Map Applied | Interpolation Threshold): {round(mse_tr, 3) :.3f}\")\n        print(f\"Mean Squared Error (Testing, {num_feat}-Feature Map Applied | Interpolation Threshold): {round(mse_tst, 3) :.3f}\")\n    else:    \n        print(\"----------\")\n        print(f\"Mean Squared Error (Training, {num_feat}-Feature Map Applied): {round(mse_tr, 3) :.3f}\")\n        print(f\"Mean Squared Error (Testing, {num_feat}-Feature Map Applied): {round(mse_tst, 3) :.3f}\")\n\n\n----------\nMean Squared Error (Training, 0-Feature Map Applied): 2474.420\nMean Squared Error (Testing, 0-Feature Map Applied): 2633.267\n----------\nMean Squared Error (Training, 75-Feature Map Applied): 65.882\nMean Squared Error (Testing, 75-Feature Map Applied): 455.639\n----------\nMean Squared Error (Training, 150-Feature Map Applied | Interpolation Threshold): 0.000\nMean Squared Error (Testing, 150-Feature Map Applied | Interpolation Threshold): 27062.819\n----------\nMean Squared Error (Training, 225-Feature Map Applied): 0.000\nMean Squared Error (Testing, 225-Feature Map Applied): 437.820\n\n\nCode above fits a LinearRegression model to the corrupted image data with several example feature maps applied (using the squaring activation function) and computes the MSE for both the training and testing data.\nThe output above displays the MSE for both the training and testing data when the model is fit to the corrupted image data with a 0-feature, 75-feature, 150-feature (this is the interpolation threshold), and 225-feature feature map applied. As expected, the MSE for the training data is lower than that of the testing data for each model, and the training data MSE strictly decreases. As expected the training data MSE becomes \\(0.0\\) once the model meets and exceeds the interpolation threshold. The testing data MSE is shown to decrease as the training data MSE decreases and before the model complexity meets the interpolation threshold. At the interpolation threshold, the testing data MSE significantly increases, strongly suggesting that the model notably overfits to the training data. However, when the model complexity exceeds the interpolation threshold, the testing data MSE appears to drop back down again. This is indicative of an instance of double-descent, suggesting that the minimal testing data MSE might be produced by an overparameterized model. The figure below investigates this double-descent phenomenon further.\n\n\nCode\n# Fitting many models with increasing feature sizes and assessing the training/testing performance\n## Book keeping arrays for future plotting\nmses_tr = []\nmses_tst = []\nnum_feats = []\nint_thrsh = X_train.size()[0]\nmin_mse_tr = sys.float_info.max\nbst_tr_feats = 0\nmin_mse_tst = sys.float_info.max\nbst_tst_feats = 0\n\n# Fitting models with increasing feature sizes\nfor i in range(301):\n    \n    # Applying the increasingly complex feature maps - using the squaring activation function\n    phi = RandomFeatures(i, activation = 1)\n    phi.fit(X_train)\n    X_map_tr = phi.transform(X_train)\n    X_map_tst = phi.transform(X_test)\n\n    # Initializing/fitting a model\n    LinR = LinearRegression()\n    opt = OPLinearRegressionOptimizer(LinR)\n    opt.fit(X_map_tr, y_train)\n    \n    # Computing the training/testing performance and updating the best training/testing MSE\n    mse_tr = LinR.loss(X_map_tr, y_train)\n    mse_tst = LinR.loss(X_map_tst, y_test)\n    if mse_tr &lt; min_mse_tr:\n        min_mse_tr = mse_tr\n        bst_tr_feats = i\n    if mse_tst &lt; min_mse_tst:\n        min_mse_tst = mse_tst\n        bst_tst_feats = i\n    \n    # Recording current performance\n    mses_tr.append(mse_tr)\n    mses_tst.append(mse_tst)\n    num_feats.append(i)\n\n# Plotting the training/testing performance of each model\nfig, ax = plt.subplots(1, 2, figsize = (12.5, 7.5))\n\n# Training performance\nax[0].scatter(num_feats, mses_tr, color = \"#A46AAE\", alpha = 0.75)\nax[0].scatter(num_feats[bst_tr_feats], min_mse_tr, color = \"black\", label = \"Min MSE\")\nax[0].set_yscale(\"log\")\nax[0].axvline(x = int_thrsh, color = \"black\", linestyle = \"--\", label = \"Interpolation Threshold\")\nax[0].set_xlabel(\"Number of Features\", fontsize = 14)\nax[0].set_ylabel(\"Mean Squared Error (Training)\", fontsize = 14)\nax[0].set_title(\"Training Performance\", fontsize = 16)\nax[0].legend(frameon = True)\n\n# Testing performance\nax[1].scatter(num_feats, mses_tst, color = \"darkcyan\", alpha = 0.75)\nax[1].scatter(num_feats[bst_tst_feats], min_mse_tst, color = \"black\", label = \"Min MSE\")\nax[1].set_yscale(\"log\")\nax[1].axvline(x = int_thrsh, color = \"black\", linestyle = \"--\", label = \"Interpolation Threshold\")\nax[1].set_xlabel(\"Number of Features\", fontsize = 14)\nax[1].set_ylabel(\"Mean Squared Error (Testing)\", fontsize = 14)\nax[1].set_title(\"Testing Performance\", fontsize = 16)\nax[1].legend(frameon = True)\n\nfig.suptitle(\"Training and Testing Performance on Corrupted Image Data\\nWith Increasing Model Complexity (0-300 Features)\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above evaluates and stores the performance of the overparameterized linear regression model on the training and testing corrupted image data as the number of model features is increased from \\([0, 300]\\). Code then plots the respective training and testing performances for each model, displaying the interpolation threshold and identifying the model/feature number corresponding to the minimum training and testing MSE (some code suggested by Prof. Chodrow).\nFigure 2\nThe plots above illustrate the evolution of the MSE for both the training and testing data as the overparameterized linear regression model complexity (i.e. number of features) increases. Prior to exceeding the interpolation threshold, the training data MSE appears to steadily drop, which is expected as the model is gradually becoming more complex. At the same time, the testing data MSE begins to drop, but soon starts to increase as the model complexity approaches the interpolation threshold. This is also expected as the model is beginning to overfit to the training data more. Once the number of features in the model exceeds the interpolation threshold, the model begins to notably overfit to the training data, ultimately bringing the training data MSE down to essentially \\(0.0\\). Aligning with this, the testing data MSE seems to be maximized when the model complexity is very close or equal to the interpolation threshold, indicating where the negative effects of training-data overfitting are most pronounced. However, as the model complexity continues past the interpolation threshold, the testing data MSE begins to decrease again. This is an instance of the double-descent phenomenon.\n\n\nCode\n# Displaying the peak testing performance and corresponding number of features\nprint(f\"Minimum MSE for Testing Data: {round(min_mse_tst, 3)} | Number of Features: {bst_tst_feats} (Interpolation Threshold: {int_thrsh})\")\nprint(f\"Minimum MSE for Training Data: {round(min_mse_tr, 3)} | Number of Features: {bst_tr_feats} (Interpolation Threshold: {int_thrsh})\")\n\n\nMinimum MSE for Testing Data: 252.059 | Number of Features: 257 (Interpolation Threshold: 150)\nMinimum MSE for Training Data: 0.0 | Number of Features: 282 (Interpolation Threshold: 150)\n\n\nCode above displays the minimum MSE and the number of features in the corresponding model for both the training and testing corrupted image data. The numbers of features in the corresponding models are compared to the interpolation threshold.\nThe output above depicts the minimum (best) MSE values for the training and testing data along with the number of features in the corresponding models and a comparison to the interpolation threshold. As expected, the minimum training data MSE is \\(0.0\\) and occurs when the model has \\(282\\) features; which is greater than the interpolation threshold at \\(150\\). The minimum testing data MSE occurs when the model has \\(257\\) features; which is also greater than the interpolation threshold at \\(150\\). Interestingly, the best training and testing performances are produced by models with distinctly different complexities. Yet overall, the information depicted above aligns with the double-descent occurrence shown in the previous plots."
  },
  {
    "objectID": "posts/post_7/index.html",
    "href": "posts/post_7/index.html",
    "title": "Post 7 - Exploring Advanced Optimization Methods",
    "section": "",
    "text": "In a previous short study, I implemented a basic logistic regression model using two rudimentary versions of gradient descent: standard gradient descent and gradient descent with momentum. The goals of this past study were to investigate the more basic, fundamental procedures under-the-hood of logistic regression. Building on my previous work, in this brief study, I will explore more advanced model optimization methods, apply these methods to my preexisting logistic regression implementation, and evaluate/compare their performances. The two advanced optimization methods I will investigate are Newton’s Method and Adam.\nOne of the most crucial components of model construction of many ML tools and algorithms (especially linear models) is effective, fast optimization. Typically, real-world applications of ML models involve massive data sets often containing complex trends and relationships. Thus, effective model training and fitting can require significant computational resources and time. Establishing the most efficient (as appropriate) optimization methods is critical in designing practical, functional, and generalizable ML models. It should be noted that the versions of the optimization methods examined in this study primarily apply to linear models (hence the revisiting of logistic regression), yet the underlying concept of advanced, fast optimization extends to highly complex, non-linear models and algorithms as well.\nTo explore and compare Newton’s Method and Adam for optimizing a logistic regression model, I have implemented each advanced procedure in my previous logistic regression implementation. To evaluate the performance and unique properties of each optimization method, I have conducted the following experiments:\nExperiments with Newton’s Method:\n\nDisplaying the convergence to the correct choice of the weights vector \\(\\mathbf{w}\\) with an appropriately selected learning rate \\(\\alpha\\).\nComparing convergence rates, through the lens of empirical risk minimization, with my previously implemented basic gradient descent methods\nInvestigating the limitation of convergence when the learning rate \\(\\alpha\\) is set too large.\n\nExperiments with Adam:\n\nDisplaying the convergence to the correct choice of the weights vector \\(\\mathbf{w} -\\) General convergence testing.\nComparing convergence rates, through the lens of empirical risk minimization, with standard minibatch stochastic gradient descent using a fixed batch size and altering the step size \\(\\alpha\\).\n\nFollowing the experiments outlined above, I have compared the performance of Newton’s Method and Adam. Since these techniques employ considerably different computational steps, the comparison of these two advanced optimization methods is conducted as a convergence rate analysis with respect to runtime.\nFor the implementation and documentation of the advanced optimization methods outlined above, check out advanced_logistic.py.\n\n\nIn order to test, evaluate, compare, and experiment with the advanced optimization methods explored in this study, it is necessary to have some binary classification data. For this analysis, I will be using the same generated data set from my previous study on logistic regression and the Palmer Penguins data set from my first study on classification. To briefly recap, the first data set is randomly generated 2D binary classification data used to test the correctness of the implementation for each advanced optimization method and the second data set is the widely used penguins classification data set from the Palmer Station collected by Dr. Kristen Gorman. Rather than trying to classify penguin species, I opted to have my models classify penguin sex (which in this data is a binary classification task). The penguins data is preprocessed by subsetting out only the numerical-value columns and standardizing each data point (using \\(\\frac{\\mathbf{x} - \\mu}{\\sigma}\\) where \\(\\mu, \\sigma\\) are the mean and STD. of the data).\n\n\nCode\n# Including all additional imports\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom pdf2image import convert_from_path\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image\nimport torch as tch\nimport pandas as pd\nimport numpy as np\nimport time\n\n# Porting over logistic regression implementation\n%load_ext autoreload\n%autoreload 2\nfrom advanced_logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer\ntch.manual_seed(100) # For consistent data generation\nplt.style.use('seaborn-v0_8-whitegrid') # For consistent plotting\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nCode above imports all necessary packages/libraries and ports over the implementation of my logistic regression model and the advanced optimizers NewtonOptimizer and AdamOptimizer.\n\n\nCode\n# Generating data for binary classification - code provided by Prof. Chodrow\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    y = tch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + tch.normal(0.0, noise, size = (n_points,p_dims))\n    X = tch.cat((X, tch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\nX_sim, y_sim = classification_data(noise = 0.25)\n\n# Accessing penguins data - data and (edited) method provided by Prof. Chodrow\ndef prepare_data(df):\n  \n  # Preprocessing data\n  le = LabelEncoder()\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  le.fit(train[\"Sex\"])\n  df = df.dropna()\n  y = le.transform(df[\"Sex\"])\n  df = df.drop([\"Sex\", \"Species\", \"Island\", \"Stage\", \"Clutch Completion\"], axis = 1)\n  \n  # Converting to torch tensors\n  X = tch.tensor(df.values).float()\n  y_ = tch.tensor(y == 1).float()\n\n  # Standardizing the data\n  mean = tch.mean(X, dim = 0, keepdim = True)\n  std = tch.std(X, dim = 0, keepdim = True)\n  X_s = (X - mean) / std\n  \n  # Adding a col of 1s to feature matrix\n  X_s = tch.cat((X_s, tch.ones(X_s.size(0), 1)), dim=1)\n  return X_s, y_\n\ntrain = pd.read_csv(\"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\")\nX_train, y_train = prepare_data(train)\n\n\nCode above generates the simulated data (generated with fairly low-noise to ensure linear separability) and imports, preprocess (feature subsetting and standardization), the empirical (real-world) data (some code provided by Prof. Chodrow).\n\n\nCode\n# Model interpretation helper methods\n## Loss value plotter\ndef loss_plot(ax, loss_vec1, loss_vec2 = None):\n    # Plotting the loss values of the model across each optimization iteration\n    ax.plot(loss_vec1, color = \"#A46AAE\", linewidth = 2)\n    title = \"Evolution of Empirical Loss Value\"\n    if (loss_vec2 != None):\n        title = \"Gradient Descent Method Comparison\\nof Empirical Loss Value Convergence\"\n        ax.plot(loss_vec2, color = \"darkcyan\", linewidth = 2)\n        ax.legend([\"Standard\", \"Momentum\"], frameon = True)\n        ax.axhline(loss_vec2[-2], color = \"black\", linestyle = \"--\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Optimization Iteration\")\n    ax.set_ylabel(\"Loss\")\n    plt.tight_layout()\n\n# Model accuracy plotter\ndef acc_plot(accs1, accs2 = None):\n    \n    # Plotting the accuracies of the model across each optimization iteration\n    fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n    ax.plot(accs1, color = \"purple\", linewidth = 2)\n    if (accs2 != None):\n        ax.plot(accs2, color = \"darkcyan\", linewidth = 2)\n        ax.legend([\"Training Accuracy\", \"Testing Accuracy\"], frameon = True)\n    ax.set_title(\"Model Accuracy Across Optimization Iteration\")\n    ax.set_xlabel(r\"Gradient Descent Iteration\")\n    ax.set_ylabel(\"Accuarcy\")\n    plt.tight_layout()\n\n# Decision line plotting helper method - code provided by Prof. Chodrow\ndef draw_line(X, w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = tch.linspace(x_min, x_max, X.shape[0])\n    y = -1 * (((w_[0] * x) + w_[2])/w_[1])\n    ax.plot(x, y, **kwargs)\n\n# Decision region plotter\ndef decision_bound(model, X, y, ax):\n\n    # Creating a mesh grid\n    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n    \n    # Drawing the decision line\n    draw_line(X, model.w, x_min, x_max, ax, color = \"black\", linewidth = 2)\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n\n    # Custom color map\n    colors = [\"#A46AAE\", \"darkcyan\"]  \n    cmap = LinearSegmentedColormap.from_list(\"my_cmap\", colors, N=256)\n\n    # Some code below provided by Prof. Chodrow\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix, 0], X[ix, 1], s = 20,  c = 2 * y[ix] - 1, facecolors = \"none\", edgecolors = \"none\", cmap = cmap, vmin = -2, vmax = 2, alpha = 0.75, marker = markers[i])\n    \n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    ax.set_title(\"Decision Regions of Logistic Regression Model\")\n    ax.text(X[:, 0].max() * 0.8, X[:, 1].max() * 0.85, f\"Model Accuracy:\\n{round(acc(model, X, y), 4) * 100}%\", fontsize = 10, ha = \"center\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad = 0.3\"))\n    plt.tight_layout()\n\n# Function to calculate model accuracy\ndef acc(model, X, y):\n    \n    # Compute model predictions\n    preds = model.predict(X)\n\n    # Determine the number of correct predictions\n    correct_preds = ((preds == y) * 1).float()\n\n    # Return the rate of correct predictions\n    return tch.mean(correct_preds).item()\n\n\nCode above defines plotting methods for observing the model’s empirical loss value evolution, the model’s accuracy, and the model’s classification decision boundaries (some code provided by Prof. Chodrow).\n\n\n\nThe implementation of Newton’s method incorporates the LinearModel and extends the LogisticRegression class implementations from my previous study. Extending the LogisticRegression class:\n\nhessian(X): A method that computes the Hessian matrix \\(H(\\mathbf{w})\\) of the empirical loss function \\(L(\\mathbf{w})\\) with respect to the weights vector \\(\\mathbf{w}\\). This Hessian matrix is a key component of Newton’s method which is a second-order optimization technique. The Hessian can be computed using matrix multiplication involving the feature matrix \\(\\mathbf{X}\\) and diagonal matrix \\(\\mathbf{D}\\) where the diagonal entries of \\(\\mathbf{D}\\) are \\(d_{k, k} = \\sigma(s_k)(1 - \\sigma(s_k))\\) (where \\(s_k\\) is the score of the kth data point). Note that to ensure numerical stability and matrix singularity, the Hessian \\(H(\\mathbf{w})\\) is normalized by the \\(n\\) (the number of data points \\(\\mathbf{X}\\)) and has diagonal entries padded by a value \\(\\epsilon = 1\\times10^-10\\). I found that these precautionary additions made for better experimentation. Below is the explicit definition of \\(H(\\mathbf{w})\\) along with the formula defining each entry \\(h_{i, j}(\\mathbf{w})\\):\n\n\\[\n\\begin{align*}\n\\mathbf{H}(\\mathbf{w}) &= \\mathbf{X}^T\\mathbf{D}\\mathbf{X} \\\\\nh_{i, j}(\\mathbf{w}) &= \\sum_{k = 1}^{n}{x_{k, i}x_{k, j}\\sigma(s_k)(1 - \\sigma(s_k))}\n\\end{align*}\n\\]\nThe actual implementation of Newton’s method resides in the following class:\nNewtonOptimizer:\n\nself.lr: An instance variable of a LogisticRegression object. This is used to reference the current weights vector \\(\\mathbf{w}\\) during an optimization step.\nstep(X, y, alpha): A method that computes an optimization step of Newton’s method. Note that \\(\\mathbf{X}, \\mathbf{y}\\) are needed to compute the gradient and Hessian matrix of the loss function \\(L(\\mathbf{w})\\). The hyperparameter alpha (denoted as \\(\\alpha\\) below) is used to set the learning rate for the gradient descent process. Note that this method technically takes the Moore-Penrose pseudoinverse of the Hessian \\(\\mathbf{H(w)}\\) to avoid computational failure in the event \\(\\mathbf{H(w)}\\) is somehow non-singular (likely due to finite numerical precision). I again found that this precautionary change made for better experimentation. This method updates the weights vector \\(\\mathbf{w_k}\\) using the following:\n\n\\[\nw_{k + 1} = w_{k} - \\alpha \\mathbf{H^{-1}}(\\mathbf{w_{k}})\\nabla L(\\mathbf{w_{k}})\n\\]\n\noptimize(X, y, alpha, tol): A method to optimize a model with Newton’s method (repeatedly calling the step(X, y, alpha) method above) until the model’s empirical loss value reaches the desired loss-value tolerance (tol).\n\n\n\nTo test my implementation of Newton’s method, I will evaluate its performance on the generated data in comparison to the standard gradient descent method I implemented in a previous study.\n\n\nCode\n# Testing the correctness of Newton's method implementation\n## Logistic regression model for Standard gradient descent\nLR_s = LogisticRegression()\nopt_s = GradientDescentOptimizer(LR_s)\n\n# Logistic regression model for Newton's method\nLR_n = LogisticRegression()\nopt_n = NewtonOptimizer(LR_n)\n\n# Initializing an equal weights vector for each model\nLR_s.w = tch.rand((X_sim.size()[1]))\nLR_n.w = LR_s.w.clone()\n\n# Optimizing both models\nfor i in range(5000):\n\n    opt_s.step(X_sim, y_sim, alpha = 0.1, beta = 0.0)\n    opt_n.step(X_sim, y_sim, alpha = 0.1)\n\n# Plotting the decision regions of both models\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\ndecision_bound(LR_s, X_sim, y_sim, ax[0])\ndecision_bound(LR_n, X_sim, y_sim, ax[1])\nax[0].set_title(\"Standard Gradient Descent\")\nax[1].set_title(\"Newton's Method\")\nfig.suptitle(\"Comparing Logistic Regression Model Performance using\\nNewton's Method and Gradient Descent\", fontsize = 16)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\nCode\n# Displaying the weights vectors of both models\nprint(f\"Standard Gradient Descent Optimizer | w_s = {LR_s.w.flatten()}\\n----------\")\nprint(f\"Newton's Method Optimizer           | w_n = {LR_n.w.flatten()}\")\n\n\nStandard Gradient Descent Optimizer | w_s = tensor([ 5.6146,  5.3198, -5.3680])\n----------\nNewton's Method Optimizer           | w_n = tensor([ 251.7622,  222.9369, -252.3581])\n\n\nCode cells above optimize two logistic regression models using the NewtonOptimizer and GradientDescentOptimizer, plot the corresponding decision boundaries, and display the weights vectors \\(w_s\\) (for standard gradient descent) and \\(w_n\\) (for Newton’s method).\nThe figure above shows the decision boundaries for the two logistic regression models fit to the simulated classification data. One model was optimized using standard gradient descent and the other was optimized using Newton’s method. As shown in the plots above, the decision boundaries for each model appear essentially identical and both models were able to achieve \\(100\\%\\) classification accuracy. This is expected as the data is generated to be linearly separable. Interestingly though, the output above shows that the weights vectors \\(\\mathbf{w_s}\\) (for standard gradient descent) and \\(\\mathbf{w_n}\\) (for Newton’s method) are notably different for each entry. This is likely attributable to the fact standard gradient descent and Newton’s method involve similarly-formatted by considerably differently-valued calculations. Nonetheless, each model was still able to converge to a “correct” weights vector \\(\\mathbf{w}\\). Note that for both the standard gradient descent and Newton’s method optimizers, the learning rate \\(\\alpha\\) was set to \\(0.1\\) (for the standard gradient descent optimizer, the momentum scalar \\(\\beta\\) was set to \\(0.0\\)). This shows that with a sufficiently small learning rate, both optimizers can converge to the same weights vector \\(\\mathbf{w}\\) and yield the same model accuracy.\n\n\n\nFor this experiment, I will compare the convergence rates of two logistic regression models optimized using the NewtonOptimizer and GradientDescentOptimizer. Each model will be trained and tested on the empirical, real-world data set.\n\n\nCode\n# Comparing convergence rates of standard gradient descent and Newton's method\n## Logistic regression model for Standard gradient descent\nLR_s = LogisticRegression()\nopt_s = GradientDescentOptimizer(LR_s)\n\n# Logistic regression model for Newton's method\nLR_n = LogisticRegression()\nopt_n = NewtonOptimizer(LR_n)\n\n# Initializing an equal weights vector for each model\nLR_s.w = tch.rand((X_train.size()[1]))\nLR_n.w = LR_s.w.clone()\n\n# Arrays to store the loss values of the models optimized with grad. descent and Newton's method\nlosses_s = []\nlosses_n = []\n\n# Optimization loop\nfor i in range(1000):\n    \n    # Recording current loss values\n    loss_s = LR_s.loss(X_train, y_train)\n    loss_n = LR_n.loss(X_train, y_train)\n    losses_s.append(loss_s)\n    losses_n.append(loss_n)\n\n    # Optimize each model\n    opt_s.step(X_train, y_train, alpha = 0.1, beta = 0.0)\n    opt_n.step(X_train, y_train, alpha = 0.1)\n    \n    # Displaying gradient descent progress for 5 iterations\n    if (i % 10 == 0) & (i &gt; 0) & (i &lt; 60):\n        print(f\"Iteration {i}:\")\n        print(f\"Current Loss value (Standard Gradient Descent): {round(losses_s[-1], 3)}\")\n        print(f\"Current Loss value (Newton's Method):           {round(losses_n[-1], 3)}\\n----------\\n\")\n\nprint(f\"...\\nAfter {i + 1} Iterations\")\nprint(f\"Standard Gradient Descent Loss Value: {round(losses_s[-1], 3)}\")\nprint(f\"Newton's Method Loss Value:           {round(losses_n[-1], 3)}\")\n\n\nIteration 10:\nCurrent Loss value (Standard Gradient Descent): 1.542\nCurrent Loss value (Newton's Method):           0.618\n----------\n\nIteration 20:\nCurrent Loss value (Standard Gradient Descent): 1.18\nCurrent Loss value (Newton's Method):           0.405\n----------\n\nIteration 30:\nCurrent Loss value (Standard Gradient Descent): 0.958\nCurrent Loss value (Newton's Method):           0.331\n----------\n\nIteration 40:\nCurrent Loss value (Standard Gradient Descent): 0.826\nCurrent Loss value (Newton's Method):           0.309\n----------\n\nIteration 50:\nCurrent Loss value (Standard Gradient Descent): 0.743\nCurrent Loss value (Newton's Method):           0.305\n----------\n\n...\nAfter 1000 Iterations\nStandard Gradient Descent Loss Value: 0.329\nNewton's Method Loss Value:           0.304\n\n\nCode above compares the convergence rates of the empirical loss value of the two logistic regression models optimized using the NewtonOptimizer and GradientDescentOptimizer. Each model is optimized for \\(1000\\) iterations. Note that the learning rate \\(\\alpha\\) for the GradientDescentOptimizer was set to \\(0.1\\) and the momentum scalar \\(\\beta\\) was set to \\(0.0\\). The learning rate \\(\\alpha\\) for the NewtonOptimizer was set to \\(0.1\\).\nThe output above displays a comparison of the convergence rates of the two logistic regression models optimized using the NewtonOptimizer and GradientDescentOptimizer. Note that the learning rate \\(\\alpha_s\\) for the GradientDescentOptimizer was set to \\(0.1\\), the momentum scalar \\(\\beta\\) was set to \\(0.0\\), and the learning rate \\(\\alpha_n\\) for the NewtonOptimizer was set to \\(0.1\\). To compare the convergence of the empirical loss value, each model was optimized for \\(1000\\) iterations. As illustrated above, the model optimized with Newton’s method achieved a lower empirical loss value than the model optimized with standard gradient descent at every optimization iteration.\n\n\nCode\n# Plotting the loss values of both models\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\nloss_plot(ax, losses_s, losses_n)\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.legend([\"Std. Grad. Descent\", \"Newton's Method\"], frameon = True)\nax.set_title(\"Optimization Method Comparison of Empirical Loss Value Convergence\\n(log-log Scale)\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above plots the empirical loss value convergence rates for each of the logistic regression models optimized using the NewtonOptimizer and GradientDescentOptimizer.\nFigure 2\nThe plot above provides a visual accompaniment to the output produced by the previous code cell. As shown, the model optimized with Newton’s method converges significantly faster than the model optimized with standard gradient descent. That is, the empirical loss value of the model employing Newton’s method clearly “levels out” in considerably fewer iterations that empirical loss value of the model using standard gradient descent. This result is evidence that under certain circumstances (i.e. when the learning rates \\(\\alpha_n, \\alpha_s\\) are appropriately/independently set), Newton’s method can achieve convergence (in the context of decreasing empirical loss value) considerably faster than standard gradient descent. Overall this experiment provides strong justification for why Newton’s method is considered significantly more efficient for optimization over standard gradient descent (under the right circumstances that is).\n\n\n\nIn this experiment, I will investigate the limitations of convergence of Newton’s method when the learning rate \\(\\alpha\\) is set too large.\n\n\nCode\n# Investigating the limitations of convergence of Newton's method\n\n# Logistic regression models with Newton's method using a large and small learning rate\nLR_lg = LogisticRegression()\nopt_lg = NewtonOptimizer(LR_lg)\nLR_sm = LogisticRegression()\nopt_sm = NewtonOptimizer(LR_sm)\n\n# Arrays to store the loss values of the model optimized with Newton's method\nlosses_lg = []\nlosses_sm = []\n\nfor i in range(1000):\n    \n    # Recording current loss value\n    loss_lg = LR_lg.loss(X_train, y_train)\n    loss_sm = LR_sm.loss(X_train, y_train)\n    losses_lg.append(loss_lg)\n    losses_sm.append(loss_sm)\n    opt_lg.step(X_train, y_train, alpha = 1.1)\n    opt_sm.step(X_train, y_train, alpha = 1.0)\n\n# Plotting the loss values of both models\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\nloss_plot(ax, losses_sm, losses_lg)\nax.set_title(\"Displaying The Limitations of Newton's Method\\nEmpirical Loss Value Convergence\")\nax.legend([\"alpha = 1.1\", \"alpha = 1.0\"], frameon = True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above optimizes two logistic regression models with a NewtonOptimizer and plots the evolution of the empirical loss values over \\(1000\\) iterations. The learning rates are set to \\(\\alpha_{sm} = 1.0, \\alpha_{lg} = 1.1\\).\nFigure 3\nThe figure above illustrates the limitations of convergence of Newton’s method when the learning rate \\(\\alpha\\) is set to large. In the plot, the evolution of the empirical loss values of two logistic regression models optimized with Newton’s method is displayed. One of the models has a learning rate of \\(\\alpha_{sm} = 1.0\\), while the other has a learning rate of \\(\\alpha_{lg} = 1.1\\). When the learning rate is set to \\(1.0\\), the model is able to converge to the minimal empirical loss value in very few iterations. However, when the learning rate is slightly increased to just \\(1.1\\), the model fails to converge to neither the minimal nor a consistent empirical loss value. Overall, it is clear that with a poorly/too-large selected learning rate, a model optimized with Newton’s method will fail to converge (with respect to decreasing empirical loss). This experiment stands as evidence that while Newton’s method can strongly outperform less efficient optimization methods under some circumstances, this will not strictly be the case as Newton’s method risks failing to optimize overall in some scenarios.\n\n\n\n\nLike the implementation of Newton’s method, the implementation of the Adam algorithm incorporates the LinearModel and the LogisticRegression class implementations from my previous study. The Adam algorithm is defined in the following class:\nAdamOptimizer:\n\nself.lr: An instance variable of a LogisticRegression object. This is used to reference the current weights vector \\(\\mathbf{w}\\) during an optimization step.\noptimizeEpoch(X, y, batch_size, alpha, beta_1, beta_2, w_0 = None): A method that computes runs the Adam algorithm on the data over one epoch. The batch_size argument specifies the size of the subset of data from the feature matrix \\(\\mathbf{X}\\) used in the improved stochastic gradient descent processed. The arguments beta_1, beta_2 determine the decay rates of the first moment (the mean of the gradient) and the second raw moment (the un-centered variance of the gradient) respectively. The w_0 argument is the initial guess for the weights vector \\(\\mathbf{w}\\) (implicitly set to None).\noptimize(X, y, tol, batch_size, alpha, beta_1, beta_2, w_0 = None): A method that runs the Adam algorithm on the data until the model’s empirical loss value reaches the desired tolerance (tol).\n\nThe Adam algorithm is a more efficient stochastic gradient descent technique that only uses first-order information. The Adam method leverages several efficiency-promoting concepts including the incorporation of adaptive learning rates \\(-\\) using the first and second moments of the gradient to give parameters with larger gradient magnitudes smaller updates and the opposite for parameters with smaller gradient magnitudes. My implementation of the Adam algorithm is defined below (adapted from Kingma, Diederik P, and Jimmy Lei Ba):\n\n\nCode\nImage(filename = \"adam.png\")\n\n\n\n\n\n\n\n\n\nImage 1\nAbove is the pseudocode for my implementation of the Adam algorithm which is a slightly edited version of the original algorithm designed by Kingma, Diederik P, and Jimmy Lei Ba.\n\n\nTo examine the correctness of my implementation of the Adam algorithm, I will again evaluate its performance on the generated data in comparison to the standard gradient descent method from my previous study.\n\n\nCode\n# Testing the correctness of Adam implementation\n## Logistic regression model for Standard gradient descent\nLR_s = LogisticRegression()\nopt_s = GradientDescentOptimizer(LR_s)\n\n# Logistic regression model for adam\nLR_a = LogisticRegression()\nopt_a = AdamOptimizer(LR_a)\n\n# Initializing an equal weights vector for each model\nLR_s.w = tch.rand((X_sim.size()[1]))\nLR_a.w = LR_s.w.clone()\n\n# Optimizing both models\nfor i in range(1000):\n    opt_s.step(X_sim, y_sim, alpha = 0.1, beta = 0.0)\n    \n    # # Optimizing with Adam for 3 epochs\n    if (i % 333 == 0):\n        \n        # Using a batchsize of (n/10) and the other suggested default argument settings\n        opt_a.optimizeEpoch(X_sim, y_sim, batch_size = int(X_sim.size(0) / 10), alpha = 0.1, beta_1 = 0.9, beta_2 = 0.9, w_0 = LR_a.w)\n\n# Plotting the decision regions of both models\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\ndecision_bound(LR_s, X_sim, y_sim, ax[0])\ndecision_bound(LR_a, X_sim, y_sim, ax[1])\nax[0].set_title(\"Standard SGD\")\nax[1].set_title(\"Adam\")\nfig.suptitle(\"Comparing Logistic Regression Model Performance using Adam\\nand Standard SGD\", fontsize = 16)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\nCode\n# Displaying the weights vectors of both models\nprint(f\"Standard Gradient Descent Optimizer | w_s = {LR_s.w.flatten()}\\n----------\")\nprint(f\"Adam Optimizer                      | w_a = {LR_a.w.flatten()}\")\n\n\nStandard Gradient Descent Optimizer | w_s = tensor([ 3.4320,  3.4643, -3.2607])\n----------\nAdam Optimizer                      | w_a = tensor([ 4.0078,  3.5353, -3.4335])\n\n\nCode cells above optimize two logistic regression models using the GradientDescentOptimizer and AdamOptimizer, plot the corresponding decision boundaries, and display the weights vectors \\(w_s\\) (for standard gradient descent) and \\(w_a\\) (for Adam).\nSimilarly to Figure 1, the figure above shows the decision boundaries for two logistic regression models fit to the simulated classification data. One model was optimized using standard gradient descent and the other was optimized using the Adam algorithm. As shown in the plots above, the decision boundaries for each model appear essentially identical and both models were able to achieve \\(100\\%\\) classification accuracy. This is expected as the data is generated to be linearly separable. In this experiment, the model optimized with standard gradient descent was optimized over \\(1000\\) iterations while the model optimized with the Adam algorithm was run for \\(3\\) epochs. Additionally, the output above shows that the weights vectors \\(\\mathbf{w_s}\\) (for standard gradient descent) and \\(\\mathbf{w_a}\\) (for the Adam algorithm) have very similar corresponding entries. Interestingly, the weights vectors \\(\\mathbf{w_s}, \\mathbf{w_a}\\) are not identical, yet they both yield models with flawless classification ability on the generated data. Note that for both the standard gradient descent and Adam algorithm optimizers, the learning rate \\(\\alpha\\) was set to \\(0.1\\), and the initial weights vector \\(\\mathbf{w_0}\\) was set randomly and assigned to each model. For the standard gradient descent optimizer, the momentum scalar \\(\\beta\\) was set to \\(0.0\\). For the Adam optimizer, the remaining parameters were set to: batch_size = \\(\\frac{n}{10}\\), and \\(\\beta_1, \\beta_2 = 0.9\\). This shows that with adequately chosen learning rates and other hyperparameters, both optimizers can converge to the same weights vector \\(\\mathbf{w}\\) and yield the same model accuracy.\n\n\n\nThis experiment aims to display the efficiency-promoting characteristics of the Adam algorithm in comparing the rate of empirical loss value convergence between two logistic regression models where one of them is optimized with SGD and the other employs the Adam algorithm.\n\n\nCode\n# Comparing convergence rates of Adam and SGD\n## Array to store different step sizes to compare convergence rates with\nalphas = [0.001, 0.01, 0.1]\n\n# Array to store the number of epochs to run for depending on step size\nepochs = [10000, 1000, 100]\n\n# Dictionaries to track decreasing loss value\nlosses_s_full = {}\nlosses_a_full = {}\n\n# For each step size\nfor j in range(len(alphas)):\n    \n    ## Logistic regression model for standard stochastic gradient descent\n    LR_s = LogisticRegression()\n    opt_s = GradientDescentOptimizer(LR_s)\n\n    # Logistic regression model for Adam\n    LR_a = LogisticRegression()\n    opt_a = AdamOptimizer(LR_a)\n\n    # Initializing an equal weights vector for each model\n    LR_s.w = tch.rand((X_train.size()[1]))\n    LR_a.w = LR_s.w.clone()\n\n    # Arrays to track decreasing loss value\n    losses_s = []\n    losses_a = []\n\n    # Optimization loop\n    for i in range(epochs[j]):\n        \n        # Using a batchsize of (n/8) and the other suggested default argument settings\n        b = int(X_train.size(0) / 8)\n        \n        # Recording current loss value\n        loss_s = LR_s.loss(X_train, y_train)\n        loss_a = LR_a.loss(X_train, y_train)\n        losses_s.append(loss_s)\n        losses_a.append(loss_a)\n        opt_s.optimizeSGD(X_train, y_train, batch_size = b, alpha = alphas[j], beta = 0.0)            \n        opt_a.optimizeEpoch(X_train, y_train, batch_size = b, alpha = alphas[j], beta_1 = 0.9, beta_2 = 0.9, w_0 = LR_a.w)\n\n    losses_s_full[alphas[j]] = losses_s\n    losses_a_full[alphas[j]] = losses_a\n\n    print(f\"----------\\nWhen alpha = {alphas[j]}\")\n    print(f\"Standard SGD Loss Value at {epochs[j]} Epochs: {round(losses_s_full[alphas[j]][-1], 3)}\")\n    print(f\"Adam Loss Value at {epochs[j]} Epochs:         {round(losses_a_full[alphas[j]][-1], 3)}\")\n\n\n----------\nWhen alpha = 0.001\nStandard SGD Loss Value at 10000 Epochs: 0.337\nAdam Loss Value at 10000 Epochs:         0.304\n----------\nWhen alpha = 0.01\nStandard SGD Loss Value at 1000 Epochs: 0.338\nAdam Loss Value at 1000 Epochs:         0.305\n----------\nWhen alpha = 0.1\nStandard SGD Loss Value at 100 Epochs: 0.336\nAdam Loss Value at 100 Epochs:         0.306\n\n\nCode above compares the convergence rates of the empirical loss value of the two logistic regression models optimized using the AdamOptimizer and GradientDescentOptimizer over three different learning rates. Each model is optimized for \\(10000, 1000, 100\\) epochs depending respectively on the learning rates \\(0.001, 0.01, 0.1\\). Note that the momentum scalar \\(\\beta\\) was set to \\(0.0\\) for the GradientDescentOptimizer the other hyperparameters were set to the recommendations of Kingma, Diederik P, and Jimmy Lei Ba for the AdamOptimizer.\nThe output above displays a comparison of the convergence rates of the two logistic regression models optimized using the AdamOptimizer and GradientDescentOptimizer over the learning rates \\(\\alpha = 0.001, 0.01, 0.1\\). To compare the convergence of the empirical loss value, each model was optimized for \\(10000, 1000, 100\\) epochs depending respectively on the given learning rate. As illustrated above, the model optimized with the Adam method produces a lower empirical loss value than the model employing standard SGD for each selected step size after the corresponding number of epochs.\n\n\nCode\n# Plotting the loss values of both models\nfig, ax = plt.subplots(1, 3, figsize = (12.5, 5))\nloss_plot(ax[0], losses_s_full[alphas[0]], losses_a_full[alphas[0]])\nax[0].set_xlabel(\"Epochs\")\nax[0].set_xscale(\"log\")\nax[0].set_yscale(\"log\")\nax[0].legend([\"Std. SGD\", \"Adam\"], frameon = True)\nax[0].set_title(f\"Alpha = {alphas[0]}\")\nloss_plot(ax[1], losses_s_full[alphas[1]], losses_a_full[alphas[1]])\nax[1].set_xlabel(\"Epochs\")\nax[1].set_xscale(\"log\")\nax[1].set_yscale(\"log\")\nax[1].legend([\"Std. SGD\", \"Adam\"], frameon = True)\nax[1].set_title(f\"Alpha = {alphas[1]}\")\nloss_plot(ax[2], losses_s_full[alphas[2]], losses_a_full[alphas[2]])\nax[2].set_xlabel(\"Epochs\")\nax[2].set_xscale(\"log\")\nax[2].set_yscale(\"log\")\nax[2].legend([\"Std. SGD\", \"Adam\"], frameon = True)\nax[2].set_title(f\"Alpha = {alphas[2]}\")\nfig.suptitle(f\"Optimization Method Comparison of Empirical Loss Value Convergence\\nWhen Alpha = {alphas[0]}, {alphas[1]}, {alphas[2]} (log-log Scale)\", fontsize = 14)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above plots the empirical loss value convergence rates for each of the logistic regression models optimized using the AdamOptimizer and GradientDescentOptimizer for each of the tested learning rates.\nFigure 5\nSimilarly to Figure 2, the plot above provides a visual accompaniment to the output produced by the previous code cell. As shown, the model optimized with the Adam algorithm converges considerably quicker than the model optimized with standard SGD for each of the tested learning rates. That is, when running for the same number of epochs (which varies depending on the selected step size) the loss value of model using the Adam algorithm “levels out” in many fewer epochs than the loss value of the model optimized with standard SGD. It is important to note that the empirical loss value appears to slightly fluctuate about some minimum threshold for the model using Adam when the step size is set to \\(0.1\\). It is possible that this “noisiness” in the loss-epochs plot for the model using Adam could indicate a failure to converge over many more epochs. However, in the number of allotted epochs, the model using Adam still clearly achieves a lower empirical loss value than that using standard SGD. The results of this experiment are evident that under certain circumstances (i.e. when the learning rates \\(\\alpha_a, \\alpha_s\\) are appropriately set), the Adam algorithm method can achieve convergence (in the context of decreasing empirical loss value) significantly faster than standard SGD. Further, these results stand as a representation of Adam’s boosted efficiencies, displaying how Adam’s performance can vastly exceed that of standard SGD even with several choices of the learning rate.\n\n\n\n\nAfter experimenting individually with each of these advanced optimization methods, it is useful to compare their performances to each other to understand how each method might outperform the other under certain circumstances. Considering that these two methods involve notably different computational procedures, I will opt to compare their convergence rates with respect to overall runtime. Specifically, I will examine the differences in runtime it takes each method to converge (i.e. minimize the empirical loss value) over three different learning rates (similarly to Experiment 2 from above). In this comparison, I will again use a pre-selected loss-value tolerance to determine convergence (note that there will be a different tolerance for each learning rate)\n\n\nCode\n# Comparing the convergence rates of Newton's method and Adam with respect to runtime\n## Array to store different step sizes and tolerances to compare convergence rates with\nalphas = [0.001, 0.01, 0.1]\ntols = [0.304, 0.30377, 0.3045]\n\n# Arrays to store the convergence runtimes\nrt_n = []\nrt_a = []\n\nfor i in range(len(alphas)):\n\n    # Initializing each model\n    LR_n = LogisticRegression()\n    opt_n = NewtonOptimizer(LR_n)\n    LR_a = LogisticRegression()\n    opt_a = AdamOptimizer(LR_a)\n\n    # Initializing the weights vectors\n    LR_n.w = tch.rand((X_train.size()[1]))\n    LR_a.w = LR_n.w.clone()\n\n    # Running Newton's method\n    start_time_n = time.time()\n    opt_n.optimize(X_train, y_train, alphas[i], tols[i])\n    rt_n.append(time.time() - start_time_n)\n\n    # Running Adam\n    start_time_a = time.time()\n    opt_a.optimize(X_train, y_train, tols[i], int(X_train.size(0) / 8), alphas[i], beta_1 = 0.9, beta_2 = 0.9, w_0 = LR_a.w) # Using a batch size of (n/8)\n    rt_a.append(time.time() - start_time_a)\n\n    # Comparison\n    print(f\"When alpha = {alphas[i]}\")\n    if (rt_n[-1] &lt; rt_a[-1]):\n        print(f\"Newton's Method converges in {rt_n[-1] :.2f} s (~{round((rt_a[-1] / rt_n[-1]), 1)}x Faster)\")\n        print(f\"Adam Converges in            {rt_a[-1] :.2f} s\\n----------\\n\")\n    else:\n        print(f\"Newton's Method converges in {rt_n[-1] :.2f} s \")\n        print(f\"Adam Converges in            {rt_a[-1] :.2f} s (~{int(rt_n[-1] / rt_a[-1])}x Faster)\\n----------\\n\")\n\n\nWhen alpha = 0.001\nNewton's Method converges in 1.71 s (~2.7x Faster)\nAdam Converges in            4.60 s\n----------\n\nWhen alpha = 0.01\nNewton's Method converges in 0.20 s (~45.4x Faster)\nAdam Converges in            9.01 s\n----------\n\nWhen alpha = 0.1\nNewton's Method converges in 0.02 s (~16.8x Faster)\nAdam Converges in            0.26 s\n----------\n\n\n\nCode above displays the runtime til convergence for a logistic regression model optimized with Newton’s method and a logistic regression model optimized with the Adam algorithm. Each model is optimized until the empirical loss value reaches a specified tolerance. The runtimes are compared across three different learning rate step sizes. The training data used is the empirical penguins classification data.\nThe output above compares the runtime between logistic regression models, one optimized with Newton’s method and the other with the Adam algorithm. In this comparison, each model is optimized over the empirical penguins classification data. The runtime required for each model to yield an empirical loss value of a specified tolerance is tracked and compared across three selected learning rates. The learning rates of this comparison are \\(\\alpha = 0.001, 0.01, 0.1\\), and the corresponding tolerances for each model given the current learning rates are \\(0.304, 0.30377, 0.304\\). As clearly depicted above, the model employing Newton’s method considerably outperforms the model using the Adam algorithm across all three tested learning rates. Based on this comparison alone, it would appear that Newton’s method is far more efficient than the Adam algorithm for optimization. However, note that the training data used in this comparison has relatively few features, which likely favors Newton’s method over Adam (primarily due to the fact that the expensive computation of Newton’s method is not amplified by a vast number of features). Thus, it is useful to recreate this comparison on data with many more features, and observe any notable similarities or differences \\(-\\) leading into the comparison below:\n\n\nCode\n# Comparing the convergence rates of Newton's method and Adam with respect to runtime - part 2\n## Generating high-dimensional data for binary classification - code provided by Prof. Chodrow\ndef classification_data(n_points = 500, noise = 0.2, p_dims = 250):\n    y = tch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + tch.normal(0.0, noise, size = (n_points,p_dims))\n    X = tch.cat((X, tch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\nX_sim, y_sim = classification_data(noise = 0.3)\n\n# Array to store different step sizes and tolerances to compare convergence rates with\nalphas = [0.001, 0.01, 0.1]\ntols = [0.4, 0.3, 0.2]\n\n# Arrays to store the convergence runtimes\nrt_n = []\nrt_a = []\n\nfor i in range(len(alphas)):\n\n    # Initializing each model\n    LR_n = LogisticRegression()\n    opt_n = NewtonOptimizer(LR_n)\n    LR_a = LogisticRegression()\n    opt_a = AdamOptimizer(LR_a)\n\n    # Initializing the weights vectors\n    LR_n.w = tch.rand((X_sim.size()[1]))\n    LR_a.w = LR_n.w.clone()\n\n    # Running Newton's method\n    start_time_n = time.time()\n    opt_n.optimize(X_sim, y_sim, alphas[i], tols[i])\n    rt_n.append(time.time() - start_time_n)\n\n    # Running Adam\n    start_time_a = time.time()\n    opt_a.optimize(X_sim, y_sim, tols[i], int(X_train.size(0) / 8), alphas[i], beta_1 = 0.9, beta_2 = 0.9, w_0 = LR_a.w) # Using a batch size of (n/8)\n    rt_a.append(time.time() - start_time_a)\n\n    # Comparison\n    print(f\"When alpha = {alphas[i]}\")\n    if (rt_n[-1] &lt; rt_a[-1]):\n        print(f\"Newton's Method converges in {rt_n[-1] :.2f} s (~{round((rt_a[-1] / rt_n[-1]), 1)}x Faster)\")\n        print(f\"Adam Converges in            {rt_a[-1] :.2f} s\\n----------\\n\")\n    else:\n        print(f\"Newton's Method converges in {rt_n[-1] :.2f} s \")\n        print(f\"Adam Converges in            {rt_a[-1] :.2f} s (~{int(rt_n[-1] / rt_a[-1])}x Faster)\\n----------\\n\")\n\n\nWhen alpha = 0.001\nNewton's Method converges in 4.96 s \nAdam Converges in            0.15 s (~33x Faster)\n----------\n\nWhen alpha = 0.01\nNewton's Method converges in 0.78 s \nAdam Converges in            0.02 s (~37x Faster)\n----------\n\nWhen alpha = 0.1\nNewton's Method converges in 0.49 s \nAdam Converges in            0.01 s (~79x Faster)\n----------\n\n\n\nCode above displays the runtime til convergence for a logistic regression model optimized with Newton’s method and a logistic regression model optimized with the Adam algorithm. Each model is optimized until the empirical loss value reaches a specified tolerance. The runtimes are compared across three different learning rate step sizes. The training data used in generated classification data with \\(250\\) features.\nThe output above again compares the runtime between logistic regression models, one optimized with Newton’s method and the other with the Adam algorithm. Note that the training data used in this comparison is another generated binary classification data set. In this data set, each data point has \\(250\\) features, much more than each data point from the empirical penguins classification data set. The runtime required for each model to yield an empirical loss value of a specified tolerance is tracked and compared across three selected learning rates. The learning rates of this comparison are again \\(\\alpha = 0.001, 0.01, 0.1\\), and the corresponding tolerances for each model given the current learning rates are now \\(0.4, 0.3, 0.32\\). This time, as clearly depicted above, the model employing the Adam algorithm method considerably outperforms the model using Newton’s method across all three tested learning rates. In this comparison, the performances of each optimization method have completely swapped with respect to the first runtime comparison. This is likely due to the fact the higher-dimensional generated data makes for expensive Hessian matrix/Hessian inversion computation for Newton’s method that the Adam algorithm does not experience. Thus, it appears that the performance of each optimization method over the other depends significantly on the number of features found in the data."
  },
  {
    "objectID": "posts/post_7/index.html#sourcing-data",
    "href": "posts/post_7/index.html#sourcing-data",
    "title": "Post 7 - Exploring Advanced Optimization Methods",
    "section": "",
    "text": "In order to test, evaluate, compare, and experiment with the advanced optimization methods explored in this study, it is necessary to have some binary classification data. For this analysis, I will be using the same generated data set from my previous study on logistic regression and the Palmer Penguins data set from my first study on classification. To briefly recap, the first data set is randomly generated 2D binary classification data used to test the correctness of the implementation for each advanced optimization method and the second data set is the widely used penguins classification data set from the Palmer Station collected by Dr. Kristen Gorman. Rather than trying to classify penguin species, I opted to have my models classify penguin sex (which in this data is a binary classification task). The penguins data is preprocessed by subsetting out only the numerical-value columns and standardizing each data point (using \\(\\frac{\\mathbf{x} - \\mu}{\\sigma}\\) where \\(\\mu, \\sigma\\) are the mean and STD. of the data).\n\n\nCode\n# Including all additional imports\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom pdf2image import convert_from_path\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image\nimport torch as tch\nimport pandas as pd\nimport numpy as np\nimport time\n\n# Porting over logistic regression implementation\n%load_ext autoreload\n%autoreload 2\nfrom advanced_logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer\ntch.manual_seed(100) # For consistent data generation\nplt.style.use('seaborn-v0_8-whitegrid') # For consistent plotting\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nCode above imports all necessary packages/libraries and ports over the implementation of my logistic regression model and the advanced optimizers NewtonOptimizer and AdamOptimizer.\n\n\nCode\n# Generating data for binary classification - code provided by Prof. Chodrow\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    y = tch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + tch.normal(0.0, noise, size = (n_points,p_dims))\n    X = tch.cat((X, tch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\nX_sim, y_sim = classification_data(noise = 0.25)\n\n# Accessing penguins data - data and (edited) method provided by Prof. Chodrow\ndef prepare_data(df):\n  \n  # Preprocessing data\n  le = LabelEncoder()\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  le.fit(train[\"Sex\"])\n  df = df.dropna()\n  y = le.transform(df[\"Sex\"])\n  df = df.drop([\"Sex\", \"Species\", \"Island\", \"Stage\", \"Clutch Completion\"], axis = 1)\n  \n  # Converting to torch tensors\n  X = tch.tensor(df.values).float()\n  y_ = tch.tensor(y == 1).float()\n\n  # Standardizing the data\n  mean = tch.mean(X, dim = 0, keepdim = True)\n  std = tch.std(X, dim = 0, keepdim = True)\n  X_s = (X - mean) / std\n  \n  # Adding a col of 1s to feature matrix\n  X_s = tch.cat((X_s, tch.ones(X_s.size(0), 1)), dim=1)\n  return X_s, y_\n\ntrain = pd.read_csv(\"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\")\nX_train, y_train = prepare_data(train)\n\n\nCode above generates the simulated data (generated with fairly low-noise to ensure linear separability) and imports, preprocess (feature subsetting and standardization), the empirical (real-world) data (some code provided by Prof. Chodrow).\n\n\nCode\n# Model interpretation helper methods\n## Loss value plotter\ndef loss_plot(ax, loss_vec1, loss_vec2 = None):\n    # Plotting the loss values of the model across each optimization iteration\n    ax.plot(loss_vec1, color = \"#A46AAE\", linewidth = 2)\n    title = \"Evolution of Empirical Loss Value\"\n    if (loss_vec2 != None):\n        title = \"Gradient Descent Method Comparison\\nof Empirical Loss Value Convergence\"\n        ax.plot(loss_vec2, color = \"darkcyan\", linewidth = 2)\n        ax.legend([\"Standard\", \"Momentum\"], frameon = True)\n        ax.axhline(loss_vec2[-2], color = \"black\", linestyle = \"--\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Optimization Iteration\")\n    ax.set_ylabel(\"Loss\")\n    plt.tight_layout()\n\n# Model accuracy plotter\ndef acc_plot(accs1, accs2 = None):\n    \n    # Plotting the accuracies of the model across each optimization iteration\n    fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n    ax.plot(accs1, color = \"purple\", linewidth = 2)\n    if (accs2 != None):\n        ax.plot(accs2, color = \"darkcyan\", linewidth = 2)\n        ax.legend([\"Training Accuracy\", \"Testing Accuracy\"], frameon = True)\n    ax.set_title(\"Model Accuracy Across Optimization Iteration\")\n    ax.set_xlabel(r\"Gradient Descent Iteration\")\n    ax.set_ylabel(\"Accuarcy\")\n    plt.tight_layout()\n\n# Decision line plotting helper method - code provided by Prof. Chodrow\ndef draw_line(X, w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = tch.linspace(x_min, x_max, X.shape[0])\n    y = -1 * (((w_[0] * x) + w_[2])/w_[1])\n    ax.plot(x, y, **kwargs)\n\n# Decision region plotter\ndef decision_bound(model, X, y, ax):\n\n    # Creating a mesh grid\n    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n    \n    # Drawing the decision line\n    draw_line(X, model.w, x_min, x_max, ax, color = \"black\", linewidth = 2)\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n\n    # Custom color map\n    colors = [\"#A46AAE\", \"darkcyan\"]  \n    cmap = LinearSegmentedColormap.from_list(\"my_cmap\", colors, N=256)\n\n    # Some code below provided by Prof. Chodrow\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix, 0], X[ix, 1], s = 20,  c = 2 * y[ix] - 1, facecolors = \"none\", edgecolors = \"none\", cmap = cmap, vmin = -2, vmax = 2, alpha = 0.75, marker = markers[i])\n    \n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    ax.set_title(\"Decision Regions of Logistic Regression Model\")\n    ax.text(X[:, 0].max() * 0.8, X[:, 1].max() * 0.85, f\"Model Accuracy:\\n{round(acc(model, X, y), 4) * 100}%\", fontsize = 10, ha = \"center\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad = 0.3\"))\n    plt.tight_layout()\n\n# Function to calculate model accuracy\ndef acc(model, X, y):\n    \n    # Compute model predictions\n    preds = model.predict(X)\n\n    # Determine the number of correct predictions\n    correct_preds = ((preds == y) * 1).float()\n\n    # Return the rate of correct predictions\n    return tch.mean(correct_preds).item()\n\n\nCode above defines plotting methods for observing the model’s empirical loss value evolution, the model’s accuracy, and the model’s classification decision boundaries (some code provided by Prof. Chodrow)."
  },
  {
    "objectID": "posts/post_7/index.html#implementing-newtons-method",
    "href": "posts/post_7/index.html#implementing-newtons-method",
    "title": "Post 7 - Exploring Advanced Optimization Methods",
    "section": "",
    "text": "The implementation of Newton’s method incorporates the LinearModel and extends the LogisticRegression class implementations from my previous study. Extending the LogisticRegression class:\n\nhessian(X): A method that computes the Hessian matrix \\(H(\\mathbf{w})\\) of the empirical loss function \\(L(\\mathbf{w})\\) with respect to the weights vector \\(\\mathbf{w}\\). This Hessian matrix is a key component of Newton’s method which is a second-order optimization technique. The Hessian can be computed using matrix multiplication involving the feature matrix \\(\\mathbf{X}\\) and diagonal matrix \\(\\mathbf{D}\\) where the diagonal entries of \\(\\mathbf{D}\\) are \\(d_{k, k} = \\sigma(s_k)(1 - \\sigma(s_k))\\) (where \\(s_k\\) is the score of the kth data point). Note that to ensure numerical stability and matrix singularity, the Hessian \\(H(\\mathbf{w})\\) is normalized by the \\(n\\) (the number of data points \\(\\mathbf{X}\\)) and has diagonal entries padded by a value \\(\\epsilon = 1\\times10^-10\\). I found that these precautionary additions made for better experimentation. Below is the explicit definition of \\(H(\\mathbf{w})\\) along with the formula defining each entry \\(h_{i, j}(\\mathbf{w})\\):\n\n\\[\n\\begin{align*}\n\\mathbf{H}(\\mathbf{w}) &= \\mathbf{X}^T\\mathbf{D}\\mathbf{X} \\\\\nh_{i, j}(\\mathbf{w}) &= \\sum_{k = 1}^{n}{x_{k, i}x_{k, j}\\sigma(s_k)(1 - \\sigma(s_k))}\n\\end{align*}\n\\]\nThe actual implementation of Newton’s method resides in the following class:\nNewtonOptimizer:\n\nself.lr: An instance variable of a LogisticRegression object. This is used to reference the current weights vector \\(\\mathbf{w}\\) during an optimization step.\nstep(X, y, alpha): A method that computes an optimization step of Newton’s method. Note that \\(\\mathbf{X}, \\mathbf{y}\\) are needed to compute the gradient and Hessian matrix of the loss function \\(L(\\mathbf{w})\\). The hyperparameter alpha (denoted as \\(\\alpha\\) below) is used to set the learning rate for the gradient descent process. Note that this method technically takes the Moore-Penrose pseudoinverse of the Hessian \\(\\mathbf{H(w)}\\) to avoid computational failure in the event \\(\\mathbf{H(w)}\\) is somehow non-singular (likely due to finite numerical precision). I again found that this precautionary change made for better experimentation. This method updates the weights vector \\(\\mathbf{w_k}\\) using the following:\n\n\\[\nw_{k + 1} = w_{k} - \\alpha \\mathbf{H^{-1}}(\\mathbf{w_{k}})\\nabla L(\\mathbf{w_{k}})\n\\]\n\noptimize(X, y, alpha, tol): A method to optimize a model with Newton’s method (repeatedly calling the step(X, y, alpha) method above) until the model’s empirical loss value reaches the desired loss-value tolerance (tol).\n\n\n\nTo test my implementation of Newton’s method, I will evaluate its performance on the generated data in comparison to the standard gradient descent method I implemented in a previous study.\n\n\nCode\n# Testing the correctness of Newton's method implementation\n## Logistic regression model for Standard gradient descent\nLR_s = LogisticRegression()\nopt_s = GradientDescentOptimizer(LR_s)\n\n# Logistic regression model for Newton's method\nLR_n = LogisticRegression()\nopt_n = NewtonOptimizer(LR_n)\n\n# Initializing an equal weights vector for each model\nLR_s.w = tch.rand((X_sim.size()[1]))\nLR_n.w = LR_s.w.clone()\n\n# Optimizing both models\nfor i in range(5000):\n\n    opt_s.step(X_sim, y_sim, alpha = 0.1, beta = 0.0)\n    opt_n.step(X_sim, y_sim, alpha = 0.1)\n\n# Plotting the decision regions of both models\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\ndecision_bound(LR_s, X_sim, y_sim, ax[0])\ndecision_bound(LR_n, X_sim, y_sim, ax[1])\nax[0].set_title(\"Standard Gradient Descent\")\nax[1].set_title(\"Newton's Method\")\nfig.suptitle(\"Comparing Logistic Regression Model Performance using\\nNewton's Method and Gradient Descent\", fontsize = 16)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\nCode\n# Displaying the weights vectors of both models\nprint(f\"Standard Gradient Descent Optimizer | w_s = {LR_s.w.flatten()}\\n----------\")\nprint(f\"Newton's Method Optimizer           | w_n = {LR_n.w.flatten()}\")\n\n\nStandard Gradient Descent Optimizer | w_s = tensor([ 5.6146,  5.3198, -5.3680])\n----------\nNewton's Method Optimizer           | w_n = tensor([ 251.7622,  222.9369, -252.3581])\n\n\nCode cells above optimize two logistic regression models using the NewtonOptimizer and GradientDescentOptimizer, plot the corresponding decision boundaries, and display the weights vectors \\(w_s\\) (for standard gradient descent) and \\(w_n\\) (for Newton’s method).\nThe figure above shows the decision boundaries for the two logistic regression models fit to the simulated classification data. One model was optimized using standard gradient descent and the other was optimized using Newton’s method. As shown in the plots above, the decision boundaries for each model appear essentially identical and both models were able to achieve \\(100\\%\\) classification accuracy. This is expected as the data is generated to be linearly separable. Interestingly though, the output above shows that the weights vectors \\(\\mathbf{w_s}\\) (for standard gradient descent) and \\(\\mathbf{w_n}\\) (for Newton’s method) are notably different for each entry. This is likely attributable to the fact standard gradient descent and Newton’s method involve similarly-formatted by considerably differently-valued calculations. Nonetheless, each model was still able to converge to a “correct” weights vector \\(\\mathbf{w}\\). Note that for both the standard gradient descent and Newton’s method optimizers, the learning rate \\(\\alpha\\) was set to \\(0.1\\) (for the standard gradient descent optimizer, the momentum scalar \\(\\beta\\) was set to \\(0.0\\)). This shows that with a sufficiently small learning rate, both optimizers can converge to the same weights vector \\(\\mathbf{w}\\) and yield the same model accuracy.\n\n\n\nFor this experiment, I will compare the convergence rates of two logistic regression models optimized using the NewtonOptimizer and GradientDescentOptimizer. Each model will be trained and tested on the empirical, real-world data set.\n\n\nCode\n# Comparing convergence rates of standard gradient descent and Newton's method\n## Logistic regression model for Standard gradient descent\nLR_s = LogisticRegression()\nopt_s = GradientDescentOptimizer(LR_s)\n\n# Logistic regression model for Newton's method\nLR_n = LogisticRegression()\nopt_n = NewtonOptimizer(LR_n)\n\n# Initializing an equal weights vector for each model\nLR_s.w = tch.rand((X_train.size()[1]))\nLR_n.w = LR_s.w.clone()\n\n# Arrays to store the loss values of the models optimized with grad. descent and Newton's method\nlosses_s = []\nlosses_n = []\n\n# Optimization loop\nfor i in range(1000):\n    \n    # Recording current loss values\n    loss_s = LR_s.loss(X_train, y_train)\n    loss_n = LR_n.loss(X_train, y_train)\n    losses_s.append(loss_s)\n    losses_n.append(loss_n)\n\n    # Optimize each model\n    opt_s.step(X_train, y_train, alpha = 0.1, beta = 0.0)\n    opt_n.step(X_train, y_train, alpha = 0.1)\n    \n    # Displaying gradient descent progress for 5 iterations\n    if (i % 10 == 0) & (i &gt; 0) & (i &lt; 60):\n        print(f\"Iteration {i}:\")\n        print(f\"Current Loss value (Standard Gradient Descent): {round(losses_s[-1], 3)}\")\n        print(f\"Current Loss value (Newton's Method):           {round(losses_n[-1], 3)}\\n----------\\n\")\n\nprint(f\"...\\nAfter {i + 1} Iterations\")\nprint(f\"Standard Gradient Descent Loss Value: {round(losses_s[-1], 3)}\")\nprint(f\"Newton's Method Loss Value:           {round(losses_n[-1], 3)}\")\n\n\nIteration 10:\nCurrent Loss value (Standard Gradient Descent): 1.542\nCurrent Loss value (Newton's Method):           0.618\n----------\n\nIteration 20:\nCurrent Loss value (Standard Gradient Descent): 1.18\nCurrent Loss value (Newton's Method):           0.405\n----------\n\nIteration 30:\nCurrent Loss value (Standard Gradient Descent): 0.958\nCurrent Loss value (Newton's Method):           0.331\n----------\n\nIteration 40:\nCurrent Loss value (Standard Gradient Descent): 0.826\nCurrent Loss value (Newton's Method):           0.309\n----------\n\nIteration 50:\nCurrent Loss value (Standard Gradient Descent): 0.743\nCurrent Loss value (Newton's Method):           0.305\n----------\n\n...\nAfter 1000 Iterations\nStandard Gradient Descent Loss Value: 0.329\nNewton's Method Loss Value:           0.304\n\n\nCode above compares the convergence rates of the empirical loss value of the two logistic regression models optimized using the NewtonOptimizer and GradientDescentOptimizer. Each model is optimized for \\(1000\\) iterations. Note that the learning rate \\(\\alpha\\) for the GradientDescentOptimizer was set to \\(0.1\\) and the momentum scalar \\(\\beta\\) was set to \\(0.0\\). The learning rate \\(\\alpha\\) for the NewtonOptimizer was set to \\(0.1\\).\nThe output above displays a comparison of the convergence rates of the two logistic regression models optimized using the NewtonOptimizer and GradientDescentOptimizer. Note that the learning rate \\(\\alpha_s\\) for the GradientDescentOptimizer was set to \\(0.1\\), the momentum scalar \\(\\beta\\) was set to \\(0.0\\), and the learning rate \\(\\alpha_n\\) for the NewtonOptimizer was set to \\(0.1\\). To compare the convergence of the empirical loss value, each model was optimized for \\(1000\\) iterations. As illustrated above, the model optimized with Newton’s method achieved a lower empirical loss value than the model optimized with standard gradient descent at every optimization iteration.\n\n\nCode\n# Plotting the loss values of both models\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\nloss_plot(ax, losses_s, losses_n)\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.legend([\"Std. Grad. Descent\", \"Newton's Method\"], frameon = True)\nax.set_title(\"Optimization Method Comparison of Empirical Loss Value Convergence\\n(log-log Scale)\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above plots the empirical loss value convergence rates for each of the logistic regression models optimized using the NewtonOptimizer and GradientDescentOptimizer.\nFigure 2\nThe plot above provides a visual accompaniment to the output produced by the previous code cell. As shown, the model optimized with Newton’s method converges significantly faster than the model optimized with standard gradient descent. That is, the empirical loss value of the model employing Newton’s method clearly “levels out” in considerably fewer iterations that empirical loss value of the model using standard gradient descent. This result is evidence that under certain circumstances (i.e. when the learning rates \\(\\alpha_n, \\alpha_s\\) are appropriately/independently set), Newton’s method can achieve convergence (in the context of decreasing empirical loss value) considerably faster than standard gradient descent. Overall this experiment provides strong justification for why Newton’s method is considered significantly more efficient for optimization over standard gradient descent (under the right circumstances that is).\n\n\n\nIn this experiment, I will investigate the limitations of convergence of Newton’s method when the learning rate \\(\\alpha\\) is set too large.\n\n\nCode\n# Investigating the limitations of convergence of Newton's method\n\n# Logistic regression models with Newton's method using a large and small learning rate\nLR_lg = LogisticRegression()\nopt_lg = NewtonOptimizer(LR_lg)\nLR_sm = LogisticRegression()\nopt_sm = NewtonOptimizer(LR_sm)\n\n# Arrays to store the loss values of the model optimized with Newton's method\nlosses_lg = []\nlosses_sm = []\n\nfor i in range(1000):\n    \n    # Recording current loss value\n    loss_lg = LR_lg.loss(X_train, y_train)\n    loss_sm = LR_sm.loss(X_train, y_train)\n    losses_lg.append(loss_lg)\n    losses_sm.append(loss_sm)\n    opt_lg.step(X_train, y_train, alpha = 1.1)\n    opt_sm.step(X_train, y_train, alpha = 1.0)\n\n# Plotting the loss values of both models\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\nloss_plot(ax, losses_sm, losses_lg)\nax.set_title(\"Displaying The Limitations of Newton's Method\\nEmpirical Loss Value Convergence\")\nax.legend([\"alpha = 1.1\", \"alpha = 1.0\"], frameon = True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above optimizes two logistic regression models with a NewtonOptimizer and plots the evolution of the empirical loss values over \\(1000\\) iterations. The learning rates are set to \\(\\alpha_{sm} = 1.0, \\alpha_{lg} = 1.1\\).\nFigure 3\nThe figure above illustrates the limitations of convergence of Newton’s method when the learning rate \\(\\alpha\\) is set to large. In the plot, the evolution of the empirical loss values of two logistic regression models optimized with Newton’s method is displayed. One of the models has a learning rate of \\(\\alpha_{sm} = 1.0\\), while the other has a learning rate of \\(\\alpha_{lg} = 1.1\\). When the learning rate is set to \\(1.0\\), the model is able to converge to the minimal empirical loss value in very few iterations. However, when the learning rate is slightly increased to just \\(1.1\\), the model fails to converge to neither the minimal nor a consistent empirical loss value. Overall, it is clear that with a poorly/too-large selected learning rate, a model optimized with Newton’s method will fail to converge (with respect to decreasing empirical loss). This experiment stands as evidence that while Newton’s method can strongly outperform less efficient optimization methods under some circumstances, this will not strictly be the case as Newton’s method risks failing to optimize overall in some scenarios."
  },
  {
    "objectID": "posts/post_7/index.html#implementing-the-adam-optimization-algorithm",
    "href": "posts/post_7/index.html#implementing-the-adam-optimization-algorithm",
    "title": "Post 7 - Exploring Advanced Optimization Methods",
    "section": "",
    "text": "Like the implementation of Newton’s method, the implementation of the Adam algorithm incorporates the LinearModel and the LogisticRegression class implementations from my previous study. The Adam algorithm is defined in the following class:\nAdamOptimizer:\n\nself.lr: An instance variable of a LogisticRegression object. This is used to reference the current weights vector \\(\\mathbf{w}\\) during an optimization step.\noptimizeEpoch(X, y, batch_size, alpha, beta_1, beta_2, w_0 = None): A method that computes runs the Adam algorithm on the data over one epoch. The batch_size argument specifies the size of the subset of data from the feature matrix \\(\\mathbf{X}\\) used in the improved stochastic gradient descent processed. The arguments beta_1, beta_2 determine the decay rates of the first moment (the mean of the gradient) and the second raw moment (the un-centered variance of the gradient) respectively. The w_0 argument is the initial guess for the weights vector \\(\\mathbf{w}\\) (implicitly set to None).\noptimize(X, y, tol, batch_size, alpha, beta_1, beta_2, w_0 = None): A method that runs the Adam algorithm on the data until the model’s empirical loss value reaches the desired tolerance (tol).\n\nThe Adam algorithm is a more efficient stochastic gradient descent technique that only uses first-order information. The Adam method leverages several efficiency-promoting concepts including the incorporation of adaptive learning rates \\(-\\) using the first and second moments of the gradient to give parameters with larger gradient magnitudes smaller updates and the opposite for parameters with smaller gradient magnitudes. My implementation of the Adam algorithm is defined below (adapted from Kingma, Diederik P, and Jimmy Lei Ba):\n\n\nCode\nImage(filename = \"adam.png\")\n\n\n\n\n\n\n\n\n\nImage 1\nAbove is the pseudocode for my implementation of the Adam algorithm which is a slightly edited version of the original algorithm designed by Kingma, Diederik P, and Jimmy Lei Ba.\n\n\nTo examine the correctness of my implementation of the Adam algorithm, I will again evaluate its performance on the generated data in comparison to the standard gradient descent method from my previous study.\n\n\nCode\n# Testing the correctness of Adam implementation\n## Logistic regression model for Standard gradient descent\nLR_s = LogisticRegression()\nopt_s = GradientDescentOptimizer(LR_s)\n\n# Logistic regression model for adam\nLR_a = LogisticRegression()\nopt_a = AdamOptimizer(LR_a)\n\n# Initializing an equal weights vector for each model\nLR_s.w = tch.rand((X_sim.size()[1]))\nLR_a.w = LR_s.w.clone()\n\n# Optimizing both models\nfor i in range(1000):\n    opt_s.step(X_sim, y_sim, alpha = 0.1, beta = 0.0)\n    \n    # # Optimizing with Adam for 3 epochs\n    if (i % 333 == 0):\n        \n        # Using a batchsize of (n/10) and the other suggested default argument settings\n        opt_a.optimizeEpoch(X_sim, y_sim, batch_size = int(X_sim.size(0) / 10), alpha = 0.1, beta_1 = 0.9, beta_2 = 0.9, w_0 = LR_a.w)\n\n# Plotting the decision regions of both models\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\ndecision_bound(LR_s, X_sim, y_sim, ax[0])\ndecision_bound(LR_a, X_sim, y_sim, ax[1])\nax[0].set_title(\"Standard SGD\")\nax[1].set_title(\"Adam\")\nfig.suptitle(\"Comparing Logistic Regression Model Performance using Adam\\nand Standard SGD\", fontsize = 16)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\nCode\n# Displaying the weights vectors of both models\nprint(f\"Standard Gradient Descent Optimizer | w_s = {LR_s.w.flatten()}\\n----------\")\nprint(f\"Adam Optimizer                      | w_a = {LR_a.w.flatten()}\")\n\n\nStandard Gradient Descent Optimizer | w_s = tensor([ 3.4320,  3.4643, -3.2607])\n----------\nAdam Optimizer                      | w_a = tensor([ 4.0078,  3.5353, -3.4335])\n\n\nCode cells above optimize two logistic regression models using the GradientDescentOptimizer and AdamOptimizer, plot the corresponding decision boundaries, and display the weights vectors \\(w_s\\) (for standard gradient descent) and \\(w_a\\) (for Adam).\nSimilarly to Figure 1, the figure above shows the decision boundaries for two logistic regression models fit to the simulated classification data. One model was optimized using standard gradient descent and the other was optimized using the Adam algorithm. As shown in the plots above, the decision boundaries for each model appear essentially identical and both models were able to achieve \\(100\\%\\) classification accuracy. This is expected as the data is generated to be linearly separable. In this experiment, the model optimized with standard gradient descent was optimized over \\(1000\\) iterations while the model optimized with the Adam algorithm was run for \\(3\\) epochs. Additionally, the output above shows that the weights vectors \\(\\mathbf{w_s}\\) (for standard gradient descent) and \\(\\mathbf{w_a}\\) (for the Adam algorithm) have very similar corresponding entries. Interestingly, the weights vectors \\(\\mathbf{w_s}, \\mathbf{w_a}\\) are not identical, yet they both yield models with flawless classification ability on the generated data. Note that for both the standard gradient descent and Adam algorithm optimizers, the learning rate \\(\\alpha\\) was set to \\(0.1\\), and the initial weights vector \\(\\mathbf{w_0}\\) was set randomly and assigned to each model. For the standard gradient descent optimizer, the momentum scalar \\(\\beta\\) was set to \\(0.0\\). For the Adam optimizer, the remaining parameters were set to: batch_size = \\(\\frac{n}{10}\\), and \\(\\beta_1, \\beta_2 = 0.9\\). This shows that with adequately chosen learning rates and other hyperparameters, both optimizers can converge to the same weights vector \\(\\mathbf{w}\\) and yield the same model accuracy.\n\n\n\nThis experiment aims to display the efficiency-promoting characteristics of the Adam algorithm in comparing the rate of empirical loss value convergence between two logistic regression models where one of them is optimized with SGD and the other employs the Adam algorithm.\n\n\nCode\n# Comparing convergence rates of Adam and SGD\n## Array to store different step sizes to compare convergence rates with\nalphas = [0.001, 0.01, 0.1]\n\n# Array to store the number of epochs to run for depending on step size\nepochs = [10000, 1000, 100]\n\n# Dictionaries to track decreasing loss value\nlosses_s_full = {}\nlosses_a_full = {}\n\n# For each step size\nfor j in range(len(alphas)):\n    \n    ## Logistic regression model for standard stochastic gradient descent\n    LR_s = LogisticRegression()\n    opt_s = GradientDescentOptimizer(LR_s)\n\n    # Logistic regression model for Adam\n    LR_a = LogisticRegression()\n    opt_a = AdamOptimizer(LR_a)\n\n    # Initializing an equal weights vector for each model\n    LR_s.w = tch.rand((X_train.size()[1]))\n    LR_a.w = LR_s.w.clone()\n\n    # Arrays to track decreasing loss value\n    losses_s = []\n    losses_a = []\n\n    # Optimization loop\n    for i in range(epochs[j]):\n        \n        # Using a batchsize of (n/8) and the other suggested default argument settings\n        b = int(X_train.size(0) / 8)\n        \n        # Recording current loss value\n        loss_s = LR_s.loss(X_train, y_train)\n        loss_a = LR_a.loss(X_train, y_train)\n        losses_s.append(loss_s)\n        losses_a.append(loss_a)\n        opt_s.optimizeSGD(X_train, y_train, batch_size = b, alpha = alphas[j], beta = 0.0)            \n        opt_a.optimizeEpoch(X_train, y_train, batch_size = b, alpha = alphas[j], beta_1 = 0.9, beta_2 = 0.9, w_0 = LR_a.w)\n\n    losses_s_full[alphas[j]] = losses_s\n    losses_a_full[alphas[j]] = losses_a\n\n    print(f\"----------\\nWhen alpha = {alphas[j]}\")\n    print(f\"Standard SGD Loss Value at {epochs[j]} Epochs: {round(losses_s_full[alphas[j]][-1], 3)}\")\n    print(f\"Adam Loss Value at {epochs[j]} Epochs:         {round(losses_a_full[alphas[j]][-1], 3)}\")\n\n\n----------\nWhen alpha = 0.001\nStandard SGD Loss Value at 10000 Epochs: 0.337\nAdam Loss Value at 10000 Epochs:         0.304\n----------\nWhen alpha = 0.01\nStandard SGD Loss Value at 1000 Epochs: 0.338\nAdam Loss Value at 1000 Epochs:         0.305\n----------\nWhen alpha = 0.1\nStandard SGD Loss Value at 100 Epochs: 0.336\nAdam Loss Value at 100 Epochs:         0.306\n\n\nCode above compares the convergence rates of the empirical loss value of the two logistic regression models optimized using the AdamOptimizer and GradientDescentOptimizer over three different learning rates. Each model is optimized for \\(10000, 1000, 100\\) epochs depending respectively on the learning rates \\(0.001, 0.01, 0.1\\). Note that the momentum scalar \\(\\beta\\) was set to \\(0.0\\) for the GradientDescentOptimizer the other hyperparameters were set to the recommendations of Kingma, Diederik P, and Jimmy Lei Ba for the AdamOptimizer.\nThe output above displays a comparison of the convergence rates of the two logistic regression models optimized using the AdamOptimizer and GradientDescentOptimizer over the learning rates \\(\\alpha = 0.001, 0.01, 0.1\\). To compare the convergence of the empirical loss value, each model was optimized for \\(10000, 1000, 100\\) epochs depending respectively on the given learning rate. As illustrated above, the model optimized with the Adam method produces a lower empirical loss value than the model employing standard SGD for each selected step size after the corresponding number of epochs.\n\n\nCode\n# Plotting the loss values of both models\nfig, ax = plt.subplots(1, 3, figsize = (12.5, 5))\nloss_plot(ax[0], losses_s_full[alphas[0]], losses_a_full[alphas[0]])\nax[0].set_xlabel(\"Epochs\")\nax[0].set_xscale(\"log\")\nax[0].set_yscale(\"log\")\nax[0].legend([\"Std. SGD\", \"Adam\"], frameon = True)\nax[0].set_title(f\"Alpha = {alphas[0]}\")\nloss_plot(ax[1], losses_s_full[alphas[1]], losses_a_full[alphas[1]])\nax[1].set_xlabel(\"Epochs\")\nax[1].set_xscale(\"log\")\nax[1].set_yscale(\"log\")\nax[1].legend([\"Std. SGD\", \"Adam\"], frameon = True)\nax[1].set_title(f\"Alpha = {alphas[1]}\")\nloss_plot(ax[2], losses_s_full[alphas[2]], losses_a_full[alphas[2]])\nax[2].set_xlabel(\"Epochs\")\nax[2].set_xscale(\"log\")\nax[2].set_yscale(\"log\")\nax[2].legend([\"Std. SGD\", \"Adam\"], frameon = True)\nax[2].set_title(f\"Alpha = {alphas[2]}\")\nfig.suptitle(f\"Optimization Method Comparison of Empirical Loss Value Convergence\\nWhen Alpha = {alphas[0]}, {alphas[1]}, {alphas[2]} (log-log Scale)\", fontsize = 14)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above plots the empirical loss value convergence rates for each of the logistic regression models optimized using the AdamOptimizer and GradientDescentOptimizer for each of the tested learning rates.\nFigure 5\nSimilarly to Figure 2, the plot above provides a visual accompaniment to the output produced by the previous code cell. As shown, the model optimized with the Adam algorithm converges considerably quicker than the model optimized with standard SGD for each of the tested learning rates. That is, when running for the same number of epochs (which varies depending on the selected step size) the loss value of model using the Adam algorithm “levels out” in many fewer epochs than the loss value of the model optimized with standard SGD. It is important to note that the empirical loss value appears to slightly fluctuate about some minimum threshold for the model using Adam when the step size is set to \\(0.1\\). It is possible that this “noisiness” in the loss-epochs plot for the model using Adam could indicate a failure to converge over many more epochs. However, in the number of allotted epochs, the model using Adam still clearly achieves a lower empirical loss value than that using standard SGD. The results of this experiment are evident that under certain circumstances (i.e. when the learning rates \\(\\alpha_a, \\alpha_s\\) are appropriately set), the Adam algorithm method can achieve convergence (in the context of decreasing empirical loss value) significantly faster than standard SGD. Further, these results stand as a representation of Adam’s boosted efficiencies, displaying how Adam’s performance can vastly exceed that of standard SGD even with several choices of the learning rate."
  },
  {
    "objectID": "posts/post_7/index.html#comparing-newtons-method-and-the-adam-algorithm",
    "href": "posts/post_7/index.html#comparing-newtons-method-and-the-adam-algorithm",
    "title": "Post 7 - Exploring Advanced Optimization Methods",
    "section": "",
    "text": "After experimenting individually with each of these advanced optimization methods, it is useful to compare their performances to each other to understand how each method might outperform the other under certain circumstances. Considering that these two methods involve notably different computational procedures, I will opt to compare their convergence rates with respect to overall runtime. Specifically, I will examine the differences in runtime it takes each method to converge (i.e. minimize the empirical loss value) over three different learning rates (similarly to Experiment 2 from above). In this comparison, I will again use a pre-selected loss-value tolerance to determine convergence (note that there will be a different tolerance for each learning rate)\n\n\nCode\n# Comparing the convergence rates of Newton's method and Adam with respect to runtime\n## Array to store different step sizes and tolerances to compare convergence rates with\nalphas = [0.001, 0.01, 0.1]\ntols = [0.304, 0.30377, 0.3045]\n\n# Arrays to store the convergence runtimes\nrt_n = []\nrt_a = []\n\nfor i in range(len(alphas)):\n\n    # Initializing each model\n    LR_n = LogisticRegression()\n    opt_n = NewtonOptimizer(LR_n)\n    LR_a = LogisticRegression()\n    opt_a = AdamOptimizer(LR_a)\n\n    # Initializing the weights vectors\n    LR_n.w = tch.rand((X_train.size()[1]))\n    LR_a.w = LR_n.w.clone()\n\n    # Running Newton's method\n    start_time_n = time.time()\n    opt_n.optimize(X_train, y_train, alphas[i], tols[i])\n    rt_n.append(time.time() - start_time_n)\n\n    # Running Adam\n    start_time_a = time.time()\n    opt_a.optimize(X_train, y_train, tols[i], int(X_train.size(0) / 8), alphas[i], beta_1 = 0.9, beta_2 = 0.9, w_0 = LR_a.w) # Using a batch size of (n/8)\n    rt_a.append(time.time() - start_time_a)\n\n    # Comparison\n    print(f\"When alpha = {alphas[i]}\")\n    if (rt_n[-1] &lt; rt_a[-1]):\n        print(f\"Newton's Method converges in {rt_n[-1] :.2f} s (~{round((rt_a[-1] / rt_n[-1]), 1)}x Faster)\")\n        print(f\"Adam Converges in            {rt_a[-1] :.2f} s\\n----------\\n\")\n    else:\n        print(f\"Newton's Method converges in {rt_n[-1] :.2f} s \")\n        print(f\"Adam Converges in            {rt_a[-1] :.2f} s (~{int(rt_n[-1] / rt_a[-1])}x Faster)\\n----------\\n\")\n\n\nWhen alpha = 0.001\nNewton's Method converges in 1.71 s (~2.7x Faster)\nAdam Converges in            4.60 s\n----------\n\nWhen alpha = 0.01\nNewton's Method converges in 0.20 s (~45.4x Faster)\nAdam Converges in            9.01 s\n----------\n\nWhen alpha = 0.1\nNewton's Method converges in 0.02 s (~16.8x Faster)\nAdam Converges in            0.26 s\n----------\n\n\n\nCode above displays the runtime til convergence for a logistic regression model optimized with Newton’s method and a logistic regression model optimized with the Adam algorithm. Each model is optimized until the empirical loss value reaches a specified tolerance. The runtimes are compared across three different learning rate step sizes. The training data used is the empirical penguins classification data.\nThe output above compares the runtime between logistic regression models, one optimized with Newton’s method and the other with the Adam algorithm. In this comparison, each model is optimized over the empirical penguins classification data. The runtime required for each model to yield an empirical loss value of a specified tolerance is tracked and compared across three selected learning rates. The learning rates of this comparison are \\(\\alpha = 0.001, 0.01, 0.1\\), and the corresponding tolerances for each model given the current learning rates are \\(0.304, 0.30377, 0.304\\). As clearly depicted above, the model employing Newton’s method considerably outperforms the model using the Adam algorithm across all three tested learning rates. Based on this comparison alone, it would appear that Newton’s method is far more efficient than the Adam algorithm for optimization. However, note that the training data used in this comparison has relatively few features, which likely favors Newton’s method over Adam (primarily due to the fact that the expensive computation of Newton’s method is not amplified by a vast number of features). Thus, it is useful to recreate this comparison on data with many more features, and observe any notable similarities or differences \\(-\\) leading into the comparison below:\n\n\nCode\n# Comparing the convergence rates of Newton's method and Adam with respect to runtime - part 2\n## Generating high-dimensional data for binary classification - code provided by Prof. Chodrow\ndef classification_data(n_points = 500, noise = 0.2, p_dims = 250):\n    y = tch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + tch.normal(0.0, noise, size = (n_points,p_dims))\n    X = tch.cat((X, tch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\nX_sim, y_sim = classification_data(noise = 0.3)\n\n# Array to store different step sizes and tolerances to compare convergence rates with\nalphas = [0.001, 0.01, 0.1]\ntols = [0.4, 0.3, 0.2]\n\n# Arrays to store the convergence runtimes\nrt_n = []\nrt_a = []\n\nfor i in range(len(alphas)):\n\n    # Initializing each model\n    LR_n = LogisticRegression()\n    opt_n = NewtonOptimizer(LR_n)\n    LR_a = LogisticRegression()\n    opt_a = AdamOptimizer(LR_a)\n\n    # Initializing the weights vectors\n    LR_n.w = tch.rand((X_sim.size()[1]))\n    LR_a.w = LR_n.w.clone()\n\n    # Running Newton's method\n    start_time_n = time.time()\n    opt_n.optimize(X_sim, y_sim, alphas[i], tols[i])\n    rt_n.append(time.time() - start_time_n)\n\n    # Running Adam\n    start_time_a = time.time()\n    opt_a.optimize(X_sim, y_sim, tols[i], int(X_train.size(0) / 8), alphas[i], beta_1 = 0.9, beta_2 = 0.9, w_0 = LR_a.w) # Using a batch size of (n/8)\n    rt_a.append(time.time() - start_time_a)\n\n    # Comparison\n    print(f\"When alpha = {alphas[i]}\")\n    if (rt_n[-1] &lt; rt_a[-1]):\n        print(f\"Newton's Method converges in {rt_n[-1] :.2f} s (~{round((rt_a[-1] / rt_n[-1]), 1)}x Faster)\")\n        print(f\"Adam Converges in            {rt_a[-1] :.2f} s\\n----------\\n\")\n    else:\n        print(f\"Newton's Method converges in {rt_n[-1] :.2f} s \")\n        print(f\"Adam Converges in            {rt_a[-1] :.2f} s (~{int(rt_n[-1] / rt_a[-1])}x Faster)\\n----------\\n\")\n\n\nWhen alpha = 0.001\nNewton's Method converges in 4.96 s \nAdam Converges in            0.15 s (~33x Faster)\n----------\n\nWhen alpha = 0.01\nNewton's Method converges in 0.78 s \nAdam Converges in            0.02 s (~37x Faster)\n----------\n\nWhen alpha = 0.1\nNewton's Method converges in 0.49 s \nAdam Converges in            0.01 s (~79x Faster)\n----------\n\n\n\nCode above displays the runtime til convergence for a logistic regression model optimized with Newton’s method and a logistic regression model optimized with the Adam algorithm. Each model is optimized until the empirical loss value reaches a specified tolerance. The runtimes are compared across three different learning rate step sizes. The training data used in generated classification data with \\(250\\) features.\nThe output above again compares the runtime between logistic regression models, one optimized with Newton’s method and the other with the Adam algorithm. Note that the training data used in this comparison is another generated binary classification data set. In this data set, each data point has \\(250\\) features, much more than each data point from the empirical penguins classification data set. The runtime required for each model to yield an empirical loss value of a specified tolerance is tracked and compared across three selected learning rates. The learning rates of this comparison are again \\(\\alpha = 0.001, 0.01, 0.1\\), and the corresponding tolerances for each model given the current learning rates are now \\(0.4, 0.3, 0.32\\). This time, as clearly depicted above, the model employing the Adam algorithm method considerably outperforms the model using Newton’s method across all three tested learning rates. In this comparison, the performances of each optimization method have completely swapped with respect to the first runtime comparison. This is likely due to the fact the higher-dimensional generated data makes for expensive Hessian matrix/Hessian inversion computation for Newton’s method that the Adam algorithm does not experience. Thus, it appears that the performance of each optimization method over the other depends significantly on the number of features found in the data."
  },
  {
    "objectID": "posts/post_1/index.html",
    "href": "posts/post_1/index.html",
    "title": "Post 1 - Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post, I’m taking on an introductory-level machine learning classification task. This task, classifying penguin species, is a ternary classification problem as there are three species options for which I am trying to construct a classifier model to correctly identify. The data for this task comes from the Palmer Station and was collected by Dr. Kristen Gorman. The following analysis begins with some preliminary data visualizations and summary statistics interpretation. From the preliminary visualizations and statistics table, I’m able to make some initial observations about the difference in penguin features across each species. The next sections show the initial model and feature selection as well as model refinement to improve the chosen model. While completing this analysis, I experimented with several different models: a logistic regression model, a decision tree classifier, and a random forest classifier. I ultimately decided to choose proceed with the random forest classifier model (see the third section in this post). For feature selection, I initially took an exhaustive search approach. However, after running into many bugs and difficulties with this approach, I opted to incorporate the use of prebuilt feature selection tools (see second section in this post). Following a thorough feature selection and cross-validation/model refinement process, I was able to produce a model with nearly \\(100\\%\\) testing accuracy. That is, my model was able to correctly identify all but a single observation from unseen test data (i.e. it made 1 classification error). In the following sections of this post are more in-depth explanations and analyses of my steps taken in accomplishing this classification task.\n\n\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# Accessing training data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# Code below provided by Prof. Chodrow\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nThe prepare_data(...) function above was provided by Prof. Chodrow\n\n\nCode\n# Includuing all additional imports\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\nimport seaborn as sns\n\n\nIncluding all additional imports\n\n\nCode\n# Creating a modified dataset for visualization and summary statistics\ntrain_viz = train[train[\"Sex\"] != \".\"].copy()\ntrain_viz[\"Culmen Ratio (L/D)\"] = train_viz[\"Culmen Length (mm)\"] / train_viz[\"Culmen Depth (mm)\"]\ntrain_viz.dropna()\n\n# Subsetting data by species to make regression plots for flipper length by body mass across each Species\nadelie = train_viz[['Species', 'Flipper Length (mm)', 'Body Mass (g)']]\nadelie = adelie[adelie['Species'] == 'Adelie Penguin (Pygoscelis adeliae)']\nadelie = adelie.dropna(subset = ['Species', 'Flipper Length (mm)', 'Body Mass (g)'])\nchinstrap = train_viz[['Species', 'Flipper Length (mm)', 'Body Mass (g)']]\nchinstrap = chinstrap[chinstrap['Species'] == 'Chinstrap penguin (Pygoscelis antarctica)']\nchinstrap = chinstrap.dropna(subset = ['Species', 'Flipper Length (mm)', 'Body Mass (g)'])\ngentoo = train_viz[['Species', 'Flipper Length (mm)', 'Body Mass (g)']]\ngentoo = gentoo[gentoo['Species'] == 'Gentoo penguin (Pygoscelis papua)']\ngentoo = gentoo.dropna(subset = ['Species', 'Flipper Length (mm)', 'Body Mass (g)'])\n\n\nAbove, I’m creating a modified dataset for visualization and summary statistics and making three subsets of the visualization data set corresponding to eac penguin species with the specific visualization features selected. This is later used for linear regression modeling by species.\n\n\n\n\n\nCode\n# Creating linear regression models and calculating r^2 values\n\n## Adelie\nc1 = np.polyfit(adelie['Body Mass (g)'], adelie['Flipper Length (mm)'], 1)\np1 = np.polyval(c1, adelie['Body Mass (g)'])\nr1 = adelie['Flipper Length (mm)'] - p1\nssr1 = np.sum(r1**2)\nsst1 = np.sum((adelie['Flipper Length (mm)'] - np.mean(adelie['Flipper Length (mm)']))**2)\nrs1 = 1 - (ssr1 / sst1)\n\n## Chinstrap\nc2 = np.polyfit(chinstrap['Body Mass (g)'], chinstrap['Flipper Length (mm)'], 1)\np2 = np.polyval(c2, chinstrap['Body Mass (g)'])\nr2 = chinstrap['Flipper Length (mm)'] - p2\nssr2 = np.sum(r2**2)\nsst2 = np.sum((chinstrap['Flipper Length (mm)'] - np.mean(chinstrap['Flipper Length (mm)']))**2)\nrs2 = 1 - (ssr2 / sst2)\n\n## Gentoo\nc3 = np.polyfit(gentoo['Body Mass (g)'], gentoo['Flipper Length (mm)'], 1)\np3 = np.polyval(c3, gentoo['Body Mass (g)'])\nr3 = gentoo['Flipper Length (mm)'] - p3\nssr3 = np.sum(r3**2)\nsst3 = np.sum((gentoo['Flipper Length (mm)'] - np.mean(gentoo['Flipper Length (mm)']))**2)\nrs3 = 1 - (ssr3 / sst3)\n\n\nAbove, I’m creating a linear regression model of flipper length by body mass for each species (using np.polyfit). I’m also extracting the \\(R^2\\) values (using predictions from np.polyval to calculate residuals) for each regression model to include in my plots below. I searched online how to use these two functions.\n\n\nCode\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize = (10, 7))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Setting up the initial scatter plot\nax[0] = sns.scatterplot(data = train_viz, x = 'Body Mass (g)', y = 'Flipper Length (mm)', hue = 'Species', palette = ['#B85ED4', '#2A7A7A', '#F28234'], style = 'Species', s = 50, ax = ax[0])\n\n# Adding regression lines for each species\nsns.regplot(data = adelie, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#F28234'}, ax = ax[0])\nsns.regplot(data = chinstrap, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#B85ED4'}, ax = ax[0])\nsns.regplot(data = gentoo, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#2A7A7A'}, ax = ax[0])\n\n# Plot styling to make colors match up and the text boxes look nicer\nax[0].set_title(\"Penguin Flipper Length (mm) by Body Mass (g)\\nColored by Species\")\nax[0].legend(frameon = True, prop = {'size': 9})\nax[0].text(5200, 175.75, '           \\n', fontsize = 20, bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'grey', boxstyle = 'round, pad=0.3'))\nax[0].text(5555, 180, f'     $R^2 = {rs3:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5555, 178, f'     $R^2 = {rs2:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5555, 176, f'     $R^2 = {rs1:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5200, 180.1, '\\u2013', color='#2A7A7A', ha='left', va='center', fontsize=15, fontweight='bold')\nax[0].text(5200, 178.1, '\\u2013', color='#B85ED4', ha='left', va='center', fontsize=15, fontweight='bold')\nax[0].text(5200, 176.1, '\\u2013', color='#F28234', ha='left', va='center', fontsize=15, fontweight='bold')\n\n# Setting up the box plot and doing some simple styling\nax[1] = sns.boxplot(train_viz, x = \"Species\", y = \"Culmen Ratio (L/D)\", hue = \"Sex\", ax = ax[1])\nax[1].set_xticks([0, 1, 2])\nax[1].set_xticklabels([\"Chinstrap\", \"Gentoo\", \"Adelie\"])\nax[1].legend(prop = {'size': 10}, frameon = True)\nax[1].set_title(\"Penguin Culmen Ratio (L/D) by Species\\nGrouped by Sex\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.2)\n\n\n\n\n\n\n\n\n\nIn the figure to the left, the relationship between penguin flipper length (mm) and body mass (g) for each species of penguin is displayed. Visually, there appears to be a notable positive, linear relationship between flipper length and body mass in general. That is, it seems that as penguin body mass increases, the flipper length increases as well regardless of species.\nAdditionally, there appears to be some clustering by species shown in this relationship. Gentoo penguins appear to have both the largest flipper lengths and body masses. There is less of a visually obvious distinction between Chinstrap and Adelie penguins shown by the relationship between body mass and flipper length. However, I believe it is reasonable to hypothesize that Chinstrap penguins have a slightly greater average flipper length while Adelie penguins have a slightly greater body mass.\nFrom the \\(R^2\\) values for the regression lines corresponding to the relationship between body mass and flipper length for each penguin species, I feel that it’s reasonable to say the positive linear relationships between the two variables in question for each species range from weak/moderate to moderate in strength. Further, while I do not have the statistical knowledge to confirm nor deny this hypothesis, perhaps the clear visual difference observed across the three species-specific regression lines could provide useful insight into this classification task.\nIn general, this plot reveals some useful information about the differences observed across each penguin species from two relevant variables. While this plot likely does not provide highly convincing information pertaining to the species classification of penguins from certain features, it does highlight a relationship between two key quantitative variables that displays a preliminary approach to this classification task.\nThe figure to the right displays the general distribution of the Culmen Ratio (L/D) feature across penguin sex and penguin species. The Culmen Ratio (L/D) is the ratio of culmen length (mm) to culmen depth (mm). With no prior knowledge about culmen size and dimensions in penguins, I thought it would be interesting to compute the length-depth ratio for each penguin and see how this ratio differed by sex and across each species. Observing the comparison of the culmen ratio between male and female penguins, there does not appear to be as much visually obvious information. However, to me, there appears to be minimal distribution overlap in the culmen ratio across each species. I believe this suggests that the culmen ratio or related features (culmen length and culmen depth) could be useful in model construction to successfully complete this classification task.\n\n\n\n\nCode\n# Summary Statistics\n\n# Helper method to calculate coefficient of variation (%)\ndef cv(col):\n    return (col.std() / col.mean()) * 100\n\n# Creating a table grouped by penguin species and sex, showing general summary stats for several quantitative variables\nsum_stats = train_viz.groupby(['Species', 'Sex']).aggregate({\"Flipper Length (mm)\" : [\"mean\", \"std\", cv], \n                                                             \"Body Mass (g)\" : [\"mean\", \"std\", cv], \"Culmen Length (mm)\": [\"mean\", \"std\", cv], \n                                                             \"Culmen Depth (mm)\": [\"mean\", \"std\", cv], \"Culmen Ratio (L/D)\": [\"mean\", \"std\", cv]})\nsum_stats = sum_stats.rename(columns = {'mean': 'Mean', 'std': 'STD', 'cv': 'CV (%)'})\nsum_stats = sum_stats.round(2)\nsum_stats\n\n\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nCulmen Ratio (L/D)\n\n\n\n\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n187.92\n5.43\n2.89\n3350.47\n262.87\n7.85\n37.43\n1.95\n5.20\n17.64\n0.92\n5.21\n2.13\n0.15\n6.96\n\n\nMALE\n192.33\n7.00\n3.64\n4052.87\n331.57\n8.18\n40.40\n2.37\n5.86\n19.08\n1.05\n5.48\n2.12\n0.17\n7.89\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n192.06\n5.90\n3.07\n3523.39\n294.95\n8.37\n46.72\n3.17\n6.78\n17.61\n0.80\n4.55\n2.66\n0.19\n7.21\n\n\nMALE\n200.69\n6.29\n3.13\n4005.77\n368.53\n9.20\n51.33\n1.60\n3.13\n19.27\n0.77\n3.97\n2.67\n0.10\n3.93\n\n\nGentoo penguin (Pygoscelis papua)\nFEMALE\n212.84\n3.47\n1.63\n4684.69\n297.46\n6.35\n45.46\n1.97\n4.34\n14.21\n0.54\n3.78\n3.20\n0.14\n4.45\n\n\nMALE\n221.20\n5.22\n2.36\n5476.70\n301.32\n5.50\n49.01\n2.29\n4.68\n15.73\n0.79\n5.00\n3.12\n0.18\n5.77\n\n\n\n\n\n\n\nIn creating the table above, I looked up how to calculate the CV for each of the columns to more easily interpret the STD and created the helper method\nAbove is a general statistics table for the quantitative features present in the penguins data. Some noteworthy observations from this table are below.\nAdelie penguins:\n\nFemales\n\nSmallest average flipper length, body mass, culmen length, and culmen ratio\nLargest average culmen depth\n\nMales\n\nSmallest average flipper length, culmen length, and culmen ratio\n\n\nChinstrap penguins:\n\nFemales\n\nLargest average culmen length\n\nMales\n\nSmallest average body mass\nLargest average culmen length and culmen depth\n\n\nGentoo penguins:\n\nFemales\n\nLargest average flipper length, body mass, and culmen ratio\nSmallest average culmen depth\n\nMales\n\nLargest average flipper length, body mass, and culmen ratio\nSmallest average culmen depth\n\n\nRegarding the variability in the distribution of each feature represented in the table above, the coefficient of variation (as a percentage) is below 10% for all feature distributions. According to some brief research on CV, the CV values shown in the table above suggest that there is a relatively low degree of variability in the distributions of each feature."
  },
  {
    "objectID": "posts/post_1/index.html#summary-statistics-and-preliminary-visualizations",
    "href": "posts/post_1/index.html#summary-statistics-and-preliminary-visualizations",
    "title": "Post 1 - Classifying Palmer Penguins",
    "section": "",
    "text": "Code\n# Creating linear regression models and calculating r^2 values\n\n## Adelie\nc1 = np.polyfit(adelie['Body Mass (g)'], adelie['Flipper Length (mm)'], 1)\np1 = np.polyval(c1, adelie['Body Mass (g)'])\nr1 = adelie['Flipper Length (mm)'] - p1\nssr1 = np.sum(r1**2)\nsst1 = np.sum((adelie['Flipper Length (mm)'] - np.mean(adelie['Flipper Length (mm)']))**2)\nrs1 = 1 - (ssr1 / sst1)\n\n## Chinstrap\nc2 = np.polyfit(chinstrap['Body Mass (g)'], chinstrap['Flipper Length (mm)'], 1)\np2 = np.polyval(c2, chinstrap['Body Mass (g)'])\nr2 = chinstrap['Flipper Length (mm)'] - p2\nssr2 = np.sum(r2**2)\nsst2 = np.sum((chinstrap['Flipper Length (mm)'] - np.mean(chinstrap['Flipper Length (mm)']))**2)\nrs2 = 1 - (ssr2 / sst2)\n\n## Gentoo\nc3 = np.polyfit(gentoo['Body Mass (g)'], gentoo['Flipper Length (mm)'], 1)\np3 = np.polyval(c3, gentoo['Body Mass (g)'])\nr3 = gentoo['Flipper Length (mm)'] - p3\nssr3 = np.sum(r3**2)\nsst3 = np.sum((gentoo['Flipper Length (mm)'] - np.mean(gentoo['Flipper Length (mm)']))**2)\nrs3 = 1 - (ssr3 / sst3)\n\n\nAbove, I’m creating a linear regression model of flipper length by body mass for each species (using np.polyfit). I’m also extracting the \\(R^2\\) values (using predictions from np.polyval to calculate residuals) for each regression model to include in my plots below. I searched online how to use these two functions.\n\n\nCode\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize = (10, 7))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Setting up the initial scatter plot\nax[0] = sns.scatterplot(data = train_viz, x = 'Body Mass (g)', y = 'Flipper Length (mm)', hue = 'Species', palette = ['#B85ED4', '#2A7A7A', '#F28234'], style = 'Species', s = 50, ax = ax[0])\n\n# Adding regression lines for each species\nsns.regplot(data = adelie, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#F28234'}, ax = ax[0])\nsns.regplot(data = chinstrap, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#B85ED4'}, ax = ax[0])\nsns.regplot(data = gentoo, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#2A7A7A'}, ax = ax[0])\n\n# Plot styling to make colors match up and the text boxes look nicer\nax[0].set_title(\"Penguin Flipper Length (mm) by Body Mass (g)\\nColored by Species\")\nax[0].legend(frameon = True, prop = {'size': 9})\nax[0].text(5200, 175.75, '           \\n', fontsize = 20, bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'grey', boxstyle = 'round, pad=0.3'))\nax[0].text(5555, 180, f'     $R^2 = {rs3:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5555, 178, f'     $R^2 = {rs2:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5555, 176, f'     $R^2 = {rs1:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5200, 180.1, '\\u2013', color='#2A7A7A', ha='left', va='center', fontsize=15, fontweight='bold')\nax[0].text(5200, 178.1, '\\u2013', color='#B85ED4', ha='left', va='center', fontsize=15, fontweight='bold')\nax[0].text(5200, 176.1, '\\u2013', color='#F28234', ha='left', va='center', fontsize=15, fontweight='bold')\n\n# Setting up the box plot and doing some simple styling\nax[1] = sns.boxplot(train_viz, x = \"Species\", y = \"Culmen Ratio (L/D)\", hue = \"Sex\", ax = ax[1])\nax[1].set_xticks([0, 1, 2])\nax[1].set_xticklabels([\"Chinstrap\", \"Gentoo\", \"Adelie\"])\nax[1].legend(prop = {'size': 10}, frameon = True)\nax[1].set_title(\"Penguin Culmen Ratio (L/D) by Species\\nGrouped by Sex\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.2)\n\n\n\n\n\n\n\n\n\nIn the figure to the left, the relationship between penguin flipper length (mm) and body mass (g) for each species of penguin is displayed. Visually, there appears to be a notable positive, linear relationship between flipper length and body mass in general. That is, it seems that as penguin body mass increases, the flipper length increases as well regardless of species.\nAdditionally, there appears to be some clustering by species shown in this relationship. Gentoo penguins appear to have both the largest flipper lengths and body masses. There is less of a visually obvious distinction between Chinstrap and Adelie penguins shown by the relationship between body mass and flipper length. However, I believe it is reasonable to hypothesize that Chinstrap penguins have a slightly greater average flipper length while Adelie penguins have a slightly greater body mass.\nFrom the \\(R^2\\) values for the regression lines corresponding to the relationship between body mass and flipper length for each penguin species, I feel that it’s reasonable to say the positive linear relationships between the two variables in question for each species range from weak/moderate to moderate in strength. Further, while I do not have the statistical knowledge to confirm nor deny this hypothesis, perhaps the clear visual difference observed across the three species-specific regression lines could provide useful insight into this classification task.\nIn general, this plot reveals some useful information about the differences observed across each penguin species from two relevant variables. While this plot likely does not provide highly convincing information pertaining to the species classification of penguins from certain features, it does highlight a relationship between two key quantitative variables that displays a preliminary approach to this classification task.\nThe figure to the right displays the general distribution of the Culmen Ratio (L/D) feature across penguin sex and penguin species. The Culmen Ratio (L/D) is the ratio of culmen length (mm) to culmen depth (mm). With no prior knowledge about culmen size and dimensions in penguins, I thought it would be interesting to compute the length-depth ratio for each penguin and see how this ratio differed by sex and across each species. Observing the comparison of the culmen ratio between male and female penguins, there does not appear to be as much visually obvious information. However, to me, there appears to be minimal distribution overlap in the culmen ratio across each species. I believe this suggests that the culmen ratio or related features (culmen length and culmen depth) could be useful in model construction to successfully complete this classification task.\n\n\n\n\nCode\n# Summary Statistics\n\n# Helper method to calculate coefficient of variation (%)\ndef cv(col):\n    return (col.std() / col.mean()) * 100\n\n# Creating a table grouped by penguin species and sex, showing general summary stats for several quantitative variables\nsum_stats = train_viz.groupby(['Species', 'Sex']).aggregate({\"Flipper Length (mm)\" : [\"mean\", \"std\", cv], \n                                                             \"Body Mass (g)\" : [\"mean\", \"std\", cv], \"Culmen Length (mm)\": [\"mean\", \"std\", cv], \n                                                             \"Culmen Depth (mm)\": [\"mean\", \"std\", cv], \"Culmen Ratio (L/D)\": [\"mean\", \"std\", cv]})\nsum_stats = sum_stats.rename(columns = {'mean': 'Mean', 'std': 'STD', 'cv': 'CV (%)'})\nsum_stats = sum_stats.round(2)\nsum_stats\n\n\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nCulmen Ratio (L/D)\n\n\n\n\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n187.92\n5.43\n2.89\n3350.47\n262.87\n7.85\n37.43\n1.95\n5.20\n17.64\n0.92\n5.21\n2.13\n0.15\n6.96\n\n\nMALE\n192.33\n7.00\n3.64\n4052.87\n331.57\n8.18\n40.40\n2.37\n5.86\n19.08\n1.05\n5.48\n2.12\n0.17\n7.89\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n192.06\n5.90\n3.07\n3523.39\n294.95\n8.37\n46.72\n3.17\n6.78\n17.61\n0.80\n4.55\n2.66\n0.19\n7.21\n\n\nMALE\n200.69\n6.29\n3.13\n4005.77\n368.53\n9.20\n51.33\n1.60\n3.13\n19.27\n0.77\n3.97\n2.67\n0.10\n3.93\n\n\nGentoo penguin (Pygoscelis papua)\nFEMALE\n212.84\n3.47\n1.63\n4684.69\n297.46\n6.35\n45.46\n1.97\n4.34\n14.21\n0.54\n3.78\n3.20\n0.14\n4.45\n\n\nMALE\n221.20\n5.22\n2.36\n5476.70\n301.32\n5.50\n49.01\n2.29\n4.68\n15.73\n0.79\n5.00\n3.12\n0.18\n5.77\n\n\n\n\n\n\n\nIn creating the table above, I looked up how to calculate the CV for each of the columns to more easily interpret the STD and created the helper method\nAbove is a general statistics table for the quantitative features present in the penguins data. Some noteworthy observations from this table are below.\nAdelie penguins:\n\nFemales\n\nSmallest average flipper length, body mass, culmen length, and culmen ratio\nLargest average culmen depth\n\nMales\n\nSmallest average flipper length, culmen length, and culmen ratio\n\n\nChinstrap penguins:\n\nFemales\n\nLargest average culmen length\n\nMales\n\nSmallest average body mass\nLargest average culmen length and culmen depth\n\n\nGentoo penguins:\n\nFemales\n\nLargest average flipper length, body mass, and culmen ratio\nSmallest average culmen depth\n\nMales\n\nLargest average flipper length, body mass, and culmen ratio\nSmallest average culmen depth\n\n\nRegarding the variability in the distribution of each feature represented in the table above, the coefficient of variation (as a percentage) is below 10% for all feature distributions. According to some brief research on CV, the CV values shown in the table above suggest that there is a relatively low degree of variability in the distributions of each feature."
  },
  {
    "objectID": "posts/post_1/index.html#testing-the-model",
    "href": "posts/post_1/index.html#testing-the-model",
    "title": "Post 1 - Classifying Palmer Penguins",
    "section": "Testing the Model",
    "text": "Testing the Model\nAfter refining my model, I then tested it on the testing data. To my excitement, the model dispayed a \\(98.529\\%\\) testing accuracy.\n\n\nCode\n# Accessing test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Testing the model!\nX_test, y_test = prepare_data(test)\ntest_score = rfc_refined.score(X_test[X_train_selected.columns], y_test)\nts2 = rfc_base.score(X_test[X_train_selected.columns], y_test)\nprint(f'Refined (cross-validated) Model Test Accuracy: {test_score * 100: .3f}%')\n\n\nRefined (cross-validated) Model Test Accuracy:  98.529%\n\n\nBelow are the decision regions determined by the model for both the training and testing data. Visually, the decision regions for the training and testing data appear to be extremely similar. Additionally, I feel that it is reasonable to state that the decision regions for both the training and testing data do not display a high degree of model-overfitting.\n\n\nCode\n# Decision region plotting - code provided by Prof. Chodrow\ndef plot_regions(model, X, y, type):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 5))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      colors = ['#2A7A7A', '#B85ED4', '#F28234']\n      og = ['red', 'green', 'blue']\n      cmap = ListedColormap(colors)\n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = cmap, alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = cmap, vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i][7:] + \" Island\")\n      \n      patches = []\n      for color, spec in zip(['#F28234', '#B85ED4', '#2A7A7A'], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n\n      if (type):\n         fig.suptitle(\"Decision Regions for Training Data\")\n      else:\n         fig.suptitle(\"Decision Regions for Testing Data\")\n      \n      plt.tight_layout()\n\nplot_regions(rfc_refined, X_train_selected, y_train, 1)\nplot_regions(rfc_refined, X_test[X_train_selected.columns], y_test, 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI made some minor adjustments to the code above provided by Prof. Chodrow (primarily debugging, color changing, and adding titles)"
  },
  {
    "objectID": "posts/post_1/index.html#confusion-matrix-for-the-model",
    "href": "posts/post_1/index.html#confusion-matrix-for-the-model",
    "title": "Post 1 - Classifying Palmer Penguins",
    "section": "Confusion Matrix for the Model",
    "text": "Confusion Matrix for the Model\nBelow is the confusion matrix to evaluate the model’s testing performance. As expected with \\(&lt; 100\\%\\) (\\(98.529\\%\\)) testing accuracy, the confusion matrix has at least some (in this case exactly 1) non-zero entries off the diagonal. This indicates that the model made 1 misclassification. That is, all Adelie penguins and Chinstrap penguins were correctly classified, but there was one Gentoo penguin who was misclassified as an Adelie penguin (shucks).\n\n\nCode\n# Establishing model predictions for the test data\ny_test_pred = rfc_refined.predict(X_test[X_train_selected.columns])\nC = confusion_matrix(y_test, y_test_pred)\nspecies = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n\n# Creating a heatmap for better confusion matrix visualization\nplt.figure(figsize=(6, 5))\nsns.heatmap(C, annot = True, fmt = \"d\", cmap = 'Greens', cbar = False, xticklabels = species, yticklabels = species)\n\n# Setting labels and title\nplt.xlabel(\"Predicted Species Classification\")\nplt.ylabel(\"True Species Classification\")\nplt.title(\"Palmer Penguins Classification Confusion Matrix\")\n\nplt.show()\n\n# Printing confusion matrix results\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\n\n\n\n\n\n\n\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/post_4/index.html",
    "href": "posts/post_4/index.html",
    "title": "Post 4 - Implementing Perceptron",
    "section": "",
    "text": "The primary goal of this brief study on perceptron is to explore and investigate the processes under the hood of this algorithm \\(-\\) one of the oldest machine learning algorithms to exist. The functionality of and logic behind the perceptron algorithm is a backbone of many modern ML methods and models. It is crucial to develop at least a basic understanding of how perceptron works and why its design is as such. This introductory dive into the inner workings of perceptron involves examining the conditions in which the algorithm is successful, the conditions in which the algorithm must be manually adjusted to prevent non-convergence, various ways the algorithm can be refined to operate on more complex data, and the general limitations and implications associated with this algorithm. Over several experiments involving multiple datasets with different key characteristics (such as linear separability and number involved variables), my implementation of the perceptron algorithm is analyzed. With low-dimensional, linearly separable data, I found perceptron to perform with high accuracy (usually perfect) over a relatively small number of iterations. However, with more complex and or not linearly separable data, the algorithm’s performance decreased in accuracy and or runtime. Yet, using an alternative version of perceptron allowed for the mitigation of some of the flaws exhibited by the original algorithm on certain datasets\nFor the implementation of the original and enhanced perceptron algorithms, visit perceptron.py and MBperceptron.py.\n\n\n\n\nCode\n# Including all additional imports\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib import pyplot as plt\nimport torch as tch\n\n# Porting over perceptron and minibatch perceptron implementations\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom MBperceptron import MBPerceptron, MBPerceptronOptimizer\ntch.manual_seed(100) # For consistent data generation\nplt.style.use('seaborn-v0_8-whitegrid') # For consistent plotting\n\n# Generating the data - Some code provided by Prof. Chodrow\n## Linearly separable 2D data\ny1 = tch.arange(500) &gt;= int(500 / 2)\nX1 = y1[:, None] + tch.normal(0.0, 0.2, size = (500, 2))\nX1 = tch.cat((X1, tch.ones((X1.shape[0], 1))), 1)\n\n# Not linearly separable 2D data\ny2 = tch.arange(500) &gt;= int(500 / 2)\nX2 = y2[:, None] + tch.normal(0.0, 0.4, size = (500, 2))\nX2 = tch.cat((X2, tch.ones((X2.shape[0], 1))), 1)\n\n# 6D data, low-noise\ny3 = tch.arange(500) &gt;= int(500 / 2)\nX3 = y3[:, None] + tch.normal(0.0, 0.2, size = (500, 6))\nX3 = tch.cat((X3, tch.ones((X3.shape[0], 1))), 1)\n\n# 6D data, high-noise\ny4 = tch.arange(500) &gt;= int(500 / 2)\nX4 = y4[:, None] + tch.normal(0.0, 0.5, size = (500, 6))\nX4 = tch.cat((X4, tch.ones((X4.shape[0], 1))), 1)\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nCode\n# Some code provided by Prof. Chodrow\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n\n    # Custom color map\n    colors = [\"purple\", \"darkorange\"]  \n    cmap = LinearSegmentedColormap.from_list(\"my_cmap\", colors, N=256)\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"none\", cmap = cmap, vmin = -2, vmax = 2, alpha = 0.75, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nplot_perceptron_data(X1, y1, ax[0])\nplot_perceptron_data(X2, y2, ax[1])\nax[0].set_title(\"Linearly Separable Data\")\nax[1].set_title(\"Not Linearly Separable Data\")\nfig.suptitle(\"Visualizations of the Generated Linearly Separable and Not Linearly Separable Data of Two Variables\", fontsize = 16)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above generates four random data sets: 1. Linearly separable data with two features. 2. Not linearly separable data with two features. 3. & 4. Possibly linearly separable data with 6 features (some code provided by Prof. Chodrow).\nFigure 1\nTo test and investigate the perceptron algorithm, I have created four random datasets. The first data set has two features and is intentionally linearly separable \\(-\\) the perceptron algorithm should converge to a loss of 0 for this dataset. The second data set also has two features but is intentionally not linearly separable \\(-\\) the perceptron algorithm will not be able to converge to a loss of 0 for this data set and will need to be manually terminated after a certain number of iterations. The third and fourth datasets have 6 features and are used to show how the perceptron works with data that can’t be easily visualized \\(-\\) the third dataset is linearly separable due to its low-noise factor while the fourth dataset is likely not linearly separable due to its high-noise factor.\n\n\n\nFor this introductory study, I have implemented a rudimentary version of the perceptron algorithm. This implementation involves three class definitions: LinearModel, Perceptron, and PerceptronOptimizer.\nLinearModel:\n\nself.w: An instance variable to store the weights vector of a linear model.\nscore(X): Method to compute the score \\(s_i\\) using \\(\\langle\\)\\(w\\)\\(, x_i\\rangle\\) for each data point in the feature matrix \\(X\\).\npredict(X): Method to compute the classification prediction \\(\\hat{y}_i\\) \\(\\in\\{0, 1\\}\\) for each data point:\n\n\\(\\hat{y_i} = 1\\) if (\\(s_i &gt; 0\\)) and \\(\\hat{y_i} = 0\\) otherwise.\n\n\nPerceptron (inherits from LinearModel):\n\nloss(X, y): Method to compute the misclassification rate in the data by taking the average number of misclassification instances \\(-\\) A point \\(x_i\\) is classified correctly if \\(s_iy_i' &gt; 0\\), where \\(y_i' \\in \\{-1, 1\\}\\) is the modified classification label (computed with \\(2y_i - 1\\)).\ngrad(x, y): Method to compute the perceptron update for a sampled data point.\n\nThis method takes as arguments x: the row of the feature matrix \\(X\\) corresponding to the sampled data point \\(-\\) and y: the classification target vector.\nThis method first computes the score \\(s_i\\) of the sampled data point with \\(\\langle\\)\\(w\\)\\(, x_i\\rangle\\).\nThis method then computes the vector \\(-\\mathbf{1}[s_i(2y_i - 1) &lt; 0](2y_i - 1)x_i\\) which represents the perceptron update (moving the score \\(s_i\\) closer to the target \\(y_i\\)) with respect to the sampled data point. This vector, which represents the gradient of the loss function, is defined by three components:\n\n\\(\\mathbf{1}[s_i(2y_i - 1) &lt; 0]\\) indicates whether or not the current data point \\(x_i\\) is correctly classified. If the point \\(x_i\\) is correctly classified, the whole vector evaluates to \\([\\mathbf{0}]\\), and thus no changes are made to the current weights vector, \\(\\mathbf{w}\\). If the point \\(x_i\\) is incorrectly classified, the vector evaluates to \\(-(2y_i - 1)x_i\\) which will change \\(\\mathbf{w}\\).\n\\((2y_i - 1)x_i\\) represents the update to the score data point \\(x_i\\) that occurs in the event of a misclassification (i.e. moving \\(s_i\\) closer to the value of \\(y_i\\)).\nLastly, the preceding “\\(-\\)” indicates that if the point \\(x_i\\) is misclassified, the vector \\(\\mathbf{w}\\) needs to be updated in the opposite direction of the gradient of the loss function.\n\nUltimately, this method computes an update to a sampled data point that later adjusts the weight vector of the perceptron algorithm to better fit the sampled data point.\n\n\n\n\n\n\n\nCode\n# Method to evaluate the perceptron algorithm - Some code provided by Prof. Chodrow\ndef perceptron_test(X, y, max_it, verbose):\n    \n    ## Instantiate a model and an optimizer\n    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n\n    # Initialize the loss\n    loss = 1.0\n\n    # Keeping track of loss values (length of this array is also the number of algorithm iterations)\n    loss_vec = []\n\n    n = X.size()[0]\n    while (loss &gt; 0):\n        \n        # Termination condition\n        if (max_it != None) and (len(loss_vec) &gt;= max_it):\n            break\n\n        # Tracking the evolution of the loss function\n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # Selecting a random data point\n        i = tch.randint(n, size = (1,))\n        x_i = X[[i],:]\n        y_i = y[i]\n        \n        # Performing perceptron update using the random data point\n        opt.step(x_i, y_i)\n\n    if (verbose):\n        # Observe the algorithm's performance with the evolution of the loss function\n        print(\"Evolution of loss values:\\n\")\n        for i in range(5):\n            print(f\"Iteration: {i} | Loss: {loss_vec[i]}\")\n        print(\"...\\n\")\n        print(f\"Total Iterations: {len(loss_vec)} | Final Loss: {loss_vec[-1]}\")\n\n\n\n\nCode\n# Evaluating perceptron on 2D, linearly separable data\npe1 = perceptron_test(X1, y1, None, True)\n\n\nEvolution of loss values:\n\nIteration: 0 | Loss: 0.5\nIteration: 1 | Loss: 0.1860000044107437\nIteration: 2 | Loss: 0.1860000044107437\nIteration: 3 | Loss: 0.1860000044107437\nIteration: 4 | Loss: 0.1860000044107437\n...\n\nTotal Iterations: 1282 | Final Loss: 0.0\n\n\nCode above checks the implementation of the perceptron algorithm on a linearly separable dataset of two variables (some code provided by Prof. Chodrow).\nIn the above code cell, the implementation of the perceptron algorithm is tested on the generated 2D, linearly separable data. Considering that this data is linearly separable, the algorithm should be able to converge to a loss of \\(0\\) with a finite number of iterations. In the output above, the evolution of the loss value across the first five (or less if appropriate) iterations of the perceptron algorithm followed by the loss value at the algorithm’s final iteration is displayed. This output aligns with the expected behavior of the algorithm as the loss value is shown to decrease during the initial iterations and eventually converge to a value of \\(0\\).\n\n\n\n\n\nCode\n# Plotting helper method provided by Prof. Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = tch.linspace(x_min, x_max, 101)\n    y = -1 * (((w_[0] * x) + w_[2])/w_[1])\n    ax.plot(x, y, **kwargs)\n\n# Function to plot the behavior of the perceptron algorithm\ndef perceptron_plotter(X, y, max_it, dim):\n    \n    # Code Provided by Prof. Chodrow\n    ## Initialize a perceptron \n    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n    p.loss(X, y)\n\n    # Initialize for main loop\n    loss = 1\n\n    # Bookkeeping arrays\n    loss_vec = []\n    updated_losses = []\n    old_w_vals = []\n    new_w_vals = []\n    sampled_points = []\n    update_its = []\n\n    n = X.size()[0]\n    while loss &gt; 0:\n        \n        # Terminating condition\n        if (max_it != None) and (len(loss_vec) &gt;= max_it):\n            break\n\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n\n        # Save the current value of w for plotting later\n        curr_w = tch.clone(p.w)\n        \n        # Sample random data point\n        i = tch.randint(n, size = (1,))\n        x_i = X[[i],:]\n        y_i = y[i]\n        \n        # Make an optimization step - Now p.w is the new weight vector\n        step = opt.step(x_i, y_i)\n        if step &gt; 0:\n            sampled_points.append(i)\n            old_w_vals.append(curr_w)\n            new_w_vals.append(tch.clone(p.w))\n            updated_loss = p.loss(X, y).item()\n            updated_losses.append(updated_loss)\n            update_its.append((len(loss_vec)))\n\n    filler = \" Not \" if (loss_vec[-1] &gt; 0) else \" \"\n    if (dim &lt;= 2):\n        # Plotting the algorithm procedure\n        plt.rcParams[\"figure.figsize\"] = (10, 7.5)\n        current_ax = 0\n        fig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\n        markers = [\"o\", \",\"]\n        marker_map = {-1 : 0, 1 : 1}\n        for i in range(5):\n            ax = axarr.ravel()[current_ax]\n            plot_perceptron_data(X, y, ax)\n            draw_line(old_w_vals[i], x_min = -1, x_max = 2, ax = ax, color = \"slategray\", linestyle = \"dashed\")\n            draw_line(new_w_vals[i], x_min = -1, x_max = 2, ax = ax, color = \"slategray\")\n            ax.scatter(X[sampled_points[i],0],X[sampled_points[i],1], color = \"slategray\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2 * (y1[sampled_points[i]].item()) - 1]])\n            ax.set_title(f\"Current Overall\\nLoss = {updated_losses[i]:.3f}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            ax.text(-0.55, 1.55, f\"Iteration: {update_its[i]}\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\n            current_ax += 1\n        ax = axarr.ravel()[-1]\n        plot_perceptron_data(X, y, ax)\n        draw_line(new_w_vals[-1], x_min = -1, x_max = 2, ax = ax, color = \"slategray\")\n        ax.set_title(f\"Final Overall\\nLoss = {loss_vec[-1]:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        ax.text(-0.55, 1.55, f\"Iteration: {len(loss_vec)}\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\n        fig.suptitle(\"Convergence of Loss Value and Separation Line for 2D\" + filler + \"Linearly Separable Data\", fontsize = 16)\n        fig.text(0.5, 0.005, \"Sampled Points at the Current Iteration are Outlined in Black\", ha = \"center\", va = \"center\", fontsize = 14)\n        plt.tight_layout()\n\n    return [loss_vec, update_its]\n\n# Plotting the evolution of the model loss values - Code provided by Prof. Chodrow\ndef loss_plotter(loss_vec, update_its):\n    fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n    ax.plot(loss_vec, color = \"slategray\", linestyle = \"--\")\n    ax.scatter(tch.arange(len(loss_vec)), loss_vec, color = \"purple\")\n    for i in range(len(update_its)):\n        ax.scatter(update_its[i], loss_vec[update_its[i]], color = \"darkorange\")\n    ax.set_ylabel(\"Model Loss Value\", fontsize = 14)\n    ax.set_xlabel(\"Perceptron Iteration\", fontsize = 14)\n    ax.set_title(\"Model Loss Values Across Perceptron Iterations\", fontsize = 16)\n    ax.axhline(0.0, color = \"black\", linestyle = \"--\")\n    plt.tight_layout()\n\n\n\n\n\n\nCode\ntch.manual_seed(100) # For consistent random sampling\n\n# Plotting the behavior of perceptron on 2D, linearly separable data\nlv1 = perceptron_plotter(X1, y1, None, 2)\n\n\n\n\n\n\n\n\n\nCode above re-runs the perceptron algorithm on 2D linearly separable data and plots the algorithm’s progress, displaying the previous separation line, the sampled point, and the updated separation line. The first 5 update-invoking iterations are displayed followed by the final iteration (some code provided by Prof. Chodrow).\nFigure 2:\nThe subplots above display the behavior of the perceptron algorithm on linearly separable data of two variables. In the first five subplots, the first five update-invoking iterations of the perceptron algorithm are illustrated, highlighting the current loss value of the model, marking the randomly sampled point, and identifying both the previous separation line (i.e. the previous weights vector) and the updated separation line (i.e. the updated weight vector). In the last subplot, the loss value of the model and the separation line of the final iteration of the algorithm is displayed. As expected, the final loss value is \\(0\\) and the final separation line perfectly separates the two groups of data points.\n\n\nCode\n# Plotting the evolution of the loss values on 2D, linearly separable data\nloss_plotter(lv1[0], lv1[1])\n\n\n\n\n\n\n\n\n\nCode above plots the changing model loss values across the iterations of the algorithm (some code provided by Prof. Chodrow).\nFigure 3:\nThe plot above portrays the evolution of the loss value over all iterations of the perceptron algorithm acting on 2D linearly separable data. The loss values corresponding to the update-invoking iterations are marked in orange. As displayed in this plot, the loss value generally decreases as the number of algorithm iterations increases. And, supported by the last subplot in Figure 2, the final loss value is \\(0\\).\n\n\n\n\n\nCode\n# Plotting perceptron behavior on 2D, not linearly separable data\nlv2 = perceptron_plotter(X2, y2, 1000, 2)\n\n\n\n\n\n\n\n\n\nCode above mimics the plots in Figure 2 but instead depicts the behavior of perceptron on 2D data that is not linearly separable (some code provided by Prof. Chodrow).\nFigure 4\nThe subplots above display the behavior of the perceptron algorithm on data of two variables that is not linearly separable. Similarly to Figure 2, the first five subplots show the first five update-invoking iterations of the perceptron algorithm. Each subplot highlights the current loss value of the model, marks the randomly sampled point, and identifies both the previous separation line (i.e. the previous weights vector) and the updated separation line (i.e. the updated weight vector). Again, the loss value of the model and the separation line of the final iteration (the last allotted iteration in this case as controlled by the max_it parameter) of the algorithm is displayed in the last subplot. As expected, the final loss value is \\(&gt;0\\) and the final separation line does not perfectly separate the two groups of data points.\n\n\nCode\n# Plotting the evolution of the loss value on 2D, not linearly separable data\nloss_plotter(lv2[0], lv2[1])\n\n\n\n\n\n\n\n\n\nCode above plots the changing model loss values across the updating iterations of the algorithm (some code provided by Prof. Chodrow).\nFigure 5:\nThe plot above portrays the evolution of the loss value over all iterations of the perceptron algorithm acting on 2D not linearly separable data. The loss values corresponding to the update-invoking iterations are marked in orange. As displayed in this plot, the loss value generally decreases as the number of algorithm iterations increases. However, within the number of allotted iterations, the loss value fails to converge to \\(0\\). This is supported by the last subplot in Figure 4 showing the final loss value to be \\(&gt;0\\).\n\n\n\n\n\nCode\n# Evaluating perceptron on 6D, low-noise data\nlv3 = perceptron_plotter(X3, y3, None, 6)\nloss_plotter(lv3[0], lv3[1])\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on low-noise 6D data (some code provided by Prof. Chodrow).\nFigure 6\nThe plot above again shows the evolution in the loss function value over the iterations of the perceptron algorithm acting on low-noise data of six variables (update-invoking iterations are again marked in orange). In this case, perceptron is being run on a dataset of six variables generated to have low noise. As indicated by the rightmost point in the above plot, the perceptron algorithm seems to converge to a loss value of \\(0\\) even when iterating over a dataset of a higher dimension. Given this, it appears that the low-noise, 6-variable data is linearly separable.\n\n\nCode\n# Evaluating Perceptron on 6D, high-noise data\nlv4 = perceptron_plotter(X4, y4, 1000, 6)\nloss_plotter(lv4[0], lv4[1])\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on high-noise 6D data (some code provided by Prof. Chodrow)\nFigure 7\nThe above plot now shows the evolution in the loss function value over the iterations of the perceptron algorithm \\(-\\) run on a dataset of six variables generated to have high noise (update-invoking iterations are again marked in orange). As indicated by the rightmost point in the above plot, the perceptron algorithm seems unable to converge to a loss value of \\(0\\) within the allotted number of iterations. Given this, it appears that the high-noise, 6-variable is not linearly separable. Overall, it seems that the behavior of the perceptron algorithm on higher-dimensional data is analogous to that of lower-dimensional data with respect to linear separability.\n\n\n\n\nAn alternate version of the perceptron algorithm is the “minibatch perceptron”. During a step, minibatch perceptron computes an average update vector from a batch of randomly sampled data points rather than a single update for a single data point. With a batch size of \\(k\\), the minibatch perceptron update formula is the following (note that \\(s_{k_i}\\) is the score of data point \\(x_{k_i}\\) given by \\(\\langle\\)\\(w\\)\\(, x_{k_i}\\rangle\\)):\n\\[\nw^{(t + 1)} = w^{(t)} + \\frac{\\alpha}{k}\\sum_{i = 1}^{k}\\mathbf{1}[s_{k_i}(2y_{k_i} - 1) &lt; 0](2y_{k_i} - 1)x_{k_i}\n\\]\n\n\nCode\n# Method to evaluate the minibatch perceptron algorithm - Some code provided by Prof. Chodrow\ndef MBperceptron_test(X, y, k, max_it, lr, verbose):\n    \n    ## Instantiate a model and an optimizer\n    mbp = MBPerceptron()\n    opt = MBPerceptronOptimizer(mbp)\n\n    # Initialize the loss\n    loss = 1.0\n\n    # Bookkeeping arrays\n    loss_vec = []\n    updated_losses = []\n    old_w_vals = []\n    new_w_vals = []\n    update_its = []\n\n    while (loss &gt; 0):\n        \n        # Terminating condition\n        if (max_it != None) and (len(loss_vec) &gt;= max_it):\n            break\n\n        loss = mbp.loss(X, y).item()\n        loss_vec.append(loss)\n\n        # Save the current value of w for plotting later\n        curr_w = tch.clone(mbp.w)\n        \n        # Selecting a batch of random data points\n        ix = tch.randperm(X.size(0))[:k]\n        X_k = X[ix,:]\n        y_k = y[ix]\n        \n        # Make an optimization step - Now p.w is the new weight vector\n        step = opt.step(X_k, y_k, lr)\n        if step &gt; 0:\n            old_w_vals.append(curr_w)\n            new_w_vals.append(tch.clone(mbp.w))\n            updated_loss = mbp.loss(X, y).item()\n            updated_losses.append(updated_loss)\n            update_its.append((len(loss_vec)))\n\n    if (verbose):\n        # Observe the algorithm's performance with the evolution of the loss function\n        print(f\"Evolution of loss values (with batchsize k = {k}):\\n\")\n        for i in range(5):\n            print(f\"Iteration: {i} | Loss: {loss_vec[i]}\")\n        print(\"...\\n\")\n        print(f\"Total Iterations: {len(loss_vec)} | Final Loss: {loss_vec[-1]}\\n\")\n\n    return [loss_vec, update_its]\n\n\n\n\nCode\n# Evaluating perceptron on 2D, linearly separable data\nmbp_eval = MBperceptron_test(X1, y1, 5, None, 0.5, True)\n\n\nEvolution of loss values (with batchsize k = 5):\n\nIteration: 0 | Loss: 0.492000013589859\nIteration: 1 | Loss: 0.4359999895095825\nIteration: 2 | Loss: 0.3720000088214874\nIteration: 3 | Loss: 0.17599999904632568\nIteration: 4 | Loss: 0.03200000151991844\n...\n\nTotal Iterations: 164 | Final Loss: 0.0\n\n\n\nCode above evaluates the minibatch perceptron on a given dataset, with batch size k and allotted iteration max_it adjustable (some code provided by Prof. Chodrow).\nAs displayed by the output above, when operating on 2D, linearly separable data, the minibatch perceptron algorithm is able to successfully converge to a loss value of \\(0\\). That is, the minibatch perceptron is able to determine the exact separation line for the two groups found in the data. Note that with a batch size of \\(k = 5\\), the minibatch perceptron converges over \\(1000\\) fewer iterations than the standard perceptron.\n\n\n\n\n\n\n\nCode\n# Plotting the evolution of the model loss values - Code provided by Prof. Chodrow\ndef MBloss_plotter(ds1, ds2):\n    fig, ax = plt.subplots(2, 1, figsize = (10, 7.5))\n    ax[0].plot(ds1[0], color = \"slategray\", linestyle = \"--\")\n    ax[0].scatter(tch.arange(len(ds1[0])), ds1[0], color = \"purple\")\n    for i in range(len(ds1[1])):\n        ax[0].scatter(ds1[1][i], ds1[0][ds1[1][i]], color = \"darkorange\")\n    ax[0].set_ylabel(\"Model Loss Value\", fontsize = 14)\n    ax[0].set_title(\"Linearly Separable Data\", fontsize = 16)\n    ax[0].axhline(0.0, color = \"black\", linestyle = \"--\")\n    ax[0].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n\n    if (ds2 != None):\n        ax[1].plot(ds2[0], color = \"gray\", linestyle = \"--\")\n        ax[1].scatter(tch.arange(len(ds2[0])), ds2[0], color = \"purple\")\n        for i in range(len(ds2[1])):\n            ax[1].scatter(ds2[1][i], ds2[0][ds2[1][i]], color = \"darkorange\")\n        ax[1].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n        ax[1].set_ylabel(\"Model Loss Value\", fontsize = 14)\n        ax[1].set_title(\"Not Linearly Separable Data\", fontsize = 16)\n        ax[1].axhline(0.0, color = \"black\", linestyle = \"--\")\n    fig.suptitle(\"Model Loss Values Across Minibatch Perceptron Iterations\", fontsize = 18)\n    plt.tight_layout()\n\n\n\n\nCode\n# Plotting the behavior of minibatch perceptron on 2D, linearly separable data -- k = 1\nprint(\"On 2D, linearly separable data:\\n\")\nmbp_ex1 = MBperceptron_test(X1, y1, 1, None, 0.5, True)\nprint(\"On 2D, not linearly separable data:\\n\")\nmbp_ex2 = MBperceptron_test(X2, y2, 1, 1000, 0.5, True)\nMBloss_plotter(mbp_ex1, mbp_ex2)\n\n\nOn 2D, linearly separable data:\n\nEvolution of loss values (with batchsize k = 1):\n\nIteration: 0 | Loss: 0.5\nIteration: 1 | Loss: 0.47600001096725464\nIteration: 2 | Loss: 0.47600001096725464\nIteration: 3 | Loss: 0.47600001096725464\nIteration: 4 | Loss: 0.014000000432133675\n...\n\nTotal Iterations: 135 | Final Loss: 0.0\n\nOn 2D, not linearly separable data:\n\nEvolution of loss values (with batchsize k = 1):\n\nIteration: 0 | Loss: 0.4740000069141388\nIteration: 1 | Loss: 0.4740000069141388\nIteration: 2 | Loss: 0.3179999887943268\nIteration: 3 | Loss: 0.15600000321865082\nIteration: 4 | Loss: 0.15600000321865082\n...\n\nTotal Iterations: 1000 | Final Loss: 0.07800000160932541\n\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on both linearly separable and not linearly separable 2D data using the minibatch perceptron alg. with batchsize \\(k = 1\\) (some code provided by Prof. Chodrow)\nFigure 8\nThe figures above illustrates the evolution of the loss value of the minibatch perceptron algorithm as it iterates over both linearly separable and not linearly separable 2D data (update-invoking iterations are again marked in orange). With a batch size \\(k = 1\\), the minibatch perceptron algorithm behaves exactly like the standard perceptron algorithm. That is, it converges to a loss value of \\(0\\) with linearly separable data and fails to converge to a loss value of \\(0\\) with not linearly separable data (in the number of allotted iterations).\n\n\nCode\n# Plotting the behavior of minibatch perceptron on 6D, linearly separable data -- k = 1\nprint(\"On 6D, linearly separable data:\\n\")\nmbp_ex3 = MBperceptron_test(X3, y3, 1, None, 0.5, True)\nprint(\"On 6D, not linearly separable data:\\n\")\nmbp_ex4 = MBperceptron_test(X4, y4, 1, 1000, 0.5, True)\nMBloss_plotter(mbp_ex3, mbp_ex4)\n\n\nOn 6D, linearly separable data:\n\nEvolution of loss values (with batchsize k = 1):\n\nIteration: 0 | Loss: 0.3959999978542328\nIteration: 1 | Loss: 0.09399999678134918\nIteration: 2 | Loss: 0.09399999678134918\nIteration: 3 | Loss: 0.09399999678134918\nIteration: 4 | Loss: 0.09399999678134918\n...\n\nTotal Iterations: 27 | Final Loss: 0.0\n\nOn 6D, not linearly separable data:\n\nEvolution of loss values (with batchsize k = 1):\n\nIteration: 0 | Loss: 0.27399998903274536\nIteration: 1 | Loss: 0.27399998903274536\nIteration: 2 | Loss: 0.27399998903274536\nIteration: 3 | Loss: 0.1080000028014183\nIteration: 4 | Loss: 0.1080000028014183\n...\n\nTotal Iterations: 1000 | Final Loss: 0.004000000189989805\n\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on both linearly separable and not linearly separable 6D data using the minibatch perceptron alg. with batchsize \\(k = 1\\) (some code provided by Prof. Chodrow)\nFigure 9\nThe figures above illustrates the evolution of the loss value of the minibatch perceptron algorithm as it iterates over both linearly separable and not linearly separable 6D data (update-invoking iterations are again marked in orange). With a batch size \\(k = 1\\), the minibatch perceptron algorithm again behaves similarly to the standard perceptron algorithm. That is, it converges to a loss value of \\(0\\) with linearly separable data and fails to converge to a loss value of exactly \\(0\\) with not linearly separable data (in the number of allotted iterations).\n\n\n\n\n\nCode\n# Plotting the evolution of the model loss values - Code provided by Prof. Chodrow - MODIFIED METHOD\ndef MBloss_plotter_m(ds1, ds2, nls): # nls: linear separability of the data set indicator parameter \n    filler =  \" Not \" if nls else \" \"\n    fig, ax = plt.subplots(2, 1, figsize = (10, 7.5))\n    ax[0].plot(ds1[0], color = \"slategray\", linestyle = \"--\")\n    ax[0].scatter(tch.arange(len(ds1[0])), ds1[0], color = \"purple\")\n    for i in range(len(ds1[1])):\n        ax[0].scatter(ds1[1][i], ds1[0][ds1[1][i]], color = \"darkorange\")\n    ax[0].set_ylabel(\"Model Loss Value\", fontsize = 14)\n    ax[0].set_title(\"2D Data\", fontsize = 16)\n    ax[0].axhline(0.0, color = \"black\", linestyle = \"--\")\n    ax[0].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n\n    if (ds2 != None):\n        ax[1].plot(ds2[0], color = \"gray\", linestyle = \"--\")\n        ax[1].scatter(tch.arange(len(ds2[0])), ds2[0], color = \"purple\")\n        for i in range(len(ds2[1])):\n            ax[1].scatter(ds2[1][i], ds2[0][ds2[1][i]], color = \"darkorange\")\n        ax[1].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n        ax[1].set_ylabel(\"Model Loss Value\", fontsize = 14)\n        ax[1].set_title(\"6D Data\", fontsize = 16)\n        ax[1].axhline(0.0, color = \"black\", linestyle = \"--\")\n    fig.suptitle(\"Model Loss Values Across Minibatch Perceptron Iterations on\" + filler + \"Linearly Separable Data\", fontsize = 18)\n    plt.tight_layout()\n\n\n\n\nCode\n# Plotting the behavior of minibatch perceptron on 6D, linearly separable data -- k = 10\nprint(\"On 2D, linearly separable data:\\n\")\nmbp_ex5 = MBperceptron_test(X1, y1, 10, None, 0.5, True)\nprint(\"On 6D, linearly separable data:\\n\")\nmbp_ex6 = MBperceptron_test(X3, y3, 10, None, 0.5, True)\nMBloss_plotter_m(mbp_ex5, mbp_ex6, False)\n\n\nOn 2D, linearly separable data:\n\nEvolution of loss values (with batchsize k = 10):\n\nIteration: 0 | Loss: 0.5\nIteration: 1 | Loss: 0.5\nIteration: 2 | Loss: 0.5\nIteration: 3 | Loss: 0.44200000166893005\nIteration: 4 | Loss: 0.07999999821186066\n...\n\nTotal Iterations: 281 | Final Loss: 0.0\n\nOn 6D, linearly separable data:\n\nEvolution of loss values (with batchsize k = 10):\n\nIteration: 0 | Loss: 0.25\nIteration: 1 | Loss: 0.2160000056028366\nIteration: 2 | Loss: 0.1860000044107437\nIteration: 3 | Loss: 0.15600000321865082\nIteration: 4 | Loss: 0.13600000739097595\n...\n\nTotal Iterations: 207 | Final Loss: 0.0\n\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on linearly separable of both 2D and 6D data using the minibatch perceptron alg. with batchsize \\(k = 10\\) (some code provided by Prof. Chodrow)\nFigure 10\nThe figures above illustrates the evolution of the loss value of the minibatch perceptron algorithm as it iterates linearly separable 2D and 6D data (update-invoking iterations are again marked in orange). With a batch size \\(k = 10\\), the minibatch perceptron algorithm is able to find the exact separation line of the data groups as indicated by the final loss value of \\(0\\). Interestingly, the number of iterations the minibatch perceptron takes while operating on both the linearly separable datasets is larger than that of the standard perceptron algorithm.\n\n\n\n\n\nCode\n# Plotting the behavior of minibatch perceptron on 6D, not linearly separable data -- k = n\nprint(\"On 2D, not linearly separable data:\\n\")\nmbp_ex7 = MBperceptron_test(X2, y2, X2.shape[0], 100000, 1e-3, True)\nprint(\"On 6D, not linearly separable data:\\n\")\nmbp_ex8 = MBperceptron_test(X4, y4, X4.shape[0], 100000, 1e-3, True)\n\nfig, ax = plt.subplots(2, 1, figsize = (10, 7.5))\nax[0].plot(mbp_ex7[0], color = \"purple\")\n# ax[0].scatter(tch.arange(len(mbp_ex7[0])), mbp_ex7[0], color = \"purple\")\nax[0].set_ylabel(\"Model Loss Value\", fontsize = 14)\nax[0].set_title(\"2D Data\", fontsize = 16)\nax[0].axhline(0.0, color = \"black\", linestyle = \"--\")\nax[0].axhline(min(mbp_ex7[0]), color = \"darkred\", linestyle = \"--\")\nax[0].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n\nif (mbp_ex8 != None):\n    ax[1].plot(mbp_ex8[0], color = \"darkorange\")\n    # ax[1].scatter(tch.arange(len(mbp_ex8[0])), mbp_ex8[0], color = \"purple\")\n    ax[1].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n    ax[1].set_ylabel(\"Model Loss Value\", fontsize = 14)\n    ax[1].set_title(\"6D Data\", fontsize = 16)\n    ax[1].axhline(0.0, color = \"black\", linestyle = \"--\")\n    ax[1].axhline(min(mbp_ex8[0]), color = \"darkred\", linestyle = \"--\")\nfig.suptitle(\"Model Loss Values Across Minibatch Perceptron Iterations on Not Linearly Separable Data\", fontsize = 18)\nplt.tight_layout()\n\n\nOn 2D, not linearly separable data:\n\nEvolution of loss values (with batchsize k = 500):\n\nIteration: 0 | Loss: 0.4819999933242798\nIteration: 1 | Loss: 0.4819999933242798\nIteration: 2 | Loss: 0.4819999933242798\nIteration: 3 | Loss: 0.4819999933242798\nIteration: 4 | Loss: 0.4819999933242798\n...\n\nTotal Iterations: 100000 | Final Loss: 0.03999999910593033\n\nOn 6D, not linearly separable data:\n\nEvolution of loss values (with batchsize k = 500):\n\nIteration: 0 | Loss: 0.3700000047683716\nIteration: 1 | Loss: 0.3700000047683716\nIteration: 2 | Loss: 0.3700000047683716\nIteration: 3 | Loss: 0.3700000047683716\nIteration: 4 | Loss: 0.3700000047683716\n...\n\nTotal Iterations: 44649 | Final Loss: 0.0\n\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on not linearly separable of both 2D and 6D data using the minibatch perceptron alg. with batchsize \\(k = n\\). The darkred dashed lines indicate the minimum loss values achieved (some code provided by Prof. Chodrow)\nFigure 11\nThe figures above illustrates the evolution of the loss value of the minibatch perceptron algorithm as it iterates over not linearly separable 2D and 6D data. The purpose of this experiment is to observe whether or not this algorithm will converge on data that is not linearly separable given a small enough learning rate \\(\\alpha\\). In this case, the learning rate is set to \\(\\alpha = 0.001\\). With a batch size \\(k = n\\), the minibatch perceptron algorithm does appear to converge (note that \\(100000\\) iterations were allotted). For the 2D data, the algorithm is unable to achieve a loss value of \\(0\\), but visually converges to a loss value of approximately \\(0.36\\). Interestingly, on the 6D data, the algorithm also converges and does seem to achieve a loss value of \\(0\\). In general, it appears that the minibatch perceptron is still able to converge to some decreased loss value even when operating on data that is not linearly separable (that’s cool!).\n\n\n\n\nStandard Perceptron Algorithm:\nDuring a given step of the standard perceptron algorithm, two primary computations occur. First, the current loss value of the linear model is calculated. The loss calculation involves computing the score of each data point in the feature matrix \\(\\mathbf{X}\\), multiplying each score \\(s_i\\) by the corresponding modified target \\(y_i'\\), and checking if \\(s_iy_i' &gt; 0\\). Then, the average of all values \\(s_iy_i' &gt; 0\\) is taken. Note that the loss calculation has a complexity of \\(O(p)\\) as there are \\(p\\) elements (features) in each row of \\(\\mathbf{X}\\). The second computation of a step involves subtracting the current gradient of the loss function from the current weights vector. Similarly, computing the gradient has a complexity of \\(O(p)\\). Overall, the complexity of a step of the standard perceptron algorithm is \\(O(p)\\) where \\(p\\) is the number of features in a given row of the feature matrix \\(\\mathbf{X}\\).\nMinibatch Perceptron Algorithm:\nIn general, the processes of a given step of the minibatch perceptron algorithm are identical to those of the standard algorithm, only they are computed using a random submatrix (size \\(k\\) x \\(p\\)) of \\(\\mathbf{X}\\) instead of a single row. Given this, the minibatch perceptron performs essentially the same computations as the standard algorithm but \\(k\\) times. Therefore, the complexity of a given step of the minibatch perceptron algorithm is \\(O(kp)\\) where \\(k\\) is the number of rows in the sampled submatrix of \\(\\mathbf{X}\\) and \\(p\\) is the number of features in a given row of \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "posts/post_4/index.html#generating-data",
    "href": "posts/post_4/index.html#generating-data",
    "title": "Post 4 - Implementing Perceptron",
    "section": "",
    "text": "Code\n# Including all additional imports\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib import pyplot as plt\nimport torch as tch\n\n# Porting over perceptron and minibatch perceptron implementations\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom MBperceptron import MBPerceptron, MBPerceptronOptimizer\ntch.manual_seed(100) # For consistent data generation\nplt.style.use('seaborn-v0_8-whitegrid') # For consistent plotting\n\n# Generating the data - Some code provided by Prof. Chodrow\n## Linearly separable 2D data\ny1 = tch.arange(500) &gt;= int(500 / 2)\nX1 = y1[:, None] + tch.normal(0.0, 0.2, size = (500, 2))\nX1 = tch.cat((X1, tch.ones((X1.shape[0], 1))), 1)\n\n# Not linearly separable 2D data\ny2 = tch.arange(500) &gt;= int(500 / 2)\nX2 = y2[:, None] + tch.normal(0.0, 0.4, size = (500, 2))\nX2 = tch.cat((X2, tch.ones((X2.shape[0], 1))), 1)\n\n# 6D data, low-noise\ny3 = tch.arange(500) &gt;= int(500 / 2)\nX3 = y3[:, None] + tch.normal(0.0, 0.2, size = (500, 6))\nX3 = tch.cat((X3, tch.ones((X3.shape[0], 1))), 1)\n\n# 6D data, high-noise\ny4 = tch.arange(500) &gt;= int(500 / 2)\nX4 = y4[:, None] + tch.normal(0.0, 0.5, size = (500, 6))\nX4 = tch.cat((X4, tch.ones((X4.shape[0], 1))), 1)\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nCode\n# Some code provided by Prof. Chodrow\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n\n    # Custom color map\n    colors = [\"purple\", \"darkorange\"]  \n    cmap = LinearSegmentedColormap.from_list(\"my_cmap\", colors, N=256)\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"none\", cmap = cmap, vmin = -2, vmax = 2, alpha = 0.75, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nplot_perceptron_data(X1, y1, ax[0])\nplot_perceptron_data(X2, y2, ax[1])\nax[0].set_title(\"Linearly Separable Data\")\nax[1].set_title(\"Not Linearly Separable Data\")\nfig.suptitle(\"Visualizations of the Generated Linearly Separable and Not Linearly Separable Data of Two Variables\", fontsize = 16)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nCode above generates four random data sets: 1. Linearly separable data with two features. 2. Not linearly separable data with two features. 3. & 4. Possibly linearly separable data with 6 features (some code provided by Prof. Chodrow).\nFigure 1\nTo test and investigate the perceptron algorithm, I have created four random datasets. The first data set has two features and is intentionally linearly separable \\(-\\) the perceptron algorithm should converge to a loss of 0 for this dataset. The second data set also has two features but is intentionally not linearly separable \\(-\\) the perceptron algorithm will not be able to converge to a loss of 0 for this data set and will need to be manually terminated after a certain number of iterations. The third and fourth datasets have 6 features and are used to show how the perceptron works with data that can’t be easily visualized \\(-\\) the third dataset is linearly separable due to its low-noise factor while the fourth dataset is likely not linearly separable due to its high-noise factor."
  },
  {
    "objectID": "posts/post_4/index.html#implementing-the-perceptron-algorithm",
    "href": "posts/post_4/index.html#implementing-the-perceptron-algorithm",
    "title": "Post 4 - Implementing Perceptron",
    "section": "",
    "text": "For this introductory study, I have implemented a rudimentary version of the perceptron algorithm. This implementation involves three class definitions: LinearModel, Perceptron, and PerceptronOptimizer.\nLinearModel:\n\nself.w: An instance variable to store the weights vector of a linear model.\nscore(X): Method to compute the score \\(s_i\\) using \\(\\langle\\)\\(w\\)\\(, x_i\\rangle\\) for each data point in the feature matrix \\(X\\).\npredict(X): Method to compute the classification prediction \\(\\hat{y}_i\\) \\(\\in\\{0, 1\\}\\) for each data point:\n\n\\(\\hat{y_i} = 1\\) if (\\(s_i &gt; 0\\)) and \\(\\hat{y_i} = 0\\) otherwise.\n\n\nPerceptron (inherits from LinearModel):\n\nloss(X, y): Method to compute the misclassification rate in the data by taking the average number of misclassification instances \\(-\\) A point \\(x_i\\) is classified correctly if \\(s_iy_i' &gt; 0\\), where \\(y_i' \\in \\{-1, 1\\}\\) is the modified classification label (computed with \\(2y_i - 1\\)).\ngrad(x, y): Method to compute the perceptron update for a sampled data point.\n\nThis method takes as arguments x: the row of the feature matrix \\(X\\) corresponding to the sampled data point \\(-\\) and y: the classification target vector.\nThis method first computes the score \\(s_i\\) of the sampled data point with \\(\\langle\\)\\(w\\)\\(, x_i\\rangle\\).\nThis method then computes the vector \\(-\\mathbf{1}[s_i(2y_i - 1) &lt; 0](2y_i - 1)x_i\\) which represents the perceptron update (moving the score \\(s_i\\) closer to the target \\(y_i\\)) with respect to the sampled data point. This vector, which represents the gradient of the loss function, is defined by three components:\n\n\\(\\mathbf{1}[s_i(2y_i - 1) &lt; 0]\\) indicates whether or not the current data point \\(x_i\\) is correctly classified. If the point \\(x_i\\) is correctly classified, the whole vector evaluates to \\([\\mathbf{0}]\\), and thus no changes are made to the current weights vector, \\(\\mathbf{w}\\). If the point \\(x_i\\) is incorrectly classified, the vector evaluates to \\(-(2y_i - 1)x_i\\) which will change \\(\\mathbf{w}\\).\n\\((2y_i - 1)x_i\\) represents the update to the score data point \\(x_i\\) that occurs in the event of a misclassification (i.e. moving \\(s_i\\) closer to the value of \\(y_i\\)).\nLastly, the preceding “\\(-\\)” indicates that if the point \\(x_i\\) is misclassified, the vector \\(\\mathbf{w}\\) needs to be updated in the opposite direction of the gradient of the loss function.\n\nUltimately, this method computes an update to a sampled data point that later adjusts the weight vector of the perceptron algorithm to better fit the sampled data point."
  },
  {
    "objectID": "posts/post_4/index.html#evaluating-the-perceptron-implementation",
    "href": "posts/post_4/index.html#evaluating-the-perceptron-implementation",
    "title": "Post 4 - Implementing Perceptron",
    "section": "",
    "text": "Code\n# Method to evaluate the perceptron algorithm - Some code provided by Prof. Chodrow\ndef perceptron_test(X, y, max_it, verbose):\n    \n    ## Instantiate a model and an optimizer\n    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n\n    # Initialize the loss\n    loss = 1.0\n\n    # Keeping track of loss values (length of this array is also the number of algorithm iterations)\n    loss_vec = []\n\n    n = X.size()[0]\n    while (loss &gt; 0):\n        \n        # Termination condition\n        if (max_it != None) and (len(loss_vec) &gt;= max_it):\n            break\n\n        # Tracking the evolution of the loss function\n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # Selecting a random data point\n        i = tch.randint(n, size = (1,))\n        x_i = X[[i],:]\n        y_i = y[i]\n        \n        # Performing perceptron update using the random data point\n        opt.step(x_i, y_i)\n\n    if (verbose):\n        # Observe the algorithm's performance with the evolution of the loss function\n        print(\"Evolution of loss values:\\n\")\n        for i in range(5):\n            print(f\"Iteration: {i} | Loss: {loss_vec[i]}\")\n        print(\"...\\n\")\n        print(f\"Total Iterations: {len(loss_vec)} | Final Loss: {loss_vec[-1]}\")\n\n\n\n\nCode\n# Evaluating perceptron on 2D, linearly separable data\npe1 = perceptron_test(X1, y1, None, True)\n\n\nEvolution of loss values:\n\nIteration: 0 | Loss: 0.5\nIteration: 1 | Loss: 0.1860000044107437\nIteration: 2 | Loss: 0.1860000044107437\nIteration: 3 | Loss: 0.1860000044107437\nIteration: 4 | Loss: 0.1860000044107437\n...\n\nTotal Iterations: 1282 | Final Loss: 0.0\n\n\nCode above checks the implementation of the perceptron algorithm on a linearly separable dataset of two variables (some code provided by Prof. Chodrow).\nIn the above code cell, the implementation of the perceptron algorithm is tested on the generated 2D, linearly separable data. Considering that this data is linearly separable, the algorithm should be able to converge to a loss of \\(0\\) with a finite number of iterations. In the output above, the evolution of the loss value across the first five (or less if appropriate) iterations of the perceptron algorithm followed by the loss value at the algorithm’s final iteration is displayed. This output aligns with the expected behavior of the algorithm as the loss value is shown to decrease during the initial iterations and eventually converge to a value of \\(0\\)."
  },
  {
    "objectID": "posts/post_4/index.html#experimenting-with-perceptron",
    "href": "posts/post_4/index.html#experimenting-with-perceptron",
    "title": "Post 4 - Implementing Perceptron",
    "section": "",
    "text": "Code\n# Plotting helper method provided by Prof. Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = tch.linspace(x_min, x_max, 101)\n    y = -1 * (((w_[0] * x) + w_[2])/w_[1])\n    ax.plot(x, y, **kwargs)\n\n# Function to plot the behavior of the perceptron algorithm\ndef perceptron_plotter(X, y, max_it, dim):\n    \n    # Code Provided by Prof. Chodrow\n    ## Initialize a perceptron \n    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n    p.loss(X, y)\n\n    # Initialize for main loop\n    loss = 1\n\n    # Bookkeeping arrays\n    loss_vec = []\n    updated_losses = []\n    old_w_vals = []\n    new_w_vals = []\n    sampled_points = []\n    update_its = []\n\n    n = X.size()[0]\n    while loss &gt; 0:\n        \n        # Terminating condition\n        if (max_it != None) and (len(loss_vec) &gt;= max_it):\n            break\n\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n\n        # Save the current value of w for plotting later\n        curr_w = tch.clone(p.w)\n        \n        # Sample random data point\n        i = tch.randint(n, size = (1,))\n        x_i = X[[i],:]\n        y_i = y[i]\n        \n        # Make an optimization step - Now p.w is the new weight vector\n        step = opt.step(x_i, y_i)\n        if step &gt; 0:\n            sampled_points.append(i)\n            old_w_vals.append(curr_w)\n            new_w_vals.append(tch.clone(p.w))\n            updated_loss = p.loss(X, y).item()\n            updated_losses.append(updated_loss)\n            update_its.append((len(loss_vec)))\n\n    filler = \" Not \" if (loss_vec[-1] &gt; 0) else \" \"\n    if (dim &lt;= 2):\n        # Plotting the algorithm procedure\n        plt.rcParams[\"figure.figsize\"] = (10, 7.5)\n        current_ax = 0\n        fig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\n        markers = [\"o\", \",\"]\n        marker_map = {-1 : 0, 1 : 1}\n        for i in range(5):\n            ax = axarr.ravel()[current_ax]\n            plot_perceptron_data(X, y, ax)\n            draw_line(old_w_vals[i], x_min = -1, x_max = 2, ax = ax, color = \"slategray\", linestyle = \"dashed\")\n            draw_line(new_w_vals[i], x_min = -1, x_max = 2, ax = ax, color = \"slategray\")\n            ax.scatter(X[sampled_points[i],0],X[sampled_points[i],1], color = \"slategray\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2 * (y1[sampled_points[i]].item()) - 1]])\n            ax.set_title(f\"Current Overall\\nLoss = {updated_losses[i]:.3f}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            ax.text(-0.55, 1.55, f\"Iteration: {update_its[i]}\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\n            current_ax += 1\n        ax = axarr.ravel()[-1]\n        plot_perceptron_data(X, y, ax)\n        draw_line(new_w_vals[-1], x_min = -1, x_max = 2, ax = ax, color = \"slategray\")\n        ax.set_title(f\"Final Overall\\nLoss = {loss_vec[-1]:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        ax.text(-0.55, 1.55, f\"Iteration: {len(loss_vec)}\", bbox = dict(facecolor = \"white\", alpha = 0.75, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\n        fig.suptitle(\"Convergence of Loss Value and Separation Line for 2D\" + filler + \"Linearly Separable Data\", fontsize = 16)\n        fig.text(0.5, 0.005, \"Sampled Points at the Current Iteration are Outlined in Black\", ha = \"center\", va = \"center\", fontsize = 14)\n        plt.tight_layout()\n\n    return [loss_vec, update_its]\n\n# Plotting the evolution of the model loss values - Code provided by Prof. Chodrow\ndef loss_plotter(loss_vec, update_its):\n    fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n    ax.plot(loss_vec, color = \"slategray\", linestyle = \"--\")\n    ax.scatter(tch.arange(len(loss_vec)), loss_vec, color = \"purple\")\n    for i in range(len(update_its)):\n        ax.scatter(update_its[i], loss_vec[update_its[i]], color = \"darkorange\")\n    ax.set_ylabel(\"Model Loss Value\", fontsize = 14)\n    ax.set_xlabel(\"Perceptron Iteration\", fontsize = 14)\n    ax.set_title(\"Model Loss Values Across Perceptron Iterations\", fontsize = 16)\n    ax.axhline(0.0, color = \"black\", linestyle = \"--\")\n    plt.tight_layout()\n\n\n\n\n\n\nCode\ntch.manual_seed(100) # For consistent random sampling\n\n# Plotting the behavior of perceptron on 2D, linearly separable data\nlv1 = perceptron_plotter(X1, y1, None, 2)\n\n\n\n\n\n\n\n\n\nCode above re-runs the perceptron algorithm on 2D linearly separable data and plots the algorithm’s progress, displaying the previous separation line, the sampled point, and the updated separation line. The first 5 update-invoking iterations are displayed followed by the final iteration (some code provided by Prof. Chodrow).\nFigure 2:\nThe subplots above display the behavior of the perceptron algorithm on linearly separable data of two variables. In the first five subplots, the first five update-invoking iterations of the perceptron algorithm are illustrated, highlighting the current loss value of the model, marking the randomly sampled point, and identifying both the previous separation line (i.e. the previous weights vector) and the updated separation line (i.e. the updated weight vector). In the last subplot, the loss value of the model and the separation line of the final iteration of the algorithm is displayed. As expected, the final loss value is \\(0\\) and the final separation line perfectly separates the two groups of data points.\n\n\nCode\n# Plotting the evolution of the loss values on 2D, linearly separable data\nloss_plotter(lv1[0], lv1[1])\n\n\n\n\n\n\n\n\n\nCode above plots the changing model loss values across the iterations of the algorithm (some code provided by Prof. Chodrow).\nFigure 3:\nThe plot above portrays the evolution of the loss value over all iterations of the perceptron algorithm acting on 2D linearly separable data. The loss values corresponding to the update-invoking iterations are marked in orange. As displayed in this plot, the loss value generally decreases as the number of algorithm iterations increases. And, supported by the last subplot in Figure 2, the final loss value is \\(0\\).\n\n\n\n\n\nCode\n# Plotting perceptron behavior on 2D, not linearly separable data\nlv2 = perceptron_plotter(X2, y2, 1000, 2)\n\n\n\n\n\n\n\n\n\nCode above mimics the plots in Figure 2 but instead depicts the behavior of perceptron on 2D data that is not linearly separable (some code provided by Prof. Chodrow).\nFigure 4\nThe subplots above display the behavior of the perceptron algorithm on data of two variables that is not linearly separable. Similarly to Figure 2, the first five subplots show the first five update-invoking iterations of the perceptron algorithm. Each subplot highlights the current loss value of the model, marks the randomly sampled point, and identifies both the previous separation line (i.e. the previous weights vector) and the updated separation line (i.e. the updated weight vector). Again, the loss value of the model and the separation line of the final iteration (the last allotted iteration in this case as controlled by the max_it parameter) of the algorithm is displayed in the last subplot. As expected, the final loss value is \\(&gt;0\\) and the final separation line does not perfectly separate the two groups of data points.\n\n\nCode\n# Plotting the evolution of the loss value on 2D, not linearly separable data\nloss_plotter(lv2[0], lv2[1])\n\n\n\n\n\n\n\n\n\nCode above plots the changing model loss values across the updating iterations of the algorithm (some code provided by Prof. Chodrow).\nFigure 5:\nThe plot above portrays the evolution of the loss value over all iterations of the perceptron algorithm acting on 2D not linearly separable data. The loss values corresponding to the update-invoking iterations are marked in orange. As displayed in this plot, the loss value generally decreases as the number of algorithm iterations increases. However, within the number of allotted iterations, the loss value fails to converge to \\(0\\). This is supported by the last subplot in Figure 4 showing the final loss value to be \\(&gt;0\\).\n\n\n\n\n\nCode\n# Evaluating perceptron on 6D, low-noise data\nlv3 = perceptron_plotter(X3, y3, None, 6)\nloss_plotter(lv3[0], lv3[1])\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on low-noise 6D data (some code provided by Prof. Chodrow).\nFigure 6\nThe plot above again shows the evolution in the loss function value over the iterations of the perceptron algorithm acting on low-noise data of six variables (update-invoking iterations are again marked in orange). In this case, perceptron is being run on a dataset of six variables generated to have low noise. As indicated by the rightmost point in the above plot, the perceptron algorithm seems to converge to a loss value of \\(0\\) even when iterating over a dataset of a higher dimension. Given this, it appears that the low-noise, 6-variable data is linearly separable.\n\n\nCode\n# Evaluating Perceptron on 6D, high-noise data\nlv4 = perceptron_plotter(X4, y4, 1000, 6)\nloss_plotter(lv4[0], lv4[1])\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on high-noise 6D data (some code provided by Prof. Chodrow)\nFigure 7\nThe above plot now shows the evolution in the loss function value over the iterations of the perceptron algorithm \\(-\\) run on a dataset of six variables generated to have high noise (update-invoking iterations are again marked in orange). As indicated by the rightmost point in the above plot, the perceptron algorithm seems unable to converge to a loss value of \\(0\\) within the allotted number of iterations. Given this, it appears that the high-noise, 6-variable is not linearly separable. Overall, it seems that the behavior of the perceptron algorithm on higher-dimensional data is analogous to that of lower-dimensional data with respect to linear separability."
  },
  {
    "objectID": "posts/post_4/index.html#minibatch-perceptron",
    "href": "posts/post_4/index.html#minibatch-perceptron",
    "title": "Post 4 - Implementing Perceptron",
    "section": "",
    "text": "An alternate version of the perceptron algorithm is the “minibatch perceptron”. During a step, minibatch perceptron computes an average update vector from a batch of randomly sampled data points rather than a single update for a single data point. With a batch size of \\(k\\), the minibatch perceptron update formula is the following (note that \\(s_{k_i}\\) is the score of data point \\(x_{k_i}\\) given by \\(\\langle\\)\\(w\\)\\(, x_{k_i}\\rangle\\)):\n\\[\nw^{(t + 1)} = w^{(t)} + \\frac{\\alpha}{k}\\sum_{i = 1}^{k}\\mathbf{1}[s_{k_i}(2y_{k_i} - 1) &lt; 0](2y_{k_i} - 1)x_{k_i}\n\\]\n\n\nCode\n# Method to evaluate the minibatch perceptron algorithm - Some code provided by Prof. Chodrow\ndef MBperceptron_test(X, y, k, max_it, lr, verbose):\n    \n    ## Instantiate a model and an optimizer\n    mbp = MBPerceptron()\n    opt = MBPerceptronOptimizer(mbp)\n\n    # Initialize the loss\n    loss = 1.0\n\n    # Bookkeeping arrays\n    loss_vec = []\n    updated_losses = []\n    old_w_vals = []\n    new_w_vals = []\n    update_its = []\n\n    while (loss &gt; 0):\n        \n        # Terminating condition\n        if (max_it != None) and (len(loss_vec) &gt;= max_it):\n            break\n\n        loss = mbp.loss(X, y).item()\n        loss_vec.append(loss)\n\n        # Save the current value of w for plotting later\n        curr_w = tch.clone(mbp.w)\n        \n        # Selecting a batch of random data points\n        ix = tch.randperm(X.size(0))[:k]\n        X_k = X[ix,:]\n        y_k = y[ix]\n        \n        # Make an optimization step - Now p.w is the new weight vector\n        step = opt.step(X_k, y_k, lr)\n        if step &gt; 0:\n            old_w_vals.append(curr_w)\n            new_w_vals.append(tch.clone(mbp.w))\n            updated_loss = mbp.loss(X, y).item()\n            updated_losses.append(updated_loss)\n            update_its.append((len(loss_vec)))\n\n    if (verbose):\n        # Observe the algorithm's performance with the evolution of the loss function\n        print(f\"Evolution of loss values (with batchsize k = {k}):\\n\")\n        for i in range(5):\n            print(f\"Iteration: {i} | Loss: {loss_vec[i]}\")\n        print(\"...\\n\")\n        print(f\"Total Iterations: {len(loss_vec)} | Final Loss: {loss_vec[-1]}\\n\")\n\n    return [loss_vec, update_its]\n\n\n\n\nCode\n# Evaluating perceptron on 2D, linearly separable data\nmbp_eval = MBperceptron_test(X1, y1, 5, None, 0.5, True)\n\n\nEvolution of loss values (with batchsize k = 5):\n\nIteration: 0 | Loss: 0.492000013589859\nIteration: 1 | Loss: 0.4359999895095825\nIteration: 2 | Loss: 0.3720000088214874\nIteration: 3 | Loss: 0.17599999904632568\nIteration: 4 | Loss: 0.03200000151991844\n...\n\nTotal Iterations: 164 | Final Loss: 0.0\n\n\n\nCode above evaluates the minibatch perceptron on a given dataset, with batch size k and allotted iteration max_it adjustable (some code provided by Prof. Chodrow).\nAs displayed by the output above, when operating on 2D, linearly separable data, the minibatch perceptron algorithm is able to successfully converge to a loss value of \\(0\\). That is, the minibatch perceptron is able to determine the exact separation line for the two groups found in the data. Note that with a batch size of \\(k = 5\\), the minibatch perceptron converges over \\(1000\\) fewer iterations than the standard perceptron."
  },
  {
    "objectID": "posts/post_4/index.html#experimenting-with-minibatch-perceptron",
    "href": "posts/post_4/index.html#experimenting-with-minibatch-perceptron",
    "title": "Post 4 - Implementing Perceptron",
    "section": "",
    "text": "Code\n# Plotting the evolution of the model loss values - Code provided by Prof. Chodrow\ndef MBloss_plotter(ds1, ds2):\n    fig, ax = plt.subplots(2, 1, figsize = (10, 7.5))\n    ax[0].plot(ds1[0], color = \"slategray\", linestyle = \"--\")\n    ax[0].scatter(tch.arange(len(ds1[0])), ds1[0], color = \"purple\")\n    for i in range(len(ds1[1])):\n        ax[0].scatter(ds1[1][i], ds1[0][ds1[1][i]], color = \"darkorange\")\n    ax[0].set_ylabel(\"Model Loss Value\", fontsize = 14)\n    ax[0].set_title(\"Linearly Separable Data\", fontsize = 16)\n    ax[0].axhline(0.0, color = \"black\", linestyle = \"--\")\n    ax[0].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n\n    if (ds2 != None):\n        ax[1].plot(ds2[0], color = \"gray\", linestyle = \"--\")\n        ax[1].scatter(tch.arange(len(ds2[0])), ds2[0], color = \"purple\")\n        for i in range(len(ds2[1])):\n            ax[1].scatter(ds2[1][i], ds2[0][ds2[1][i]], color = \"darkorange\")\n        ax[1].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n        ax[1].set_ylabel(\"Model Loss Value\", fontsize = 14)\n        ax[1].set_title(\"Not Linearly Separable Data\", fontsize = 16)\n        ax[1].axhline(0.0, color = \"black\", linestyle = \"--\")\n    fig.suptitle(\"Model Loss Values Across Minibatch Perceptron Iterations\", fontsize = 18)\n    plt.tight_layout()\n\n\n\n\nCode\n# Plotting the behavior of minibatch perceptron on 2D, linearly separable data -- k = 1\nprint(\"On 2D, linearly separable data:\\n\")\nmbp_ex1 = MBperceptron_test(X1, y1, 1, None, 0.5, True)\nprint(\"On 2D, not linearly separable data:\\n\")\nmbp_ex2 = MBperceptron_test(X2, y2, 1, 1000, 0.5, True)\nMBloss_plotter(mbp_ex1, mbp_ex2)\n\n\nOn 2D, linearly separable data:\n\nEvolution of loss values (with batchsize k = 1):\n\nIteration: 0 | Loss: 0.5\nIteration: 1 | Loss: 0.47600001096725464\nIteration: 2 | Loss: 0.47600001096725464\nIteration: 3 | Loss: 0.47600001096725464\nIteration: 4 | Loss: 0.014000000432133675\n...\n\nTotal Iterations: 135 | Final Loss: 0.0\n\nOn 2D, not linearly separable data:\n\nEvolution of loss values (with batchsize k = 1):\n\nIteration: 0 | Loss: 0.4740000069141388\nIteration: 1 | Loss: 0.4740000069141388\nIteration: 2 | Loss: 0.3179999887943268\nIteration: 3 | Loss: 0.15600000321865082\nIteration: 4 | Loss: 0.15600000321865082\n...\n\nTotal Iterations: 1000 | Final Loss: 0.07800000160932541\n\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on both linearly separable and not linearly separable 2D data using the minibatch perceptron alg. with batchsize \\(k = 1\\) (some code provided by Prof. Chodrow)\nFigure 8\nThe figures above illustrates the evolution of the loss value of the minibatch perceptron algorithm as it iterates over both linearly separable and not linearly separable 2D data (update-invoking iterations are again marked in orange). With a batch size \\(k = 1\\), the minibatch perceptron algorithm behaves exactly like the standard perceptron algorithm. That is, it converges to a loss value of \\(0\\) with linearly separable data and fails to converge to a loss value of \\(0\\) with not linearly separable data (in the number of allotted iterations).\n\n\nCode\n# Plotting the behavior of minibatch perceptron on 6D, linearly separable data -- k = 1\nprint(\"On 6D, linearly separable data:\\n\")\nmbp_ex3 = MBperceptron_test(X3, y3, 1, None, 0.5, True)\nprint(\"On 6D, not linearly separable data:\\n\")\nmbp_ex4 = MBperceptron_test(X4, y4, 1, 1000, 0.5, True)\nMBloss_plotter(mbp_ex3, mbp_ex4)\n\n\nOn 6D, linearly separable data:\n\nEvolution of loss values (with batchsize k = 1):\n\nIteration: 0 | Loss: 0.3959999978542328\nIteration: 1 | Loss: 0.09399999678134918\nIteration: 2 | Loss: 0.09399999678134918\nIteration: 3 | Loss: 0.09399999678134918\nIteration: 4 | Loss: 0.09399999678134918\n...\n\nTotal Iterations: 27 | Final Loss: 0.0\n\nOn 6D, not linearly separable data:\n\nEvolution of loss values (with batchsize k = 1):\n\nIteration: 0 | Loss: 0.27399998903274536\nIteration: 1 | Loss: 0.27399998903274536\nIteration: 2 | Loss: 0.27399998903274536\nIteration: 3 | Loss: 0.1080000028014183\nIteration: 4 | Loss: 0.1080000028014183\n...\n\nTotal Iterations: 1000 | Final Loss: 0.004000000189989805\n\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on both linearly separable and not linearly separable 6D data using the minibatch perceptron alg. with batchsize \\(k = 1\\) (some code provided by Prof. Chodrow)\nFigure 9\nThe figures above illustrates the evolution of the loss value of the minibatch perceptron algorithm as it iterates over both linearly separable and not linearly separable 6D data (update-invoking iterations are again marked in orange). With a batch size \\(k = 1\\), the minibatch perceptron algorithm again behaves similarly to the standard perceptron algorithm. That is, it converges to a loss value of \\(0\\) with linearly separable data and fails to converge to a loss value of exactly \\(0\\) with not linearly separable data (in the number of allotted iterations).\n\n\n\n\n\nCode\n# Plotting the evolution of the model loss values - Code provided by Prof. Chodrow - MODIFIED METHOD\ndef MBloss_plotter_m(ds1, ds2, nls): # nls: linear separability of the data set indicator parameter \n    filler =  \" Not \" if nls else \" \"\n    fig, ax = plt.subplots(2, 1, figsize = (10, 7.5))\n    ax[0].plot(ds1[0], color = \"slategray\", linestyle = \"--\")\n    ax[0].scatter(tch.arange(len(ds1[0])), ds1[0], color = \"purple\")\n    for i in range(len(ds1[1])):\n        ax[0].scatter(ds1[1][i], ds1[0][ds1[1][i]], color = \"darkorange\")\n    ax[0].set_ylabel(\"Model Loss Value\", fontsize = 14)\n    ax[0].set_title(\"2D Data\", fontsize = 16)\n    ax[0].axhline(0.0, color = \"black\", linestyle = \"--\")\n    ax[0].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n\n    if (ds2 != None):\n        ax[1].plot(ds2[0], color = \"gray\", linestyle = \"--\")\n        ax[1].scatter(tch.arange(len(ds2[0])), ds2[0], color = \"purple\")\n        for i in range(len(ds2[1])):\n            ax[1].scatter(ds2[1][i], ds2[0][ds2[1][i]], color = \"darkorange\")\n        ax[1].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n        ax[1].set_ylabel(\"Model Loss Value\", fontsize = 14)\n        ax[1].set_title(\"6D Data\", fontsize = 16)\n        ax[1].axhline(0.0, color = \"black\", linestyle = \"--\")\n    fig.suptitle(\"Model Loss Values Across Minibatch Perceptron Iterations on\" + filler + \"Linearly Separable Data\", fontsize = 18)\n    plt.tight_layout()\n\n\n\n\nCode\n# Plotting the behavior of minibatch perceptron on 6D, linearly separable data -- k = 10\nprint(\"On 2D, linearly separable data:\\n\")\nmbp_ex5 = MBperceptron_test(X1, y1, 10, None, 0.5, True)\nprint(\"On 6D, linearly separable data:\\n\")\nmbp_ex6 = MBperceptron_test(X3, y3, 10, None, 0.5, True)\nMBloss_plotter_m(mbp_ex5, mbp_ex6, False)\n\n\nOn 2D, linearly separable data:\n\nEvolution of loss values (with batchsize k = 10):\n\nIteration: 0 | Loss: 0.5\nIteration: 1 | Loss: 0.5\nIteration: 2 | Loss: 0.5\nIteration: 3 | Loss: 0.44200000166893005\nIteration: 4 | Loss: 0.07999999821186066\n...\n\nTotal Iterations: 281 | Final Loss: 0.0\n\nOn 6D, linearly separable data:\n\nEvolution of loss values (with batchsize k = 10):\n\nIteration: 0 | Loss: 0.25\nIteration: 1 | Loss: 0.2160000056028366\nIteration: 2 | Loss: 0.1860000044107437\nIteration: 3 | Loss: 0.15600000321865082\nIteration: 4 | Loss: 0.13600000739097595\n...\n\nTotal Iterations: 207 | Final Loss: 0.0\n\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on linearly separable of both 2D and 6D data using the minibatch perceptron alg. with batchsize \\(k = 10\\) (some code provided by Prof. Chodrow)\nFigure 10\nThe figures above illustrates the evolution of the loss value of the minibatch perceptron algorithm as it iterates linearly separable 2D and 6D data (update-invoking iterations are again marked in orange). With a batch size \\(k = 10\\), the minibatch perceptron algorithm is able to find the exact separation line of the data groups as indicated by the final loss value of \\(0\\). Interestingly, the number of iterations the minibatch perceptron takes while operating on both the linearly separable datasets is larger than that of the standard perceptron algorithm.\n\n\n\n\n\nCode\n# Plotting the behavior of minibatch perceptron on 6D, not linearly separable data -- k = n\nprint(\"On 2D, not linearly separable data:\\n\")\nmbp_ex7 = MBperceptron_test(X2, y2, X2.shape[0], 100000, 1e-3, True)\nprint(\"On 6D, not linearly separable data:\\n\")\nmbp_ex8 = MBperceptron_test(X4, y4, X4.shape[0], 100000, 1e-3, True)\n\nfig, ax = plt.subplots(2, 1, figsize = (10, 7.5))\nax[0].plot(mbp_ex7[0], color = \"purple\")\n# ax[0].scatter(tch.arange(len(mbp_ex7[0])), mbp_ex7[0], color = \"purple\")\nax[0].set_ylabel(\"Model Loss Value\", fontsize = 14)\nax[0].set_title(\"2D Data\", fontsize = 16)\nax[0].axhline(0.0, color = \"black\", linestyle = \"--\")\nax[0].axhline(min(mbp_ex7[0]), color = \"darkred\", linestyle = \"--\")\nax[0].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n\nif (mbp_ex8 != None):\n    ax[1].plot(mbp_ex8[0], color = \"darkorange\")\n    # ax[1].scatter(tch.arange(len(mbp_ex8[0])), mbp_ex8[0], color = \"purple\")\n    ax[1].set_xlabel(\"M.B. Perceptron Iteration\", fontsize = 14)\n    ax[1].set_ylabel(\"Model Loss Value\", fontsize = 14)\n    ax[1].set_title(\"6D Data\", fontsize = 16)\n    ax[1].axhline(0.0, color = \"black\", linestyle = \"--\")\n    ax[1].axhline(min(mbp_ex8[0]), color = \"darkred\", linestyle = \"--\")\nfig.suptitle(\"Model Loss Values Across Minibatch Perceptron Iterations on Not Linearly Separable Data\", fontsize = 18)\nplt.tight_layout()\n\n\nOn 2D, not linearly separable data:\n\nEvolution of loss values (with batchsize k = 500):\n\nIteration: 0 | Loss: 0.4819999933242798\nIteration: 1 | Loss: 0.4819999933242798\nIteration: 2 | Loss: 0.4819999933242798\nIteration: 3 | Loss: 0.4819999933242798\nIteration: 4 | Loss: 0.4819999933242798\n...\n\nTotal Iterations: 100000 | Final Loss: 0.03999999910593033\n\nOn 6D, not linearly separable data:\n\nEvolution of loss values (with batchsize k = 500):\n\nIteration: 0 | Loss: 0.3700000047683716\nIteration: 1 | Loss: 0.3700000047683716\nIteration: 2 | Loss: 0.3700000047683716\nIteration: 3 | Loss: 0.3700000047683716\nIteration: 4 | Loss: 0.3700000047683716\n...\n\nTotal Iterations: 44649 | Final Loss: 0.0\n\n\n\n\n\n\n\n\n\n\nCode above displays the evolution of the model loss value on not linearly separable of both 2D and 6D data using the minibatch perceptron alg. with batchsize \\(k = n\\). The darkred dashed lines indicate the minimum loss values achieved (some code provided by Prof. Chodrow)\nFigure 11\nThe figures above illustrates the evolution of the loss value of the minibatch perceptron algorithm as it iterates over not linearly separable 2D and 6D data. The purpose of this experiment is to observe whether or not this algorithm will converge on data that is not linearly separable given a small enough learning rate \\(\\alpha\\). In this case, the learning rate is set to \\(\\alpha = 0.001\\). With a batch size \\(k = n\\), the minibatch perceptron algorithm does appear to converge (note that \\(100000\\) iterations were allotted). For the 2D data, the algorithm is unable to achieve a loss value of \\(0\\), but visually converges to a loss value of approximately \\(0.36\\). Interestingly, on the 6D data, the algorithm also converges and does seem to achieve a loss value of \\(0\\). In general, it appears that the minibatch perceptron is still able to converge to some decreased loss value even when operating on data that is not linearly separable (that’s cool!)."
  },
  {
    "objectID": "posts/post_4/index.html#runtime-analysis",
    "href": "posts/post_4/index.html#runtime-analysis",
    "title": "Post 4 - Implementing Perceptron",
    "section": "",
    "text": "Standard Perceptron Algorithm:\nDuring a given step of the standard perceptron algorithm, two primary computations occur. First, the current loss value of the linear model is calculated. The loss calculation involves computing the score of each data point in the feature matrix \\(\\mathbf{X}\\), multiplying each score \\(s_i\\) by the corresponding modified target \\(y_i'\\), and checking if \\(s_iy_i' &gt; 0\\). Then, the average of all values \\(s_iy_i' &gt; 0\\) is taken. Note that the loss calculation has a complexity of \\(O(p)\\) as there are \\(p\\) elements (features) in each row of \\(\\mathbf{X}\\). The second computation of a step involves subtracting the current gradient of the loss function from the current weights vector. Similarly, computing the gradient has a complexity of \\(O(p)\\). Overall, the complexity of a step of the standard perceptron algorithm is \\(O(p)\\) where \\(p\\) is the number of features in a given row of the feature matrix \\(\\mathbf{X}\\).\nMinibatch Perceptron Algorithm:\nIn general, the processes of a given step of the minibatch perceptron algorithm are identical to those of the standard algorithm, only they are computed using a random submatrix (size \\(k\\) x \\(p\\)) of \\(\\mathbf{X}\\) instead of a single row. Given this, the minibatch perceptron performs essentially the same computations as the standard algorithm but \\(k\\) times. Therefore, the complexity of a given step of the minibatch perceptron algorithm is \\(O(kp)\\) where \\(k\\) is the number of rows in the sampled submatrix of \\(\\mathbf{X}\\) and \\(p\\) is the number of features in a given row of \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "posts/post_2/index.html",
    "href": "posts/post_2/index.html",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "",
    "text": "The following study is an introductory exploration of an automated decision model. The primary aims of this analysis are to develop a deeper understanding of binary decision-making circumstances and how the use of ML models/algorithms can be used to assist and even automate this process. This analysis takes on a realistic problem using simulated data. Based on several characteristics of a potential borrower – like their financial status, reasons for seeking a loan, and age – is it wise or ill-advised to approve them for a given loan? The sections below attempt to produce a reasonable answer to this question through the creation of an automated decision-making model.\nThe general methodologies for this study involve an initial exploration of the data, constructing a model using certain features, and determining an optimal threshold based on a defined scoring function. It is necessary to observe some general patterns and trends present in the data before building a model. After the preliminary visualization and preprocessing steps, an initial model will be constructed and further refined using an iterative feature selection process supported by cross-validation. After an improved (by training accuracy) model is obtained, a scoring function will be defined using the weights determined by the best model version. With this scoring function, an optimal threshold can then be examined and set to maximize desirable values such as model accuracy and bank profit. Finally, with an optimal threshold selected and a refined model, the behavior of this model on unseen data can be evaluated. Following the final model’s evaluation this study concludes with a discussion of the implications behind automated decision algorithms with respect to general “fairness” from the perspective of all stakeholders (model designers, model users, and all impacted individuals, and etc.).\n\n\n\n\nCode\n# Includuing all additional imports\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport itertools\n\ntrain = pd.read_csv(\"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\")\n\n\nIncluding all additional imports\nThe data used in this analysis is a simulated data set of credit bureau data based on patterns observed in real credit bureau data.\n\n\n\n\n\n\n\nCode\n# Data modification\ntrain_viz = train.copy()\nloan_status_recode = {0: \"Paid in Full\", 1: \"Defaulted\"}\nloan_intent_recode = {\"VENTURE\": \"Venture\", \"EDUCATION\": \"Education\", \"MEDICAL\": \"Medical\", \"HOMEIMPROVEMENT\": \"Home Improvement\", \"PERSONAL\": \"Personal\",\n \"DEBTCONSOLIDATION\" : \"Debt Consolidation\"}\ntrain_viz[\"loan_repayment\"] = train_viz[\"loan_status\"].map(loan_status_recode)\ntrain_viz[\"loan_intent\"] = train_viz[\"loan_intent\"].map(loan_intent_recode)\ntrain_viz.loc[train_viz[\"person_age\"] &gt;= 100, \"person_age\"] = None\ntrain_viz = train_viz.dropna()\n\n# Subsetting data by loan status\ndefaulted = train_viz[train_viz[\"loan_status\"] == 1].copy().dropna()\nrepaid = train_viz[train_viz[\"loan_status\"] == 0].copy().dropna()\n\n\nModifying training data for visualization\n\n\nCode\n# Creating linear regression models and calculating R^2 values\n## Defaulted\nc1 = np.polyfit(defaulted[\"loan_amnt\"], defaulted[\"person_income\"], 1)\np1 = np.polyval(c1, defaulted[\"loan_amnt\"])\nr1 = defaulted[\"person_income\"] - p1\nssr1 = np.sum(r1 ** 2)\nsst1 = np.sum((defaulted[\"person_income\"] - np.mean(defaulted[\"loan_amnt\"])) ** 2)\nrs1 = 1 - (ssr1 / sst1)\n\n## Repaid\nc2 = np.polyfit(repaid[\"loan_amnt\"], repaid[\"person_income\"], 1)\np2 = np.polyval(c2, repaid[\"loan_amnt\"])\nr2 = repaid[\"person_income\"] - p2\nssr2 = np.sum(r2**2)\nsst2 = np.sum((repaid[\"person_income\"] - np.mean(repaid[\"loan_amnt\"]))**2)\nrs2 = 1 - (ssr2 / sst2)\n\n\nCreating linear regression models and calculating R^2 values for subsets of defaulted and fully repaid loans\n\n\nCode\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize = (15, 10))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Scatterplot and regression line for borrower income by loan amount of defaulted loans\nax[0] = sns.scatterplot(data = defaulted, x = \"loan_amnt\", y = \"person_income\", color = \"purple\", edgecolor = \"purple\", alpha = 0.25, ax = ax[0])\nsns.regplot(data = defaulted, x = \"loan_amnt\", y = \"person_income\", scatter = False, line_kws={\"color\": \"darkorange\"}, ax = ax[0])\nax[0].set_yscale(\"log\", base = 2)\nax[0].set_xlabel(\"\")\nax[0].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000, 35000])\nax[0].set_xticklabels([\"$0\", \"$5000\", \"$10000\", \"$15000\", \"$20000\", \"$25000\", \"$30000\", \"$35000\"], rotation = 30, fontsize = 14)\nax[0].set_ylabel(f\"Borrower\\'s Income ($\\log_2$ scale)\", fontsize = 16, labelpad = 15)\nax[0].set_yticks([2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23])\nax[0].set_yticklabels([\"&gt;$4000\", \"&gt;$8000\", \"&gt;$16000\", \"&gt;$32000\", \"&gt;$64000\", \"&gt;$128000\", \"&gt;$256000\", \"&gt;$512000\", \"&gt;$1024000\", \"&gt;$2048000\", \"&gt;$4096000\", \"&lt;$8192000\"], rotation = 15, fontsize = 14)\nax[0].set_title(\"Defaulted Loans\", fontsize = 18)\nax[0].text(27000, 12288, f\"     $R^2 = {rs1:.3f}$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(24000, 12288, \"\\u2013\", color = \"darkorange\", ha = \"left\", va = \"center\", fontsize = 15, fontweight = \"bold\")\n\n# Scatterplot and regression line for borrower income by loan amount of repaid loans\nax[1] = sns.scatterplot(data = repaid, x = \"loan_amnt\", y = \"person_income\", color = \"darkorange\", edgecolor = \"darkorange\", alpha = 0.5, ax = ax[1])\nsns.regplot(data = repaid, x = \"loan_amnt\", y = \"person_income\", scatter = False, line_kws={\"color\": \"purple\"}, ax = ax[1])\nax[1].set_yscale(\"log\", base = 2)\nax[1].set_xlabel(\"\")\nax[1].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000, 35000])\nax[1].set_xticklabels([\"$0\", \"$5000\", \"$10000\", \"$15000\", \"$20000\", \"$25000\", \"$30000\", \"$35000\"], rotation=30, fontsize = 14)\nax[1].set_ylabel(\"\")\nax[1].set_yticks([2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23])\nax[1].set_yticklabels(12 * [\"\"], rotation = 15)\nax[1].set_title(\"Repaid Loans\", fontsize = 18)\nax[1].text(27000, 12288, f\"     $R^2 = {rs2:.3f}$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(24000, 12288, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight = \"bold\")\n\nfig.suptitle(\"Borrower\\'s Income by Loan Amount Displayed by Loan Repayment Status\", fontsize = 20)\nfig.text(0.47, 0.025, \"Loan Amount\", fontsize = 16)\nplt.subplots_adjust(wspace = 0.1)\n\n\n\n\n\n\n\n\n\nFigure 1\nThe plots above display the relationship between a borrower’s income and the loan amount borrowed for both defaulted and fully repaid loans. To assist the visualization of the data points, the dependent variable person_income is adjusted using a \\(\\log_2\\) scale. With this scale in place and for both defaulted and fully repaid loans, there appears to be a transformed linear relationship between a borrower’s income and the loan amount borrowed. Additionally, this transformed linear relationship appears to be positive and moderately strong (given both \\(R^2\\) values &gt; 0.6). For both borrowers who defaulted on loans and borrowers who fully repaid their loans, it appears that as the loan amount increases, a borrower’s income increases (adjusted by a \\(\\log_2\\) scale). Considering this observed relationship, it is reasonable to assert that bigger loans are borrowed by individuals with higher income levels, regardless of whether or not those individuals defaulted or fully repaid their loans.\n\n\nCode\n# Subsetting data to the three most common loan intents\ncommon_loan_intents = train_viz.copy()\ncommon_loan_intents = train_viz[train_viz[\"loan_intent\"].isin([\"Education\", \"Medical\", \"Venture\"])].copy()\n\n# Creating Boxen Plot\nfig, ax = plt.subplots(1, 1, figsize = (10, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\np = sns.boxenplot(common_loan_intents, x = \"loan_intent\", y = \"person_age\", hue = \"loan_repayment\", palette = [\"darkorange\", \"purple\"])\np.set_xlabel(\"Intention for Loan\", fontsize = 14)\np.set_ylabel(\"Borrower\\'s Age\", fontsize = 14)\np.set_xticks([\"Education\", \"Medical\", \"Venture\"])\np.legend(title = \"Loan Status\", frameon = True)\np.set_title(\"Distribution of Borrower\\'s Age for Top Three Loan Intents\\nGrouped by Loan Repayment Status\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFigure 2\nThe figure above shows an sns.boxenplot displaying the distribution of borrower’s age for the three most common loan intents across both defaulted and fully repaid loans.\nFor education loans: The median age of both borrowers who defaulted on loans and borrowers who fully repaid their loans is less than or equal to that of defaulting and repaying borrowers taking out medical and venture loans. Additionally, the median age of educational loan borrowers is lower for those who fully repaid their loans than it is for those who defaulted – Thus, a topic for another analysis could be to explore how the age of educational loan borrowers has an impact on such loans being repaid.\nFor medical loans: It appears that the median age of borrowers is greater than those taking out loans for education or venture. There is also very little visually observable difference in the median age of borrowers who defaulted and borrowers who fully repaid their medical loans.\nFor venture loans: The median age of borrowers who fully repaid their loans is greater than that of those who defaulted on their loans. Similarly to the topic posed for educational loan borrowers, it could be interesting to explore in a future study how the age of venture loan borrowers has an effect on such loans being repaid.\n\n\n\n\n\nCode\n# Summary Statistics\n\n# Helper method to calculate coefficient of variation (%)\ndef cv(col):\n    return (col.std() / col.mean()) * 100\n\n# Creating a table grouped by penguin species and sex, showing general summary stats for several quantitative variables\nsum_stats = train_viz.rename(columns = {\"loan_repayment\": \"Loan Repayment\", \"loan_amnt\": \"Loan Amount\", \"person_income\": \"Borrower\\'s Income\", \"loan_intent\": \"Loan Intent\", \"person_age\": \"Borrower\\'s Age\", \"loan_percent_income\": \"Loan Percent Income\"}).copy()\nsum_stats = sum_stats.groupby([\"Loan Repayment\", \"Loan Intent\"]).aggregate({\"Loan Amount\" : [\"mean\", \"std\", cv], \n                                                             \"Borrower\\'s Income\" : [\"mean\", \"std\", cv], \"Loan Percent Income\": [\"mean\", \"std\", cv], \n                                                             \"Borrower\\'s Age\": [\"mean\", \"std\", cv]})\nsum_stats = sum_stats.rename(columns = {\"mean\": \"Mean\", \"std\": \"STD\", \"cv\": \"CV (%)\"})\nsum_stats = sum_stats.round(2)\nsum_stats\n\n\n\n\n\n\n\n\n\n\nLoan Amount\nBorrower's Income\nLoan Percent Income\nBorrower's Age\n\n\n\n\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\n\n\nLoan Repayment\nLoan Intent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefaulted\nDebt Consolidation\n11283.45\n7357.29\n65.20\n54553.15\n37624.33\n68.97\n0.24\n0.13\n57.00\n27.76\n6.52\n23.49\n\n\nEducation\n10912.82\n6979.08\n63.95\n47283.67\n30264.34\n64.01\n0.26\n0.13\n51.90\n27.09\n6.10\n22.50\n\n\nHome Improvement\n10035.04\n7324.85\n72.99\n49794.13\n33062.93\n66.40\n0.22\n0.13\n57.24\n27.62\n6.04\n21.86\n\n\nMedical\n11438.49\n7190.60\n62.86\n52477.12\n44092.21\n84.02\n0.24\n0.13\n54.74\n27.70\n6.31\n22.78\n\n\nPersonal\n10459.89\n6884.86\n65.82\n46965.14\n39080.81\n83.21\n0.25\n0.13\n51.99\n27.24\n6.09\n22.37\n\n\nVenture\n11115.68\n6695.34\n60.23\n44439.34\n27743.83\n62.43\n0.28\n0.13\n47.39\n26.74\n5.45\n20.38\n\n\nPaid in Full\nDebt Consolidation\n9050.14\n5810.33\n64.20\n72388.92\n61644.36\n85.16\n0.14\n0.08\n58.11\n27.54\n5.66\n20.56\n\n\nEducation\n9206.44\n6010.44\n65.29\n67866.15\n41045.51\n60.48\n0.15\n0.09\n57.71\n26.43\n5.49\n20.79\n\n\nHome Improvement\n10518.51\n6398.11\n60.83\n82499.92\n50452.00\n61.15\n0.14\n0.09\n61.15\n29.47\n5.48\n18.60\n\n\nMedical\n8570.88\n5645.54\n65.87\n65116.70\n51476.94\n79.05\n0.15\n0.08\n56.46\n27.96\n6.17\n22.07\n\n\nPersonal\n9441.60\n6133.81\n64.97\n72343.94\n54121.45\n74.81\n0.15\n0.09\n57.83\n28.49\n7.42\n26.03\n\n\nVenture\n9307.47\n6064.40\n65.16\n70493.68\n58762.60\n83.36\n0.15\n0.09\n59.45\n27.74\n5.98\n21.57\n\n\n\n\n\n\n\nIn creating the table above, I did some brief research on CV to more easily interpret the STD and wrote the helper method\nTable 1\nThe table above presents some general summary statistics from the data. I was interested in examining the mean, STD, and CV for some of the primary quantitative features in the data (loan_amnt, person_income, loan_percent_income, person_age) across each loan intent for both defaulted and fully repaid loans.\nFor Defaulted Loans:\n\nMedical loans are greatest in size on average compared to the other loan intents.\n\nHome Improvement loans are the smallest in size on average.\n\nDebt Consolidation loans are taken out by borrowers with the greatest average income.\n\nVenture loans are taken out by borrowers with the smallest average income.\n\nVenture loans have the largest average loan-value-percent-income in borrowers\n\nHome improvement loans have the smallest average loan-value-percent-income among borrowers\n\nDebt Consolidation loans are taken out by borrowers with the largest age on average.\n\nVenture loans correspond to borrowers with the lowest age on average.\n\n\nFor Fully Repaid Loans:\n\nHome Improvement loans are greatest in size on average compared to the other loan intents.\n\nMedical loans are the smallest in size on average. (Interestingly, this is the complete opposite of defaulted loans)\n\nHome Improvement loans are taken out by borrowers with the greatest average income.\n\nMedical loans are taken out by borrowers with the smallest average income.\n\nMedical, venture, education, and personal loans share the largest average loan-value-percent-income in borrowers while home improvement and debt consolidation loans correspond to the smallest average loan-value-percent-income in borrowers.\n\nIn terms of the spread of data across the variables displayed in the table, all four features possess relatively high coefficient of variation (CV%) values. This suggests that there is a notable degree of variability in the distribution of each of these features. However, some features appear to vary more than others: Loan Amount, Borrower's Income, and Loan Percent Income all have average CV (%) values &gt; 50% while Borrower's Age has an average CV (%) \\(\\approx\\) 20%. Additionally, there appears to be no considerable difference in CV (%) across each of the recorded loan intents nor between defaulted and fully repaid loans in general."
  },
  {
    "objectID": "posts/post_2/index.html#exploring-the-data",
    "href": "posts/post_2/index.html#exploring-the-data",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "",
    "text": "Code\n# Data modification\ntrain_viz = train.copy()\nloan_status_recode = {0: \"Paid in Full\", 1: \"Defaulted\"}\nloan_intent_recode = {\"VENTURE\": \"Venture\", \"EDUCATION\": \"Education\", \"MEDICAL\": \"Medical\", \"HOMEIMPROVEMENT\": \"Home Improvement\", \"PERSONAL\": \"Personal\",\n \"DEBTCONSOLIDATION\" : \"Debt Consolidation\"}\ntrain_viz[\"loan_repayment\"] = train_viz[\"loan_status\"].map(loan_status_recode)\ntrain_viz[\"loan_intent\"] = train_viz[\"loan_intent\"].map(loan_intent_recode)\ntrain_viz.loc[train_viz[\"person_age\"] &gt;= 100, \"person_age\"] = None\ntrain_viz = train_viz.dropna()\n\n# Subsetting data by loan status\ndefaulted = train_viz[train_viz[\"loan_status\"] == 1].copy().dropna()\nrepaid = train_viz[train_viz[\"loan_status\"] == 0].copy().dropna()\n\n\nModifying training data for visualization\n\n\nCode\n# Creating linear regression models and calculating R^2 values\n## Defaulted\nc1 = np.polyfit(defaulted[\"loan_amnt\"], defaulted[\"person_income\"], 1)\np1 = np.polyval(c1, defaulted[\"loan_amnt\"])\nr1 = defaulted[\"person_income\"] - p1\nssr1 = np.sum(r1 ** 2)\nsst1 = np.sum((defaulted[\"person_income\"] - np.mean(defaulted[\"loan_amnt\"])) ** 2)\nrs1 = 1 - (ssr1 / sst1)\n\n## Repaid\nc2 = np.polyfit(repaid[\"loan_amnt\"], repaid[\"person_income\"], 1)\np2 = np.polyval(c2, repaid[\"loan_amnt\"])\nr2 = repaid[\"person_income\"] - p2\nssr2 = np.sum(r2**2)\nsst2 = np.sum((repaid[\"person_income\"] - np.mean(repaid[\"loan_amnt\"]))**2)\nrs2 = 1 - (ssr2 / sst2)\n\n\nCreating linear regression models and calculating R^2 values for subsets of defaulted and fully repaid loans\n\n\nCode\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize = (15, 10))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Scatterplot and regression line for borrower income by loan amount of defaulted loans\nax[0] = sns.scatterplot(data = defaulted, x = \"loan_amnt\", y = \"person_income\", color = \"purple\", edgecolor = \"purple\", alpha = 0.25, ax = ax[0])\nsns.regplot(data = defaulted, x = \"loan_amnt\", y = \"person_income\", scatter = False, line_kws={\"color\": \"darkorange\"}, ax = ax[0])\nax[0].set_yscale(\"log\", base = 2)\nax[0].set_xlabel(\"\")\nax[0].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000, 35000])\nax[0].set_xticklabels([\"$0\", \"$5000\", \"$10000\", \"$15000\", \"$20000\", \"$25000\", \"$30000\", \"$35000\"], rotation = 30, fontsize = 14)\nax[0].set_ylabel(f\"Borrower\\'s Income ($\\log_2$ scale)\", fontsize = 16, labelpad = 15)\nax[0].set_yticks([2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23])\nax[0].set_yticklabels([\"&gt;$4000\", \"&gt;$8000\", \"&gt;$16000\", \"&gt;$32000\", \"&gt;$64000\", \"&gt;$128000\", \"&gt;$256000\", \"&gt;$512000\", \"&gt;$1024000\", \"&gt;$2048000\", \"&gt;$4096000\", \"&lt;$8192000\"], rotation = 15, fontsize = 14)\nax[0].set_title(\"Defaulted Loans\", fontsize = 18)\nax[0].text(27000, 12288, f\"     $R^2 = {rs1:.3f}$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(24000, 12288, \"\\u2013\", color = \"darkorange\", ha = \"left\", va = \"center\", fontsize = 15, fontweight = \"bold\")\n\n# Scatterplot and regression line for borrower income by loan amount of repaid loans\nax[1] = sns.scatterplot(data = repaid, x = \"loan_amnt\", y = \"person_income\", color = \"darkorange\", edgecolor = \"darkorange\", alpha = 0.5, ax = ax[1])\nsns.regplot(data = repaid, x = \"loan_amnt\", y = \"person_income\", scatter = False, line_kws={\"color\": \"purple\"}, ax = ax[1])\nax[1].set_yscale(\"log\", base = 2)\nax[1].set_xlabel(\"\")\nax[1].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000, 35000])\nax[1].set_xticklabels([\"$0\", \"$5000\", \"$10000\", \"$15000\", \"$20000\", \"$25000\", \"$30000\", \"$35000\"], rotation=30, fontsize = 14)\nax[1].set_ylabel(\"\")\nax[1].set_yticks([2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23])\nax[1].set_yticklabels(12 * [\"\"], rotation = 15)\nax[1].set_title(\"Repaid Loans\", fontsize = 18)\nax[1].text(27000, 12288, f\"     $R^2 = {rs2:.3f}$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(24000, 12288, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight = \"bold\")\n\nfig.suptitle(\"Borrower\\'s Income by Loan Amount Displayed by Loan Repayment Status\", fontsize = 20)\nfig.text(0.47, 0.025, \"Loan Amount\", fontsize = 16)\nplt.subplots_adjust(wspace = 0.1)\n\n\n\n\n\n\n\n\n\nFigure 1\nThe plots above display the relationship between a borrower’s income and the loan amount borrowed for both defaulted and fully repaid loans. To assist the visualization of the data points, the dependent variable person_income is adjusted using a \\(\\log_2\\) scale. With this scale in place and for both defaulted and fully repaid loans, there appears to be a transformed linear relationship between a borrower’s income and the loan amount borrowed. Additionally, this transformed linear relationship appears to be positive and moderately strong (given both \\(R^2\\) values &gt; 0.6). For both borrowers who defaulted on loans and borrowers who fully repaid their loans, it appears that as the loan amount increases, a borrower’s income increases (adjusted by a \\(\\log_2\\) scale). Considering this observed relationship, it is reasonable to assert that bigger loans are borrowed by individuals with higher income levels, regardless of whether or not those individuals defaulted or fully repaid their loans.\n\n\nCode\n# Subsetting data to the three most common loan intents\ncommon_loan_intents = train_viz.copy()\ncommon_loan_intents = train_viz[train_viz[\"loan_intent\"].isin([\"Education\", \"Medical\", \"Venture\"])].copy()\n\n# Creating Boxen Plot\nfig, ax = plt.subplots(1, 1, figsize = (10, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\np = sns.boxenplot(common_loan_intents, x = \"loan_intent\", y = \"person_age\", hue = \"loan_repayment\", palette = [\"darkorange\", \"purple\"])\np.set_xlabel(\"Intention for Loan\", fontsize = 14)\np.set_ylabel(\"Borrower\\'s Age\", fontsize = 14)\np.set_xticks([\"Education\", \"Medical\", \"Venture\"])\np.legend(title = \"Loan Status\", frameon = True)\np.set_title(\"Distribution of Borrower\\'s Age for Top Three Loan Intents\\nGrouped by Loan Repayment Status\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFigure 2\nThe figure above shows an sns.boxenplot displaying the distribution of borrower’s age for the three most common loan intents across both defaulted and fully repaid loans.\nFor education loans: The median age of both borrowers who defaulted on loans and borrowers who fully repaid their loans is less than or equal to that of defaulting and repaying borrowers taking out medical and venture loans. Additionally, the median age of educational loan borrowers is lower for those who fully repaid their loans than it is for those who defaulted – Thus, a topic for another analysis could be to explore how the age of educational loan borrowers has an impact on such loans being repaid.\nFor medical loans: It appears that the median age of borrowers is greater than those taking out loans for education or venture. There is also very little visually observable difference in the median age of borrowers who defaulted and borrowers who fully repaid their medical loans.\nFor venture loans: The median age of borrowers who fully repaid their loans is greater than that of those who defaulted on their loans. Similarly to the topic posed for educational loan borrowers, it could be interesting to explore in a future study how the age of venture loan borrowers has an effect on such loans being repaid.\n\n\n\n\n\nCode\n# Summary Statistics\n\n# Helper method to calculate coefficient of variation (%)\ndef cv(col):\n    return (col.std() / col.mean()) * 100\n\n# Creating a table grouped by penguin species and sex, showing general summary stats for several quantitative variables\nsum_stats = train_viz.rename(columns = {\"loan_repayment\": \"Loan Repayment\", \"loan_amnt\": \"Loan Amount\", \"person_income\": \"Borrower\\'s Income\", \"loan_intent\": \"Loan Intent\", \"person_age\": \"Borrower\\'s Age\", \"loan_percent_income\": \"Loan Percent Income\"}).copy()\nsum_stats = sum_stats.groupby([\"Loan Repayment\", \"Loan Intent\"]).aggregate({\"Loan Amount\" : [\"mean\", \"std\", cv], \n                                                             \"Borrower\\'s Income\" : [\"mean\", \"std\", cv], \"Loan Percent Income\": [\"mean\", \"std\", cv], \n                                                             \"Borrower\\'s Age\": [\"mean\", \"std\", cv]})\nsum_stats = sum_stats.rename(columns = {\"mean\": \"Mean\", \"std\": \"STD\", \"cv\": \"CV (%)\"})\nsum_stats = sum_stats.round(2)\nsum_stats\n\n\n\n\n\n\n\n\n\n\nLoan Amount\nBorrower's Income\nLoan Percent Income\nBorrower's Age\n\n\n\n\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\n\n\nLoan Repayment\nLoan Intent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefaulted\nDebt Consolidation\n11283.45\n7357.29\n65.20\n54553.15\n37624.33\n68.97\n0.24\n0.13\n57.00\n27.76\n6.52\n23.49\n\n\nEducation\n10912.82\n6979.08\n63.95\n47283.67\n30264.34\n64.01\n0.26\n0.13\n51.90\n27.09\n6.10\n22.50\n\n\nHome Improvement\n10035.04\n7324.85\n72.99\n49794.13\n33062.93\n66.40\n0.22\n0.13\n57.24\n27.62\n6.04\n21.86\n\n\nMedical\n11438.49\n7190.60\n62.86\n52477.12\n44092.21\n84.02\n0.24\n0.13\n54.74\n27.70\n6.31\n22.78\n\n\nPersonal\n10459.89\n6884.86\n65.82\n46965.14\n39080.81\n83.21\n0.25\n0.13\n51.99\n27.24\n6.09\n22.37\n\n\nVenture\n11115.68\n6695.34\n60.23\n44439.34\n27743.83\n62.43\n0.28\n0.13\n47.39\n26.74\n5.45\n20.38\n\n\nPaid in Full\nDebt Consolidation\n9050.14\n5810.33\n64.20\n72388.92\n61644.36\n85.16\n0.14\n0.08\n58.11\n27.54\n5.66\n20.56\n\n\nEducation\n9206.44\n6010.44\n65.29\n67866.15\n41045.51\n60.48\n0.15\n0.09\n57.71\n26.43\n5.49\n20.79\n\n\nHome Improvement\n10518.51\n6398.11\n60.83\n82499.92\n50452.00\n61.15\n0.14\n0.09\n61.15\n29.47\n5.48\n18.60\n\n\nMedical\n8570.88\n5645.54\n65.87\n65116.70\n51476.94\n79.05\n0.15\n0.08\n56.46\n27.96\n6.17\n22.07\n\n\nPersonal\n9441.60\n6133.81\n64.97\n72343.94\n54121.45\n74.81\n0.15\n0.09\n57.83\n28.49\n7.42\n26.03\n\n\nVenture\n9307.47\n6064.40\n65.16\n70493.68\n58762.60\n83.36\n0.15\n0.09\n59.45\n27.74\n5.98\n21.57\n\n\n\n\n\n\n\nIn creating the table above, I did some brief research on CV to more easily interpret the STD and wrote the helper method\nTable 1\nThe table above presents some general summary statistics from the data. I was interested in examining the mean, STD, and CV for some of the primary quantitative features in the data (loan_amnt, person_income, loan_percent_income, person_age) across each loan intent for both defaulted and fully repaid loans.\nFor Defaulted Loans:\n\nMedical loans are greatest in size on average compared to the other loan intents.\n\nHome Improvement loans are the smallest in size on average.\n\nDebt Consolidation loans are taken out by borrowers with the greatest average income.\n\nVenture loans are taken out by borrowers with the smallest average income.\n\nVenture loans have the largest average loan-value-percent-income in borrowers\n\nHome improvement loans have the smallest average loan-value-percent-income among borrowers\n\nDebt Consolidation loans are taken out by borrowers with the largest age on average.\n\nVenture loans correspond to borrowers with the lowest age on average.\n\n\nFor Fully Repaid Loans:\n\nHome Improvement loans are greatest in size on average compared to the other loan intents.\n\nMedical loans are the smallest in size on average. (Interestingly, this is the complete opposite of defaulted loans)\n\nHome Improvement loans are taken out by borrowers with the greatest average income.\n\nMedical loans are taken out by borrowers with the smallest average income.\n\nMedical, venture, education, and personal loans share the largest average loan-value-percent-income in borrowers while home improvement and debt consolidation loans correspond to the smallest average loan-value-percent-income in borrowers.\n\nIn terms of the spread of data across the variables displayed in the table, all four features possess relatively high coefficient of variation (CV%) values. This suggests that there is a notable degree of variability in the distribution of each of these features. However, some features appear to vary more than others: Loan Amount, Borrower's Income, and Loan Percent Income all have average CV (%) values &gt; 50% while Borrower's Age has an average CV (%) \\(\\approx\\) 20%. Additionally, there appears to be no considerable difference in CV (%) across each of the recorded loan intents nor between defaulted and fully repaid loans in general."
  },
  {
    "objectID": "posts/post_2/index.html#feature-selection",
    "href": "posts/post_2/index.html#feature-selection",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n\nCode\n# Modifying the training data\nX = train.drop([\"loan_status\", \"loan_grade\"], axis = 1, errors = \"ignore\")\ny = train[\"loan_status\"]\n\n# Creating one-hot encodings for qualitative variables\nX = pd.get_dummies(X, drop_first = True)\n\n# Removing NAs from data while maintaining alignment\nclean = pd.concat([X, y], axis = 1)\nclean = clean.dropna()\nX_train = clean.drop(\"loan_status\", axis = 1, errors = \"ignore\").copy()\ny_train = clean[\"loan_status\"].copy()\n\n# Adding new columns to X_train\n## Ratio of employment length to credit history\nX_train[\"credit_hist_emp_len\"] =  X_train[\"person_emp_length\"] / X_train[\"cb_person_cred_hist_length\"]\n\n# Identifying quantitative and qualitative variables in the data\nquant_vars = [\n              \"person_age\", \n              \"person_income\", \n              \"person_emp_length\", \n              \"loan_amnt\", \n              \"loan_int_rate\", \n              \"loan_percent_income\", \n              \"cb_person_cred_hist_length\",\n              \"credit_hist_emp_len\"\n            ]\nqual_vars = [\n              \"person_home_ownership_OTHER\",\n              \"person_home_ownership_OWN\",\n              \"person_home_ownership_RENT\",\n              \"loan_intent_EDUCATION\",\n              \"loan_intent_HOMEIMPROVEMENT\",\n              \"loan_intent_MEDICAL\",\n              \"loan_intent_PERSONAL\",\n              \"loan_intent_VENTURE\",\n              \"cb_person_default_on_file_Y\"\n            ]\n\n# Combination of selected quant and qual variables\ncols = quant_vars + qual_vars\n\n# Toggle to run feature selection process\nif (False):\n\n  # Iterating through all possible subsets of cols, evaluating a model with each subset, to determine the optimal set of variables to use\n  best_vars = []\n  best_avg_score = 0\n\n  # Paramters to determine the size of tested subsets\n  lower_bound = 4\n  upper_bound = 5\n  for i in range(lower_bound, upper_bound):\n      \n      # Iterating through all subsets\n      for subset in itertools.combinations(cols, i):\n        curr_vars = list(subset)\n        # Fitting a model to each subset\n        LR = LogisticRegression(random_state = 69)\n        LR.fit(X_train[curr_vars], y_train)\n        curr_avg_score = cross_val_score(LR, X_train[curr_vars], y_train, cv = 5).mean()\n        \n        # Update the best average score and the best variables to use for the model\n        if (curr_avg_score &gt; best_avg_score):\n          best_avg_score = curr_avg_score\n          best_vars = curr_vars\n\n# These are the best 5 variables selected after the iterative feature selection process above\nbest_vars = [\"loan_percent_income\", \"cb_person_cred_hist_length\", \"person_home_ownership_OTHER\", \"person_home_ownership_RENT\", \"loan_intent_HOMEIMPROVEMENT\"]\n\n# Refined model\nLR = LogisticRegression(random_state = 69)\nLR.fit(X_train[best_vars], y_train)\nscore = LR.score(X_train[best_vars], y_train)\nprint(f\"Refined Model Accuracy: {score * 100: .3f}%\")\n\n# Setting the weights vector\nw = np.array(LR.coef_[0])\n\n\nRefined Model Accuracy:  85.096%\n\n\nCode above shows the iterative process of trying all combinations of certain sizes for all features and taking those that lead to the best cross-validated model performance\nTo begin the feature selection process, the training data was informally divided into its quantitative and qualitative variables. The feature selection process involved iterating through a subset of the power set (all possible subsets) of the training data. All combinations of 1 up to 5 (out of 17) features were evaluated (beyond 5 features was too computationally expensive). According to the feature selection above, the 5 best variables to train the model with are loan_percent_income, cb_person_cred_hist_length (length of borrower’s credit history), person_home_ownership_OTHER (borrowers with home ownership status as “OTHER”), person_home_ownership_RENT (borrowers who are renting a home), and loan_intent_HOMEIMPROVEMENT (borrowers who intend to use a loan for home improvement). With these features used, the LogisticRegression model achieved just over \\(85\\%\\) training accuracy. While there is certainly room to improve the model’s training accuracy, this is the highest score achieved using the variables provided by the data and without undertaking a massive computational load (i.e. super long run time to iterate through all elements of the variable power set). The weights used by the model were extracted and stored in a weights vector w to be used in the scoring function described below."
  },
  {
    "objectID": "posts/post_2/index.html#creating-a-scoring-function",
    "href": "posts/post_2/index.html#creating-a-scoring-function",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Creating a Scoring Function",
    "text": "Creating a Scoring Function\n\n\nCode\n# Function to compute the score for each observation\ndef lin_score(X, w):\n    return X@w\n\n# Computing scores of each training observation and observing confusion matrix for the training data\nscores = lin_score(X_train[best_vars], w)\n\n# Normalizing the scores\nscores = (scores - scores.min()) / (scores.max() - scores.min())\n\n# Establishing model predictions for the train data\ny_train_pred = LR.predict(X_train[best_vars])\nC = confusion_matrix(y_train, y_train_pred)\nloan_status = [\"Fully Repaid\", \"Defaulted\"]\n\n# Creating a heatmap for better confusion matrix visualization\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 7.5))\nmask1 = np.zeros_like(C, dtype=bool)\nmask1[:, 1] = True\nmask2 = np.zeros_like(C, dtype=bool)\nmask2[:, 0] = True\npurples = LinearSegmentedColormap.from_list(\"purple_gradient\", [\"#ce93d8\", \"#4a148c\"])\ndarkoranges = LinearSegmentedColormap.from_list(\"darkorange_gradient\", [\"#FFD580\", \"darkorange\"])\n# Plot the first column with purple gradient\nax1 = sns.heatmap(C, annot=True, fmt=\"d\", cmap=purples, \n                mask=mask1, cbar=False, xticklabels=[\"Fully Repaid\", \"Defaulted\"],\n                yticklabels=loan_status)\n\n# Plot the second column with orange gradient\nax2 = sns.heatmap(C, annot=True, fmt=\"d\", cmap=darkoranges, \n                mask=mask2, cbar=False, xticklabels=[\"Fully Repaid\", \"Defaulted\"],\n                yticklabels=loan_status)\n\n# Setting labels and title\nplt.xlabel(\"Predicted Loan Status\")\nplt.ylabel(\"True Loan Status\")\nplt.title(\"Lending Data Classification Confusion Matrix\")\n\nplt.show()\n\n# Printing confusion matrix results\nprint(f\"There were {C[0, 0]} fully repaid loan(s) that were predicted to be fully repaid.\")\nprint(f\"There were {C[0, 1]} fully repaid loan(s) that were predicted to be defaulted.\")\nprint(f\"There were {C[1, 0]} defaulted loan(s) that were predicted to be fully repaid.\")\nprint(f\"There were {C[1, 1]} defaulted loan(s) that were predicted to be defaulted.\")\n\n\n\n\n\n\n\n\n\nThere were 17702 fully repaid loan(s) that were predicted to be fully repaid.\nThere were 279 fully repaid loan(s) that were predicted to be defaulted.\nThere were 3135 defaulted loan(s) that were predicted to be fully repaid.\nThere were 1791 defaulted loan(s) that were predicted to be defaulted.\n\n\nCode above shows the linear scoring function used to determine model predictions and evaluate model accuracy. The scores were normalized to ensure that all scores are within the interval [0, 1]\nFigure 3\nFor this analysis, a linear scoring function is used. Specifically, the linear scoring function computes the score of each data observation by taking the dot product of each observation with the weights vector w. To ensure that all scores are values on the interval [0, 1], the scores are normalized after they are calculated (i.e. the scores are updated to be the difference between each raw score and the minimum raw score divided by the range of raw scores). Additionally, the confusion matrix for the model’s performance is displayed above. The model’s false positive rate is approximately \\(1.55\\%\\) while the true positive rate is approximately \\(36.34\\%\\). Conversely, the model’s false negative rate is approximately \\(63.66\\%\\) while the true negative rate is approximately \\(98.45\\%\\). With this information, it appears that the model rarely misclassifies fully repaid loans as defaulted (low FPR). However, the model frequently (moderately high FNR) misclassifies defaulted loans as being fully repaid. In general, with a very high TNR, the model is very accurate at detecting truly fully repaid loans but much less accurate (low TPR) at recognizing truly defaulted loans. It is important to note here that the initial objective for this model was to accurately predict if a prospective borrower will default on a loan, thus suggesting that perhaps the model and or the scoring function should be slightly redesigned to increase the TPR. However, given the introductory nature of this brief study, the analysis will continue with the current model as is.\n\nPlotting Distribution of Scores\n\n\nCode\n# Distribution of Scores - Code Provided by Prof. Chodrow\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nhist = ax.hist(scores, bins = 50, color = \"darkorange\", alpha = 0.75, linewidth = 1, edgecolor = \"purple\")\nlabs = ax.set(xlabel = r\"Scores (Normalized)\", ylabel = \"Frequency\") \n\n# Percentage (the majority) of scores below 0.5\nmaj_scores = (scores &lt;= 0.5).mean()\n\n\n\n\n\n\n\n\n\nFigure 4\nAs observed in the distribution of normalized scores above, it appears that the strong majority (\\(\\approx 91.81\\%\\))of normalized scores are less than or equal to 0.5. This may indicate that a useful threshold value for maximizing model accuracy and or bank profit per borrower (as informed by the model) might be close to \\(t = 0.5\\)."
  },
  {
    "objectID": "posts/post_2/index.html#choosing-a-threshold",
    "href": "posts/post_2/index.html#choosing-a-threshold",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Choosing a Threshold",
    "text": "Choosing a Threshold\n\n\nCode\n# Threshold selection based on model accuracy\nbest_accuracy = 0\nbest_threshold = 0\n\n# Following code is adapted from Prof. Chodrow\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\nfor t in np.linspace(0, 1, 100): \n    y_pred = scores &gt;= t\n    acc = (y_pred == y_train).mean()\n    ax.scatter(t, acc, color = \"purple\", s = 12.5)\n    if acc &gt; best_accuracy: \n        best_accuracy = acc\n        best_threshold = t\nax.axvline(best_threshold, linestyle = \"--\", color = \"darkorange\", linewidth = 2.5, zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Model Accuracy\", title = \"Accuracy by Threshold\")\nax.text(0.75, 0.5, f\"Best Accuracy: {best_accuracy: .3f}\\n  At Threshold $t = ${best_threshold: .3f}\", ha = \"center\",\n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"grey\", boxstyle = \"round,pad=0.3\"))\nax.grid(False)\n\n\n\n\n\n\n\n\n\nFigure 5\nShown above is the trend in overall model accuracy as the threshold \\(t\\) increases from \\(0: 1\\). As outlined in the plot, the peak model accuracy (\\(\\approx 85\\%\\)) is achieved using a threshold of \\(t = 0.505\\). However, what is likely more interesting (especially for the bank) is how the profit-per-borrower can be maximized through selecting a value for \\(t\\). Below is an examination of thresholding for profit maximization.\n\n\nCode\n# Threshold selection based on profit for the bank created by the model\n\n# Updated variables\nbest_ppb = 0\nbest_threshold = 0\n\n# Following code is adapted from Prof. Chodrow\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\n\nfor t in np.linspace(0, 1, 100): \n    y_pred = scores &gt;= t\n    \n    # Determining whether each prediction is a a true or false negative to inform the amount of profit loss or gain for a given loan\n    X_train[\"TN\"] = ((y_pred == 0) & (y_train == 0)).astype(int)\n    X_train[\"FN\"] = ((y_pred == 0) & (y_train == 1)).astype(int)\n    \n    # Calculating the profit gain and loss for each borrower using the formulas provided by Prof. Chodrow\n    profit_gain = X_train[\"TN\"] * (X_train[\"loan_amnt\"] * ((1 + (0.25 * (X_train[\"loan_int_rate\"] / 100))) ** 10) - X_train[\"loan_amnt\"])\n    profit_loss = X_train[\"FN\"] * (X_train[\"loan_amnt\"] * ((1 + (0.25 * (X_train[\"loan_int_rate\"] / 100))) ** 3) - (1.7 * X_train[\"loan_amnt\"]))\n    \n    # Profit-per-borrower with the current threshold\n    ppb = (profit_gain + profit_loss).sum() / X_train.shape[0]\n    \n    # Ploting PPB point and updating variables\n    ax.scatter(t, ppb, color = \"purple\", s = 12.5)\n    if ppb &gt; best_ppb: \n        best_ppb = ppb\n        best_threshold = t\n\n# Plot styling\nax.axvline(best_threshold, linestyle = \"--\", color = \"darkorange\", linewidth = 2.5, zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Bank Profit (Per Borrower)\", title = \"Profit Per Borrower by Threshold\")\nax.text(0.75, 500, f\"Best Profit: ${best_ppb: .2f}\\n  At Threshold $t = ${best_threshold: .3f}\", ha = \"center\",\n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"grey\", boxstyle = \"round,pad=0.3\"))\nax.grid(False)\n\n\n\n\n\n\n\n\n\nIn the code above, the bank’s profit-per-borrower is plotted against the threshold \\(t\\). The profit per borrower is determined by calculating the profit gain and profit loss (using the formulas provided by Prof. Chordow) for each individual loan. The correct profit value (either gain or loss) is then added to the overall profit sum depending on whether or not the model correctly classified a given loan as a true negative or incorrectly classified a given loan as a false negative. The overall profit is then divided by the number of observations (borrowers) in the training data to yield the profit value per borrower.\nFigure 6\nAs depicted above, the bank’s profit-per-borrower is shown to increase as the threshold \\(t\\) increases before notably dropping and ultimately plateauing. The bank’s peak profit-per-borrower is approximately \\(\\$1450.00\\), corresponding to using a threshold value of \\(t = 0.495\\). Not surprisingly, both the model’s overall accuracy as well as the bank’s profit-per-borrower are maximized at very similar thresholds (about 0.5)."
  },
  {
    "objectID": "posts/post_2/index.html#perspective-of-the-bank",
    "href": "posts/post_2/index.html#perspective-of-the-bank",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Perspective of the Bank",
    "text": "Perspective of the Bank\n\n\nCode\n# Reading in and modifying the test data\ntest = pd.read_csv(\"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\")\ntest = test.dropna()\nXt = test.drop([\"loan_status\", \"loan_grade\"], axis = 1, errors = \"ignore\")\nyt = test[\"loan_status\"]\n\n# Creating one-hot encodings for qualitative variables\nXt = pd.get_dummies(Xt, drop_first = True)\n\n# Removing NAs from data while maintaining alignment\nclean = pd.concat([Xt, yt], axis = 1)\nclean = clean.dropna()\nX_test = clean.drop(\"loan_status\", axis = 1, errors = \"ignore\").copy()\ny_test = clean[\"loan_status\"].copy()\n\n# Computing and normalizing the test scores\ntest_scores = lin_score(X_test[best_vars], w)\n# Normalizing the scores\ntest_scores = (test_scores - scores.min()) / (test_scores.max() - test_scores.min())\n\n# Adding the model's calculated score for each observation in the testing data\ntest[\"score\"] = test_scores\ntest[\"score\"] = pd.to_numeric(test[\"score\"], errors = \"coerce\")\n\n# Determining predictions\ny_test_pred = test_scores &gt;= best_threshold\n\n# Calculating the profit-per-borrower for test data (copied from code chunk above)\n# Determining whether each prediction is a a true or false negative to inform the amount of profit loss or gain for a given loan\nX_test[\"TN\"] = ((y_test_pred == 0) & (y_test == 0)).astype(int)\nX_test[\"FN\"] = ((y_test_pred == 0) & (y_test == 1)).astype(int)\n\n# Calculating the profit gain and loss for each borrower using the foromulas provided by Prof. Chodrow\nprofit_gain_t = X_test[\"TN\"] * (X_test[\"loan_amnt\"] * ((1 + (0.25 * (X_test[\"loan_int_rate\"] / 100))) ** 10) - X_test[\"loan_amnt\"])\nprofit_loss_t = X_test[\"FN\"] * (X_test[\"loan_amnt\"] * ((1 + (0.25 * (X_test[\"loan_int_rate\"] / 100))) ** 3) - (1.7 * X_test[\"loan_amnt\"]))\n\n# Profit-per-borrower for test data using the best threshold\nppbt = (profit_gain_t + profit_loss_t).sum() / X_test.shape[0]\n\nprint(f\"The Bank's Profit-Per-Borrower from the test data: ${ppbt :.2f}\")\nprint(f\"(Test Data PPB to Training Data PPB Ratio: {((ppbt / best_ppb) * 100): .2f}%)\")\n\n\nThe Bank's Profit-Per-Borrower from the test data: $1326.67\n(Test Data PPB to Training Data PPB Ratio:  91.64%)\n\n\nIn the code above, the test data is processed the exact same way as the training data to ensure compatibility with the scoring function and profit-per-borrower calculations as previously done.\nAccording to the computation above, the bank’s expected profit-per-borrower on the test data is approximately \\(\\$1330.00\\). This value is roughly \\(92\\%\\) of (i.e. considerably similar to) the profit-per-borrower produced by the training data. Consider the most popular loan intent from the test data: Educational loans. According to the statistics provided by the Education Data Initiative, there were nearly 43 million student loan borrowers in the US in 2024. Additionally, MX Technologies reports there being about 4550 banks in the US. If the number of US educational loan borrowers is divided amongst the number of US banks, each US bank could have approximately 9450 educational loan borrowers in 2024. So suppose the hypothetical bank in this study has about 9450 educational loan borrowers. Thus, the bank’s profit (theoretically in 2024) from educational loans alone would be about $12.57 million. This is of course a wildly crude calculation that is ultimately not based on any reviewed methodology. But, it at least goes to show how large the hypothetical bank’s annual profit theoretically could be, even for only one type of loan."
  },
  {
    "objectID": "posts/post_2/index.html#perspective-of-the-borrower",
    "href": "posts/post_2/index.html#perspective-of-the-borrower",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Perspective of the Borrower",
    "text": "Perspective of the Borrower\n\nLoan Approval and Borrower Age\n\n\nCode\n# Function to convert numerical ages to age categories\ndef age_cat(age):\n    if (age &lt; 30):\n        return \"Young Adult\"\n    elif (age &lt; 60):\n        return \"Adult\"\n    else:\n        return \"Senior\"\n\ntest[\"age_cat\"] = test[\"person_age\"].apply(age_cat)\n\n# Independent and dependent variables for bar chart\nage_cats = np.array(test[\"age_cat\"].unique())\n\n# Avg. scores calculated using .groupby(...).aggregate(...)\nac_avg_scores = np.array([\n    test[test[\"age_cat\"] == \"Young Adult\"][\"score\"].mean(),\n    test[test[\"age_cat\"] == \"Adult\"][\"score\"].mean(),\n    test[test[\"age_cat\"] == \"Senior\"][\"score\"].mean(),\n    ])\nage_scores_df = pd.DataFrame({\"Age Cat\": age_cats, \"Avg Score\": ac_avg_scores})\n\n# Arrays of percentage of high scores and percentage of all scores above the threshold from each age cat\nnum_high_scores = (test[\"score\"] &gt;= best_threshold).sum()\n\n# Perc. of high scores among each age cat\nac_perc_high_scores_1 = np.array([(test[test[\"age_cat\"] == \"Young Adult\"][\"score\"] &gt;= best_threshold).mean() * 100, \n                           (test[test[\"age_cat\"] == \"Adult\"][\"score\"] &gt;= best_threshold).mean() * 100, \n                           (test[test[\"age_cat\"] == \"Senior\"][\"score\"] &gt;= best_threshold).mean() * 100])\n\n# Perc. of all high scores contributed by each age cat\nac_perc_high_scores_2 = np.array([(test[test[\"age_cat\"] == \"Young Adult\"][\"score\"] &gt;= best_threshold).sum(), \n                           (test[test[\"age_cat\"] == \"Adult\"][\"score\"] &gt;= best_threshold).sum(), \n                           (test[test[\"age_cat\"] == \"Senior\"][\"score\"] &gt;= best_threshold).sum()])\n\n# Displaying average model score for each age cat\nfig, ax = plt.subplots(2, 2, figsize = (15, 12.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Distribution of scores by age cat\nsns.boxenplot(test, x = \"age_cat\", y = \"score\", hue = \"age_cat\", legend = False, ax = ax[0, 0])\nax[0, 0].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0, 0].set_title(\"Distribution of Model Scores\", fontsize = 18)\nax[0, 0].set_ylabel(\"Score (Normalized)\", fontsize = 16)\nax[0, 0].set_xticks([0, 1, 2])\nax[0, 0].set_xticklabels([\"Young Adult\\n(Early-Career)\", \"Adult\\n(Mid-Career)\", \"Senior\\n(Late-Career/Retired)\"])\nax[0, 0].set_xlabel(\"\")\nax[0, 0].text(1.75, 0.85, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0, 0].text(1.5, 0.85, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Average score by age cat\nsns.barplot(x = age_cats, y = ac_avg_scores, data = age_scores_df, hue = age_cats, legend = False, ax = ax[0, 1])\nax[0, 1].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0, 1].set_title(\"Average Model Score\", fontsize = 18)\nax[0, 1].set_ylabel(\"Average Score (Normalized)\", fontsize = 16)\nax[0, 1].set_xticks([0, 1, 2])\nax[0, 1].set_xticklabels([\"Young Adult\\n(Early-Career)\", \"Adult\\n(Mid-Career)\", \"Senior\\n(Late-Career/Retired)\"])\nax[0, 1].text(1.75, 0.425, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0, 1].text(1.5, 0.425, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Percentage of high scores (above threshold) for each age cat\nsns.barplot(x = age_cats, y = ac_perc_high_scores_1, hue = age_cats, legend = False, ax = ax[1, 0])\nax[1, 0].set_title(\"Percentage of Model Scores\\nAbove Threshold \", fontsize = 18)\nax[1, 0].set_ylabel(\"% of Scores Above Threshold\", fontsize = 16)\nax[1, 0].set_xticks([0, 1, 2])\nax[1, 0].set_xticklabels([\"Young Adult\\n(Early-Career)\", \"Adult\\n(Mid-Career)\", \"Senior\\n(Late-Career/Retired)\"])\nax[1, 0].set_yticks([0, 2, 4, 6, 8, 10, 12])\nax[1, 0].set_yticklabels([\"0%\", \"2%\", \"4%\", \"6%\", \"8%\", \"10%\", \"12%\"])\n\n# Percentage of all high scores (above threshold) from each age cat\nsns.barplot(x = age_cats, y = ((ac_perc_high_scores_2 / num_high_scores) * 100), hue = age_cats, legend = False, ax = ax[1, 1])\nax[1, 1].set_title(\"Percentage of all Model Scores\\nAbove Threshold From each Age Category\", fontsize = 18)\nax[1, 1].set_ylabel(\"% of all Scores Above Threshold\", fontsize = 16)\nax[1, 1].set_xticks([0, 1, 2])\nax[1, 1].set_xticklabels([\"Young Adult\\n(Early-Career)\", \"Adult\\n(Mid-Career)\", \"Senior\\n(Late-Career/Retired)\"])\nax[1, 1].set_yticks([0, 10, 20, 30, 40, 50, 60, 70, 80])\nax[1, 1].set_yticklabels([\"0%\", \"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\", \"80%\"])\n\nfig.text(0.5, 0.95, \"Model Score Metrics by Borrower Age category\", ha = \"center\", fontsize = 20)\nfig.text(0.5, 0.05, \"Borrower Age Category\", ha = \"center\", fontsize = 18)\nplt.subplots_adjust(wspace = 0.25, hspace = 0.3)\n\n\n\n\n\n\n\n\n\nCode above constructs 4 plots to observe how the model score varies across age category among borrowers. The top two plots simply display the distribution of model scores and the average score for each age category. The bottom two plots show the percentage of high model scores within each age category and the percentage of overall high model scores attributed to each age category (computed by taking the number of high model scores for each age category over the total number of high model scores).\nFigure 7\nAs displayed in the plots above, it appears that young adults and middle-aged borrowers receive lower scores than senior borrowers. Additionally, a smaller percentage of scores for both young adult and middle-aged borrowers are above the threshold than that of senior borrowers. Considering this, it is reasonable to hypothesize that the model makes it more difficult for older borrowers to be approved for loans than it is for younger borrowers. However, this is possibly a result of inconsistent age category sizes. That is, the percentage of all high scores (scores that advise the bank to deny a borrower a loan) attributed to young adults is much greater than that of middle-aged and senior borrowers. It is possible that the metrics found in this data are not truly representative of the population of senior borrowers.\n\n\nLoan Approval and Loan Intent\n\n\nCode\n# Independent and dependent variables for bar chart\ntest[\"loan_intent\"] = test[\"loan_intent\"].map(loan_intent_recode)\nloan_intents = np.array(test[\"loan_intent\"].unique())\nprint(loan_intents)\n\n# Avg. scores calculated using .groupby(...).aggregate(...)\nli_avg_scores = np.array([\n    test[test[\"loan_intent\"] == \"Venture\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"Debt Consolidation\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"Medical\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"Home Improvement\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"Education\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"Personal\"][\"score\"].mean()\n])\nli_scores_df = pd.DataFrame({\"Loan Intent\": loan_intents, \"Avg Score\": li_avg_scores})\n\n# Arrays of percentage of high scores and percentage of all scores above the threshold from each loan intent\nli_perc_high_scores_1 = np.zeros(0)\nli_perc_high_scores_2 = np.zeros(0)\n\nfor loan in loan_intents:\n    \n    # Calculating each metric\n    li_perc_high_score_1 = (test[test[\"loan_intent\"] == loan][\"score\"] &gt;= best_threshold).mean() * 100\n    li_perc_high_score_2 = ((test[test[\"loan_intent\"] == loan][\"score\"] &gt;= best_threshold).sum() / num_high_scores) * 100\n\n    # Adding each metric to corresponding array\n    li_perc_high_scores_1 = np.append(li_perc_high_scores_1, li_perc_high_score_1)\n    li_perc_high_scores_2 = np.append(li_perc_high_scores_2, li_perc_high_score_2)\n\n# Displaying average model score for each age cat\nfig, ax = plt.subplots(2, 2, figsize = (15, 12.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Distribution of scores by age cat\nsns.boxenplot(test, x = \"loan_intent\", y = \"score\", hue = \"loan_intent\", legend = False, ax = ax[0, 0])\nax[0, 0].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0, 0].set_title(\"Distribution of Model Scores\", fontsize = 18)\nax[0, 0].set_ylabel(\"Score (Normalized)\", fontsize = 16)\nax[0, 0].set_xticks([0, 1, 2, 3, 4 ,5])\nax[0, 0].set_xticklabels(loan_intents, rotation = 20)\nax[0, 0].set_xlabel(\"\")\nax[0, 0].text(4, 0.9, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0, 0].text(3.5, 0.9, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Average score by age cat\nsns.barplot(x = loan_intents, y = li_avg_scores, data = li_scores_df, hue = loan_intents, legend = False, ax = ax[0, 1])\nax[0, 1].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0, 1].set_title(\"Average Model Score\", fontsize = 18)\nax[0, 1].set_ylabel(\"Average Score (Normalized)\", fontsize = 16)\nax[0, 1].set_xticks([0, 1, 2, 3, 4, 5])\nax[0, 1].set_xticklabels(loan_intents, rotation = 20)\nax[0, 1].text(4, 0.45, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0, 1].text(3.5, 0.45, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Percentage of high scores (above threshold) for each age cat\nsns.barplot(x = loan_intents, y = li_perc_high_scores_1, hue = loan_intents, legend = False, ax = ax[1, 0])\nax[1, 0].set_title(\"Percentage of Model Scores\\nAbove Threshold \", fontsize = 18)\nax[1, 0].set_ylabel(\"% of Scores Above Threshold\", fontsize = 16)\nax[1, 0].set_xticks([0, 1, 2, 3, 4, 5])\nax[1, 0].set_xticklabels(loan_intents, rotation = 20)\nax[1, 0].set_yticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\nax[1, 0].set_yticklabels([\"0%\", \"1%\", \"2%\", \"3%\", \"4%\", \"5%\", \"6%\", \"7%\", \"8%\"])\n\n# Percentage of all high scores (above threshold) from each age cat\nsns.barplot(x = loan_intents, y = li_perc_high_scores_2, hue = loan_intents, legend = False, ax = ax[1, 1])\nax[1, 1].set_title(\"Percentage of all Model Scores\\nAbove Threshold From each Age Category\", fontsize = 18)\nax[1, 1].set_ylabel(\"% of all Scores Above Threshold\", fontsize = 16)\nax[1, 1].set_xticks([0, 1, 2, 3, 4, 5])\nax[1, 1].set_xticklabels(loan_intents, rotation = 20)\nax[1, 1].set_yticks([0, 5, 10, 15, 20])\nax[1, 1].set_yticklabels([\"0%\", \"5%\", \"10%\", \"15%\", \"20%\"])\n\nfig.text(0.5, 0.95, \"Model Score Metrics by Loan Intent\", ha = \"center\", fontsize = 20)\nfig.text(0.5, 0.05, \"Loan Intent\", ha = \"center\", fontsize = 18)\nplt.subplots_adjust(wspace = 0.25, hspace = 0.35)\n\n\n# Computing true defualt rates for medical, venture, and educational loans\nm = test[test[\"loan_intent\"] == \"Medical\"].copy()\nv = test[test[\"loan_intent\"] == \"Venture\"].copy()\ne = test[test[\"loan_intent\"] == \"Education\"].copy()\nd = test[test[\"loan_intent\"] == \"Debt Consolidation\"].copy()\np = test[test[\"loan_intent\"] == \"Personal\"].copy()\nh = test[test[\"loan_intent\"] == \"Home Improvement\"].copy()\ntrue_default_rates = np.array([\n    (m[\"loan_status\"] == 1).mean(),\n    (v[\"loan_status\"] == 1).mean(),\n    (e[\"loan_status\"] == 1).mean(),\n    (d[\"loan_status\"] == 1).mean(),\n    (p[\"loan_status\"] == 1).mean(),\n    (h[\"loan_status\"] == 1).mean(),\n])\n\n\n['Venture' 'Debt Consolidation' 'Medical' 'Home Improvement' 'Education'\n 'Personal']\n\n\n\n\n\n\n\n\n\nCode above constructs 4 plots to observe how the model score varies across loan intent. The top two plots simply display the distribution of model scores and the average score for each loan intent. The bottom two plots show the percentage of high model scores within each loan intent and the percentage of overall high model scores attributed to each loan intent (computed by taking the number of high model scores for each loan intent over the total number of high model scores).\nFigure 8\nAs shown in the figures above, the the general distribution and average model scores across each loan intent are considerably similar. That is, it does not appear that any specific loan intents correspond to significantly high or low model scores. However, as shown in the bottom right plot, the largest percentage of all high model scores (scores that advise the bank to deny a borrower a loan), is attributed to medical loans. This may suggest that it is more difficult for borrowers to have medical loans approved. The actual rate of default for medical loans in the test data is about \\(30\\%\\), which is the second highest default rate (nearly equal to that of debt consolidation loans). Considering that medical loans make up the largest percentage of overall high model scores and have the highest true rate of default in the test data, it may be justified that the bank makes it more difficult for borrowers seeking these types of loans to be approved.\nAs for venture and educational loans, it appears that borrowers looking to take out these types of loans are similarly likely to be approved by the bank (they have very similar average model scores, percentage of high scores within themselves, and percentage makeups of the overall high model scores). Additionally, both venture loans and educational loans have similar default rates in the test data of about \\(15\\%\\) and \\(17\\%\\) respectively.\n\n\nLoan Approval and Borrower Income\n\n\nCode\n# Adding high/low score indicator col\ntest[\"high_score\"] = test[\"score\"] &gt;= best_threshold\ntest[\"low_inc\"] = (test[\"person_income\"] &lt;= 30000).astype(int)\ntest[\"high_inc\"] = (test[\"person_income\"] &gt;= 150000).astype(int)\ntest[\"person_inc_log\"] = np.log2(test[\"person_income\"])\n\nlow_inc = test[test[\"low_inc\"] == 1].copy()\nhigh_inc = test[test[\"high_inc\"] == 1].copy()\n\n# Plotting model score against borrower income\nfig, ax = plt.subplots(1, 3, figsize = (17.5, 10))\n\n# Score by income for low income borrowers\nsns.scatterplot(data = low_inc, x = \"person_income\", y = \"score\", hue = \"high_score\", palette = [\"darkorange\", \"purple\"], legend = False, ax = ax[0])\nax[0].set_title(\"Low Income\", fontsize = 16)\nax[0].set_ylabel(\"Score (Normalized)\", fontsize = 14)\nax[0].set_xlabel(\"\")\nax[0].set_xticks([5e3, 10e3, 15e3, 20e3, 25e3, 30e3])\nax[0].set_xticklabels([\"~$5000\", \"~$10000\", \"~$15000\", \"~$20000\", \"~$25000\", \"~$30000\"], rotation = 15)\nax[0].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0].text(8000, 0.9, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(5000, 0.9, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Score by income for high income borrowers\nsns.scatterplot(data = high_inc, x = \"person_income\", y = \"score\", hue = \"high_score\", palette = [\"darkorange\", \"purple\"], legend = False, ax = ax[1])\nax[1].set_xscale(\"log\", base = 2)\nax[1].set_title(\"High Income\", fontsize = 16)\nax[1].set_ylabel(\"\")\nax[1].set_xlabel(\"\")\nax[1].set_xticks([2 ** 18, 2 ** 19, 2 ** 20])\nax[1].set_xticklabels([\"~$260000\", \"~$500000\", \"~$1M\"], rotation = 15)\nax[1].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[1].text(2 ** 17.75, 0.45, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(2 ** 17.325, 0.45, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Score by income for all borrowers\nsns.scatterplot(data = test, x = \"person_income\", y = \"score\", hue = \"high_score\", palette = [\"darkorange\", \"purple\"], legend = False, ax = ax[2])\nax[2].set_xscale(\"log\", base = 2)\nax[2].set_title(\"All Borrowers\", fontsize = 16)\nax[2].set_ylabel(\"\")\nax[2].set_xlabel(\"\")\nax[2].set_xticks([2 ** 13, 2 ** 15, 2 ** 17, 2 ** 19, 2 ** 21])\nax[2].set_xticklabels([\"~$8000\", \"~$30000\", \"~$100000\", \"~$500000\", \"~$2M\"], rotation = 15)\nax[2].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[2].text(2 ** 18, 0.9, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[2].text(2 ** 17, 0.9, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\nfig.suptitle(\"Observing Model Score Against Borrower Income\", fontsize = 20)\nfig.text(0.5, 0.01, \"Borrower Income ($\\log_2$ scale on rightmost subplots)\", ha = \"center\", fontsize = 16)\n\nplt.show()\n\n# Calculating default rates for low and high income borrowers\ntrue_default_rates_inc = np.array([\n    (low_inc[\"loan_status\"] == 1).mean(),\n    (high_inc[\"loan_status\"] == 1).mean()\n])\n\n\n\n\n\n\n\n\n\nCode above constructs scatter plots observing model score by borrower income for low-income borrowers, high-income borrowers, and all borrowers. The threshold is included in each plot to depict where the score cutoff is.\nFigure 9\nAs depicted in the plots above, there seems to be a general trend between borrower income and their corresponding model score. That is, it appears that as a borrower’s income increases, their score decreases. This may suggest that the model makes it easier for higher-income individuals to be approved for loans than those at lower-income levels. This is supported by the fact that a considerable proportion of low-income borrowers (\\(\\leq \\$30000\\) annual earnings) were scored above the threshold (i.e. likely not approved) while none of the high-income borrowers (\\(\\geq \\$150000\\) annual earnings) were scored above the threshold. This may be sufficient evidence for the bank, but it is important to note that a borrower’s income is not actually one of the features the model was trained on. Thus, this observation may be due to other related factors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451A Blog - Spring 2025",
    "section": "",
    "text": "Project Solar-Searcher\n\n\n\n\n\nIdentifying the optimal locations for solar energy system development in the continental US\n\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPost 7 - Exploring Advanced Optimization Methods\n\n\n\n\n\nAn introductory examination of two advanced optimization methods, Newton’s Method and Adam, in basic logistic regression\n\n\n\n\n\nMay 1, 2025\n\n\nCol McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nPost 6 - Investigating Overfitting, Overparameterization, and Double-Descent\n\n\n\n\n\nAn introductory examination of overfitting, overparameterization, and double-descent instances with ML models.\n\n\n\n\n\nApr 12, 2025\n\n\nCol McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nPost 5 - Implementing Logistic Regression\n\n\n\n\n\nAn introductory exploration of logistic regression and gradient descent.\n\n\n\n\n\nApr 9, 2025\n\n\nCol McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nPost 4 - Implementing Perceptron\n\n\n\n\n\nAn introductory exploration of the Perceptron algorithm.\n\n\n\n\n\nMar 31, 2025\n\n\nCol McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nPost 3 - Auditing Bias in Machine Learning Models\n\n\n\n\n\nAn introductory examination of racial gender bias in a predictive ML model (predicting employment).\n\n\n\n\n\nMar 12, 2025\n\n\nCol McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nPost 2 - Exploring Automated Decision Models\n\n\n\n\n\nAn introductory analysis of an automated decision model in the context of credit-risk prediction\n\n\n\n\n\nMar 4, 2025\n\n\nCol McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nPost 1 - Classifying Palmer Penguins\n\n\n\n\n\nA ternary classification of penguin species: Gentoo, Adelie, and Chinstrap\n\n\n\n\n\nFeb 17, 2025\n\n\nCol McDermott\n\n\n\n\n\n\nNo matching items"
  }
]