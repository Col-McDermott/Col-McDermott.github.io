[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Blog Info",
    "section": "",
    "text": "Middlebury College CSCI 0451A Blog - Col McDermott\n\n\n\nCheck out my completed blog posts from\n\n\nCSCI 0451 Machine Learning\n\n\nwith Professor Phil Chodrow"
  },
  {
    "objectID": "posts/post_2/index.html",
    "href": "posts/post_2/index.html",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "",
    "text": "Code\n# Includuing all additional imports\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport itertools\n\ntrain = pd.read_csv(\"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\")\n\n\nIncluding all additional imports\nThe data used in this analysis is a simulated data set of credit bureau data based on patterns observed in real credit bureau data.\n\n\n\n\n\n\n\nCode\n# Data modification\ntrain_viz = train.copy()\nloan_status_recode = {0: \"Paid in Full\", 1: \"Defaulted\"}\nloan_intent_recode = {\"VENTURE\": \"Venture\", \"EDUCATION\": \"Education\", \"MEDICAL\": \"Medical\", \"HOMEIMPROVEMENT\": \"Home Improvement\", \"PERSONAL\": \"Personal\",\n \"DEBTCONSOLIDATION\" : \"Debt Consolidation\"}\ntrain_viz[\"loan_repayment\"] = train_viz[\"loan_status\"].map(loan_status_recode)\ntrain_viz[\"loan_intent\"] = train_viz[\"loan_intent\"].map(loan_intent_recode)\ntrain_viz.loc[train_viz[\"person_age\"] &gt;= 100, \"person_age\"] = None\ntrain_viz = train_viz.dropna()\n\n# Subsetting data by loan status\ndefaulted = train_viz[train_viz[\"loan_status\"] == 1].copy().dropna()\nrepaid = train_viz[train_viz[\"loan_status\"] == 0].copy().dropna()\n\n\nModifying training data for visualization\n\n\nCode\n# Creating linear regression models and calculating R^2 values\n## Defaulted\nc1 = np.polyfit(defaulted[\"loan_amnt\"], defaulted[\"person_income\"], 1)\np1 = np.polyval(c1, defaulted[\"loan_amnt\"])\nr1 = defaulted[\"person_income\"] - p1\nssr1 = np.sum(r1 ** 2)\nsst1 = np.sum((defaulted[\"person_income\"] - np.mean(defaulted[\"loan_amnt\"])) ** 2)\nrs1 = 1 - (ssr1 / sst1)\n\n## Repaid\nc2 = np.polyfit(repaid[\"loan_amnt\"], repaid[\"person_income\"], 1)\np2 = np.polyval(c2, repaid[\"loan_amnt\"])\nr2 = repaid[\"person_income\"] - p2\nssr2 = np.sum(r2**2)\nsst2 = np.sum((repaid[\"person_income\"] - np.mean(repaid[\"loan_amnt\"]))**2)\nrs2 = 1 - (ssr2 / sst2)\n\n\nCreating linear regression models and calculating R^2 values for subsets of defaulted and fully repaid loans\n\n\nCode\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize = (15, 10))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Scatterplot and regression line for borrower income by loan amount of defaulted loans\nax[0] = sns.scatterplot(data = defaulted, x = \"loan_amnt\", y = \"person_income\", color = \"purple\", edgecolor = \"purple\", alpha = 0.25, ax = ax[0])\nsns.regplot(data = defaulted, x = \"loan_amnt\", y = \"person_income\", scatter = False, line_kws={\"color\": \"darkorange\"}, ax = ax[0])\nax[0].set_yscale(\"log\", base = 2)\nax[0].set_xlabel(\"\")\nax[0].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000, 35000])\nax[0].set_xticklabels([\"$0\", \"$5000\", \"$10000\", \"$15000\", \"$20000\", \"$25000\", \"$30000\", \"$35000\"], rotation = 30, fontsize = 14)\nax[0].set_ylabel(f\"Borrower\\'s Income ($\\log_2$ scale)\", fontsize = 16, labelpad = 15)\nax[0].set_yticks([2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23])\nax[0].set_yticklabels([\"&gt;$4000\", \"&gt;$8000\", \"&gt;$16000\", \"&gt;$32000\", \"&gt;$64000\", \"&gt;$128000\", \"&gt;$256000\", \"&gt;$512000\", \"&gt;$1024000\", \"&gt;$2048000\", \"&gt;$4096000\", \"&lt;$8192000\"], rotation = 15, fontsize = 14)\nax[0].set_title(\"Defaulted Loans\", fontsize = 18)\nax[0].text(27000, 12288, f\"     $R^2 = {rs1:.3f}$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(24000, 12288, \"\\u2013\", color=\"darkorange\", ha=\"left\", va=\"center\", fontsize=15, fontweight=\"bold\")\n\n# Scatterplot and regression line for borrower income by loan amount of repaid loans\nax[1] = sns.scatterplot(data = repaid, x = \"loan_amnt\", y = \"person_income\", color = \"darkorange\", edgecolor = \"darkorange\", alpha = 0.5, ax = ax[1])\nsns.regplot(data = repaid, x = \"loan_amnt\", y = \"person_income\", scatter = False, line_kws={\"color\": \"purple\"}, ax = ax[1])\nax[1].set_yscale(\"log\", base = 2)\nax[1].set_xlabel(\"\")\nax[1].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000, 35000])\nax[1].set_xticklabels([\"$0\", \"$5000\", \"$10000\", \"$15000\", \"$20000\", \"$25000\", \"$30000\", \"$35000\"], rotation=30, fontsize = 14)\nax[1].set_ylabel(\"\")\nax[1].set_yticks([2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23])\nax[1].set_yticklabels(12 * [\"\"], rotation = 15)\nax[1].set_title(\"Repaid Loans\", fontsize = 18)\nax[1].text(27000, 12288, f\"     $R^2 = {rs2:.3f}$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(24000, 12288, \"\\u2013\", color=\"purple\", ha=\"left\", va=\"center\", fontsize=15, fontweight=\"bold\")\n\nfig.suptitle(\"Borrower\\'s Income by Loan Amount Displayed by Loan Repayment Status\", fontsize = 20)\nfig.text(0.47, 0.025, \"Loan Amount\", fontsize = 16)\nplt.subplots_adjust(wspace=0.1)\n\n\n\n\n\n\n\n\n\nFigure 1\nThe plots above display the relationship between a borrower’s income and the loan amount borrowed for both defaulted and fully repaid loans. To assist the visualization of the data points, the dependent variable person_income is adjusted using a \\(\\log_2\\) scale. With this scale in place and for both defaulted and fully repaid loans, there appears to be a transformed linear relationship between a borrower’s income and the loan amount borrowed. Additionally, this transformed linear relationship appears to be positive and moderately strong (given both \\(R^2\\) values &gt; 0.6). For both borrowers who defaulted on loans and borrower’s who fully repaid their loans, it appears that as the loan amount increases, a borrower’s income increases (adjusted by a \\(\\log_2\\) scale). Considering this observed relationship, it is reasonable to assert that bigger loans are borrowed by individuals with higher income levels, regardless of whether or not those individuals defaulted or fully repaid their loans.\n\n\nCode\n# Subsetting data to the three most common loan intents\ncommon_loan_intents = train_viz.copy()\ncommon_loan_intents = train_viz[train_viz[\"loan_intent\"].isin([\"Education\", \"Medical\", \"Venture\"])].copy()\n\n# Creating Boxen Plot\nfig, ax = plt.subplots(1, 1, figsize = (10, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\np = sns.boxenplot(common_loan_intents, x = \"loan_intent\", y = \"person_age\", hue = \"loan_repayment\", palette = [\"darkorange\", \"purple\"])\np.set_xlabel(\"Intention for Loan\", fontsize = 14)\np.set_ylabel(\"Borrower\\'s Age\", fontsize = 14)\np.set_xticks([\"Education\", \"Medical\", \"Venture\"])\np.legend(title = \"Loan Status\", frameon = True)\np.set_title(\"Distribution of Borrower\\'s Age for Top Three Loan Intents\\nGrouped by Loan Repayment Status\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFigure 2\nThe figure above shows an sns.boxenplot displaying the distribution of borrower’s age for the three most common loan intents across both defaulted and fully repaid loans.\nFor education loans: The median age of both borrowers who defaulted on loans and borrowers who fully repaid their loans is less than or equal to that of defaulting and repaying borrowers taking out medical and venture loans. Additionally, the median age of educational loan borrowers is lower for those who fully repaid their loans than it is for those who defaulted – Thus, a topic for another analysis could be to explore how the age of educational loan borrowers has an impact on such loans being repaid.\nFor medical loans: It appears that the median age of borrowers is greater than those taking out loans for education or venture. There is also very little visually observable difference in the median age of borrowers who defaulted and borrowers who fully repaid their medical loans.\nFor ventures loans: The median age of borrowers who fully repaid their loans is greater than that of those who defaulted on their loans. Similarly to the topic posed for educational loan borrowers, it could be interesting to explore in a future study how the age of venture loan borrowers has an effect on such loans being repaid.\n\n\n\n\n\nCode\n# Summary Statistics\n\n# Helper method to calculate coefficient of variation (%)\ndef cv(col):\n    return (col.std() / col.mean()) * 100\n\n# Creating a table grouped by penguin species and sex, showing general summary stats for several quantitative variables\nsum_stats = train_viz.rename(columns = {\"loan_repayment\": \"Loan Repayment\", \"loan_amnt\": \"Loan Amount\", \"person_income\": \"Borrower\\'s Income\", \"loan_intent\": \"Loan Intent\", \"person_age\": \"Borrower\\'s Age\", \"loan_percent_income\": \"Loan Percent Income\"}).copy()\nsum_stats = sum_stats.groupby([\"Loan Repayment\", \"Loan Intent\"]).aggregate({\"Loan Amount\" : [\"mean\", \"std\", cv], \n                                                             \"Borrower\\'s Income\" : [\"mean\", \"std\", cv], \"Loan Percent Income\": [\"mean\", \"std\", cv], \n                                                             \"Borrower\\'s Age\": [\"mean\", \"std\", cv]})\nsum_stats = sum_stats.rename(columns = {\"mean\": \"Mean\", \"std\": \"STD\", \"cv\": \"CV (%)\"})\nsum_stats = sum_stats.round(2)\nsum_stats\n\n\n\n\n\n\n\n\n\n\nLoan Amount\nBorrower's Income\nLoan Percent Income\nBorrower's Age\n\n\n\n\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\n\n\nLoan Repayment\nLoan Intent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefaulted\nDebt Consolidation\n11283.45\n7357.29\n65.20\n54553.15\n37624.33\n68.97\n0.24\n0.13\n57.00\n27.76\n6.52\n23.49\n\n\nEducation\n10912.82\n6979.08\n63.95\n47283.67\n30264.34\n64.01\n0.26\n0.13\n51.90\n27.09\n6.10\n22.50\n\n\nHome Improvement\n10035.04\n7324.85\n72.99\n49794.13\n33062.93\n66.40\n0.22\n0.13\n57.24\n27.62\n6.04\n21.86\n\n\nMedical\n11438.49\n7190.60\n62.86\n52477.12\n44092.21\n84.02\n0.24\n0.13\n54.74\n27.70\n6.31\n22.78\n\n\nPersonal\n10459.89\n6884.86\n65.82\n46965.14\n39080.81\n83.21\n0.25\n0.13\n51.99\n27.24\n6.09\n22.37\n\n\nVenture\n11115.68\n6695.34\n60.23\n44439.34\n27743.83\n62.43\n0.28\n0.13\n47.39\n26.74\n5.45\n20.38\n\n\nPaid in Full\nDebt Consolidation\n9050.14\n5810.33\n64.20\n72388.92\n61644.36\n85.16\n0.14\n0.08\n58.11\n27.54\n5.66\n20.56\n\n\nEducation\n9206.44\n6010.44\n65.29\n67866.15\n41045.51\n60.48\n0.15\n0.09\n57.71\n26.43\n5.49\n20.79\n\n\nHome Improvement\n10518.51\n6398.11\n60.83\n82499.92\n50452.00\n61.15\n0.14\n0.09\n61.15\n29.47\n5.48\n18.60\n\n\nMedical\n8570.88\n5645.54\n65.87\n65116.70\n51476.94\n79.05\n0.15\n0.08\n56.46\n27.96\n6.17\n22.07\n\n\nPersonal\n9441.60\n6133.81\n64.97\n72343.94\n54121.45\n74.81\n0.15\n0.09\n57.83\n28.49\n7.42\n26.03\n\n\nVenture\n9307.47\n6064.40\n65.16\n70493.68\n58762.60\n83.36\n0.15\n0.09\n59.45\n27.74\n5.98\n21.57\n\n\n\n\n\n\n\nIn creating the table above, I did some brief research on CV to more easily interpret the STD and wrote the helper method\nTable 1\nThe table above presents some general summary statistics from the data. I was interested in examining the mean, STD, and CV for some of the primary quantitative features in the data (loan_amnt, person_income, loan_percent_income, person_age) across each loan intent for both defaulted and fully repaid loans.\nFor Defaulted Loans:\n\nMedical loans are greatest in size on average compared to the other loan intents.\n\nHome Improvement loans are the smallest in size on average.\n\nDebt Consolidation loans are taken out by borrowers with the greatest average income.\n\nVenture loans are taken out by borrowers with the smallest average income.\n\nVenture loans have the largest average loan-value-percent-income in borrowers\n\nHome improvement loans have the smallest average loan-value-percent-income among borrowers\n\nDebt Consolidation loans are taken out by borrowers with the largest age on average.\n\nVenture loans correspond to borrowers with the lowest age on average.\n\n\nFor Fully Repaid Loans:\n\nHome Improvement loans are greatest in size on average compared to the other loan intents.\n\nMedical loans are the smallest in size on average. (Interestingly, this is the complete opposite of defaulted loans)\n\nHome Improvement loans are taken out by borrowers with the greatest average income.\n\nMedical loans are taken out by borrowers with the smallest average income.\n\nMedical, venture, education, and personal loans share the largest average loan-value-percent-income in borrowers while home improvement and debt consolidation loans correspond to the smallest average loan-value-percent-income in borrowers.\n\nIn terms of the spread of data across the variables displayed in the table, all four features possess relatively high coefficient of variation (CV%) values. This suggests that there is a notable degree of variability in the distribution of each of these features. However, some features appear to vary more than others: Loan Amount, Borrower's Income, and Loan Percent Income all have average CV (%) values &gt; 50% while Borrower's Age has an average CV (%) \\(\\approx\\) 20%. Additionally, there appears to be no considerable difference in CV (%) across each of the recorded loan intents nor between defaulted and fully repaid loans in general."
  },
  {
    "objectID": "posts/post_2/index.html#exploring-the-data",
    "href": "posts/post_2/index.html#exploring-the-data",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "",
    "text": "Code\n# Data modification\ntrain_viz = train.copy()\nloan_status_recode = {0: \"Paid in Full\", 1: \"Defaulted\"}\nloan_intent_recode = {\"VENTURE\": \"Venture\", \"EDUCATION\": \"Education\", \"MEDICAL\": \"Medical\", \"HOMEIMPROVEMENT\": \"Home Improvement\", \"PERSONAL\": \"Personal\",\n \"DEBTCONSOLIDATION\" : \"Debt Consolidation\"}\ntrain_viz[\"loan_repayment\"] = train_viz[\"loan_status\"].map(loan_status_recode)\ntrain_viz[\"loan_intent\"] = train_viz[\"loan_intent\"].map(loan_intent_recode)\ntrain_viz.loc[train_viz[\"person_age\"] &gt;= 100, \"person_age\"] = None\ntrain_viz = train_viz.dropna()\n\n# Subsetting data by loan status\ndefaulted = train_viz[train_viz[\"loan_status\"] == 1].copy().dropna()\nrepaid = train_viz[train_viz[\"loan_status\"] == 0].copy().dropna()\n\n\nModifying training data for visualization\n\n\nCode\n# Creating linear regression models and calculating R^2 values\n## Defaulted\nc1 = np.polyfit(defaulted[\"loan_amnt\"], defaulted[\"person_income\"], 1)\np1 = np.polyval(c1, defaulted[\"loan_amnt\"])\nr1 = defaulted[\"person_income\"] - p1\nssr1 = np.sum(r1 ** 2)\nsst1 = np.sum((defaulted[\"person_income\"] - np.mean(defaulted[\"loan_amnt\"])) ** 2)\nrs1 = 1 - (ssr1 / sst1)\n\n## Repaid\nc2 = np.polyfit(repaid[\"loan_amnt\"], repaid[\"person_income\"], 1)\np2 = np.polyval(c2, repaid[\"loan_amnt\"])\nr2 = repaid[\"person_income\"] - p2\nssr2 = np.sum(r2**2)\nsst2 = np.sum((repaid[\"person_income\"] - np.mean(repaid[\"loan_amnt\"]))**2)\nrs2 = 1 - (ssr2 / sst2)\n\n\nCreating linear regression models and calculating R^2 values for subsets of defaulted and fully repaid loans\n\n\nCode\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize = (15, 10))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Scatterplot and regression line for borrower income by loan amount of defaulted loans\nax[0] = sns.scatterplot(data = defaulted, x = \"loan_amnt\", y = \"person_income\", color = \"purple\", edgecolor = \"purple\", alpha = 0.25, ax = ax[0])\nsns.regplot(data = defaulted, x = \"loan_amnt\", y = \"person_income\", scatter = False, line_kws={\"color\": \"darkorange\"}, ax = ax[0])\nax[0].set_yscale(\"log\", base = 2)\nax[0].set_xlabel(\"\")\nax[0].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000, 35000])\nax[0].set_xticklabels([\"$0\", \"$5000\", \"$10000\", \"$15000\", \"$20000\", \"$25000\", \"$30000\", \"$35000\"], rotation = 30, fontsize = 14)\nax[0].set_ylabel(f\"Borrower\\'s Income ($\\log_2$ scale)\", fontsize = 16, labelpad = 15)\nax[0].set_yticks([2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23])\nax[0].set_yticklabels([\"&gt;$4000\", \"&gt;$8000\", \"&gt;$16000\", \"&gt;$32000\", \"&gt;$64000\", \"&gt;$128000\", \"&gt;$256000\", \"&gt;$512000\", \"&gt;$1024000\", \"&gt;$2048000\", \"&gt;$4096000\", \"&lt;$8192000\"], rotation = 15, fontsize = 14)\nax[0].set_title(\"Defaulted Loans\", fontsize = 18)\nax[0].text(27000, 12288, f\"     $R^2 = {rs1:.3f}$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(24000, 12288, \"\\u2013\", color=\"darkorange\", ha=\"left\", va=\"center\", fontsize=15, fontweight=\"bold\")\n\n# Scatterplot and regression line for borrower income by loan amount of repaid loans\nax[1] = sns.scatterplot(data = repaid, x = \"loan_amnt\", y = \"person_income\", color = \"darkorange\", edgecolor = \"darkorange\", alpha = 0.5, ax = ax[1])\nsns.regplot(data = repaid, x = \"loan_amnt\", y = \"person_income\", scatter = False, line_kws={\"color\": \"purple\"}, ax = ax[1])\nax[1].set_yscale(\"log\", base = 2)\nax[1].set_xlabel(\"\")\nax[1].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000, 35000])\nax[1].set_xticklabels([\"$0\", \"$5000\", \"$10000\", \"$15000\", \"$20000\", \"$25000\", \"$30000\", \"$35000\"], rotation=30, fontsize = 14)\nax[1].set_ylabel(\"\")\nax[1].set_yticks([2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21, 2**22, 2**23])\nax[1].set_yticklabels(12 * [\"\"], rotation = 15)\nax[1].set_title(\"Repaid Loans\", fontsize = 18)\nax[1].text(27000, 12288, f\"     $R^2 = {rs2:.3f}$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(24000, 12288, \"\\u2013\", color=\"purple\", ha=\"left\", va=\"center\", fontsize=15, fontweight=\"bold\")\n\nfig.suptitle(\"Borrower\\'s Income by Loan Amount Displayed by Loan Repayment Status\", fontsize = 20)\nfig.text(0.47, 0.025, \"Loan Amount\", fontsize = 16)\nplt.subplots_adjust(wspace=0.1)\n\n\n\n\n\n\n\n\n\nFigure 1\nThe plots above display the relationship between a borrower’s income and the loan amount borrowed for both defaulted and fully repaid loans. To assist the visualization of the data points, the dependent variable person_income is adjusted using a \\(\\log_2\\) scale. With this scale in place and for both defaulted and fully repaid loans, there appears to be a transformed linear relationship between a borrower’s income and the loan amount borrowed. Additionally, this transformed linear relationship appears to be positive and moderately strong (given both \\(R^2\\) values &gt; 0.6). For both borrowers who defaulted on loans and borrower’s who fully repaid their loans, it appears that as the loan amount increases, a borrower’s income increases (adjusted by a \\(\\log_2\\) scale). Considering this observed relationship, it is reasonable to assert that bigger loans are borrowed by individuals with higher income levels, regardless of whether or not those individuals defaulted or fully repaid their loans.\n\n\nCode\n# Subsetting data to the three most common loan intents\ncommon_loan_intents = train_viz.copy()\ncommon_loan_intents = train_viz[train_viz[\"loan_intent\"].isin([\"Education\", \"Medical\", \"Venture\"])].copy()\n\n# Creating Boxen Plot\nfig, ax = plt.subplots(1, 1, figsize = (10, 7.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\np = sns.boxenplot(common_loan_intents, x = \"loan_intent\", y = \"person_age\", hue = \"loan_repayment\", palette = [\"darkorange\", \"purple\"])\np.set_xlabel(\"Intention for Loan\", fontsize = 14)\np.set_ylabel(\"Borrower\\'s Age\", fontsize = 14)\np.set_xticks([\"Education\", \"Medical\", \"Venture\"])\np.legend(title = \"Loan Status\", frameon = True)\np.set_title(\"Distribution of Borrower\\'s Age for Top Three Loan Intents\\nGrouped by Loan Repayment Status\", fontsize = 18)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFigure 2\nThe figure above shows an sns.boxenplot displaying the distribution of borrower’s age for the three most common loan intents across both defaulted and fully repaid loans.\nFor education loans: The median age of both borrowers who defaulted on loans and borrowers who fully repaid their loans is less than or equal to that of defaulting and repaying borrowers taking out medical and venture loans. Additionally, the median age of educational loan borrowers is lower for those who fully repaid their loans than it is for those who defaulted – Thus, a topic for another analysis could be to explore how the age of educational loan borrowers has an impact on such loans being repaid.\nFor medical loans: It appears that the median age of borrowers is greater than those taking out loans for education or venture. There is also very little visually observable difference in the median age of borrowers who defaulted and borrowers who fully repaid their medical loans.\nFor ventures loans: The median age of borrowers who fully repaid their loans is greater than that of those who defaulted on their loans. Similarly to the topic posed for educational loan borrowers, it could be interesting to explore in a future study how the age of venture loan borrowers has an effect on such loans being repaid.\n\n\n\n\n\nCode\n# Summary Statistics\n\n# Helper method to calculate coefficient of variation (%)\ndef cv(col):\n    return (col.std() / col.mean()) * 100\n\n# Creating a table grouped by penguin species and sex, showing general summary stats for several quantitative variables\nsum_stats = train_viz.rename(columns = {\"loan_repayment\": \"Loan Repayment\", \"loan_amnt\": \"Loan Amount\", \"person_income\": \"Borrower\\'s Income\", \"loan_intent\": \"Loan Intent\", \"person_age\": \"Borrower\\'s Age\", \"loan_percent_income\": \"Loan Percent Income\"}).copy()\nsum_stats = sum_stats.groupby([\"Loan Repayment\", \"Loan Intent\"]).aggregate({\"Loan Amount\" : [\"mean\", \"std\", cv], \n                                                             \"Borrower\\'s Income\" : [\"mean\", \"std\", cv], \"Loan Percent Income\": [\"mean\", \"std\", cv], \n                                                             \"Borrower\\'s Age\": [\"mean\", \"std\", cv]})\nsum_stats = sum_stats.rename(columns = {\"mean\": \"Mean\", \"std\": \"STD\", \"cv\": \"CV (%)\"})\nsum_stats = sum_stats.round(2)\nsum_stats\n\n\n\n\n\n\n\n\n\n\nLoan Amount\nBorrower's Income\nLoan Percent Income\nBorrower's Age\n\n\n\n\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\n\n\nLoan Repayment\nLoan Intent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefaulted\nDebt Consolidation\n11283.45\n7357.29\n65.20\n54553.15\n37624.33\n68.97\n0.24\n0.13\n57.00\n27.76\n6.52\n23.49\n\n\nEducation\n10912.82\n6979.08\n63.95\n47283.67\n30264.34\n64.01\n0.26\n0.13\n51.90\n27.09\n6.10\n22.50\n\n\nHome Improvement\n10035.04\n7324.85\n72.99\n49794.13\n33062.93\n66.40\n0.22\n0.13\n57.24\n27.62\n6.04\n21.86\n\n\nMedical\n11438.49\n7190.60\n62.86\n52477.12\n44092.21\n84.02\n0.24\n0.13\n54.74\n27.70\n6.31\n22.78\n\n\nPersonal\n10459.89\n6884.86\n65.82\n46965.14\n39080.81\n83.21\n0.25\n0.13\n51.99\n27.24\n6.09\n22.37\n\n\nVenture\n11115.68\n6695.34\n60.23\n44439.34\n27743.83\n62.43\n0.28\n0.13\n47.39\n26.74\n5.45\n20.38\n\n\nPaid in Full\nDebt Consolidation\n9050.14\n5810.33\n64.20\n72388.92\n61644.36\n85.16\n0.14\n0.08\n58.11\n27.54\n5.66\n20.56\n\n\nEducation\n9206.44\n6010.44\n65.29\n67866.15\n41045.51\n60.48\n0.15\n0.09\n57.71\n26.43\n5.49\n20.79\n\n\nHome Improvement\n10518.51\n6398.11\n60.83\n82499.92\n50452.00\n61.15\n0.14\n0.09\n61.15\n29.47\n5.48\n18.60\n\n\nMedical\n8570.88\n5645.54\n65.87\n65116.70\n51476.94\n79.05\n0.15\n0.08\n56.46\n27.96\n6.17\n22.07\n\n\nPersonal\n9441.60\n6133.81\n64.97\n72343.94\n54121.45\n74.81\n0.15\n0.09\n57.83\n28.49\n7.42\n26.03\n\n\nVenture\n9307.47\n6064.40\n65.16\n70493.68\n58762.60\n83.36\n0.15\n0.09\n59.45\n27.74\n5.98\n21.57\n\n\n\n\n\n\n\nIn creating the table above, I did some brief research on CV to more easily interpret the STD and wrote the helper method\nTable 1\nThe table above presents some general summary statistics from the data. I was interested in examining the mean, STD, and CV for some of the primary quantitative features in the data (loan_amnt, person_income, loan_percent_income, person_age) across each loan intent for both defaulted and fully repaid loans.\nFor Defaulted Loans:\n\nMedical loans are greatest in size on average compared to the other loan intents.\n\nHome Improvement loans are the smallest in size on average.\n\nDebt Consolidation loans are taken out by borrowers with the greatest average income.\n\nVenture loans are taken out by borrowers with the smallest average income.\n\nVenture loans have the largest average loan-value-percent-income in borrowers\n\nHome improvement loans have the smallest average loan-value-percent-income among borrowers\n\nDebt Consolidation loans are taken out by borrowers with the largest age on average.\n\nVenture loans correspond to borrowers with the lowest age on average.\n\n\nFor Fully Repaid Loans:\n\nHome Improvement loans are greatest in size on average compared to the other loan intents.\n\nMedical loans are the smallest in size on average. (Interestingly, this is the complete opposite of defaulted loans)\n\nHome Improvement loans are taken out by borrowers with the greatest average income.\n\nMedical loans are taken out by borrowers with the smallest average income.\n\nMedical, venture, education, and personal loans share the largest average loan-value-percent-income in borrowers while home improvement and debt consolidation loans correspond to the smallest average loan-value-percent-income in borrowers.\n\nIn terms of the spread of data across the variables displayed in the table, all four features possess relatively high coefficient of variation (CV%) values. This suggests that there is a notable degree of variability in the distribution of each of these features. However, some features appear to vary more than others: Loan Amount, Borrower's Income, and Loan Percent Income all have average CV (%) values &gt; 50% while Borrower's Age has an average CV (%) \\(\\approx\\) 20%. Additionally, there appears to be no considerable difference in CV (%) across each of the recorded loan intents nor between defaulted and fully repaid loans in general."
  },
  {
    "objectID": "posts/post_2/index.html#feature-selection",
    "href": "posts/post_2/index.html#feature-selection",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n\nCode\n# Modifying the training data\nX = train.drop([\"loan_status\", \"loan_grade\"], axis = 1, errors = \"ignore\")\ny = train[\"loan_status\"]\n\n# Creating one-hot encodings for qualitative variables\nX = pd.get_dummies(X, drop_first = True)\n\n# Removing NAs from data while maintaining alignment\nclean = pd.concat([X, y], axis = 1)\nclean = clean.dropna()\nX_train = clean.drop(\"loan_status\", axis = 1, errors = \"ignore\").copy()\ny_train = clean[\"loan_status\"].copy()\n\n# Adding new columns to X_train\n## Ratio of employment length to credit history\nX_train[\"credit_hist_emp_len\"] =  X_train[\"person_emp_length\"] / X_train[\"cb_person_cred_hist_length\"]\n\n# Identifying quantitative and qualitative variables in the data\nquant_vars = [\n              \"person_age\", \n              \"person_income\", \n              \"person_emp_length\", \n              \"loan_amnt\", \n              \"loan_int_rate\", \n              \"loan_percent_income\", \n              \"cb_person_cred_hist_length\",\n              \"credit_hist_emp_len\"\n            ]\nqual_vars = [\n              \"person_home_ownership_OTHER\",\n              \"person_home_ownership_OWN\",\n              \"person_home_ownership_RENT\",\n              \"loan_intent_EDUCATION\",\n              \"loan_intent_HOMEIMPROVEMENT\",\n              \"loan_intent_MEDICAL\",\n              \"loan_intent_PERSONAL\",\n              \"loan_intent_VENTURE\",\n              \"cb_person_default_on_file_Y\"\n            ]\n\n# Combination of selected quant and qual variables\ncols = quant_vars + qual_vars\n\n# Toggle to run feature selection process\nif (False):\n\n  # Iterating through all possible subsets of cols, evaluating a model with each subset, to determine the optimal set of variables to use\n  best_vars = []\n  best_avg_score = 0\n\n  # Paramters to determine the size of tested subsets\n  lower_bound = 4\n  upper_bound = 5\n  for i in range(lower_bound, upper_bound):\n      \n      # Iterating through all subsets\n      for subset in itertools.combinations(cols, i):\n        curr_vars = list(subset)\n        # Fitting a model to each subset\n        LR = LogisticRegression(random_state = 69)\n        LR.fit(X_train[curr_vars], y_train)\n        curr_avg_score = cross_val_score(LR, X_train[curr_vars], y_train, cv = 5).mean()\n        \n        # Update the best average score and the best variables to use for the model\n        if (curr_avg_score &gt; best_avg_score):\n          best_avg_score = curr_avg_score\n          best_vars = curr_vars\n\n# These are the best 5 variables selected after the iterative feature selection process above\nbest_vars = [\"loan_percent_income\", \"cb_person_cred_hist_length\", \"person_home_ownership_OTHER\", \"person_home_ownership_RENT\", \"loan_intent_HOMEIMPROVEMENT\"]\n\n# Refined model\nLR = LogisticRegression(random_state = 69)\nLR.fit(X_train[best_vars], y_train)\nscore = LR.score(X_train[best_vars], y_train)\nprint(f\"Refined Model Accuracy: {score * 100: .3f}%\")\n\n# Setting the weights vector\nw = np.array(LR.coef_[0])\n\n\nRefined Model Accuracy:  85.096%\n\n\nCode above shows the iterative process of trying all combinations of certain sizes for all features and taking those that lead to the best cross-validated model performance\nTo begin the feature selection process, the training data was informally divided into its quantitative and qualitative variables. The feature selection process involved iterating through a subset of the power set (all possible subsets) of the training data. All combinations of 1 up to 5 (out of 17) features were evaluated (beyond 5 features was too computationally expensive). According to the feature selection above, the 5 best variables to train the model with are loan_percent_income, cb_person_cred_hist_length (length of borrower’s credit history), person_home_ownership_OTHER (borrowers with home ownership status as “OTHER”), person_home_ownership_RENT (borrowers who are renting a home), and loan_intent_HOMEIMPROVEMENT (borrowers who intend to use a loan for home improvement). With these features used, the LogisticRegression model achieved just over \\(85\\%\\) training accuracy. While there is certainly room to improve the model’s training accuracy, this is the highest score achieved using the variables provided by the data and without undertaking a massive computational load (i.e. super long run time to iterate through all elements of the variable power set). The weights used by the model were extracted and stored in a weights vector w to be used in the scoring function described below."
  },
  {
    "objectID": "posts/post_2/index.html#creating-a-scoring-function",
    "href": "posts/post_2/index.html#creating-a-scoring-function",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Creating a Scoring Function",
    "text": "Creating a Scoring Function\n\n\nCode\n# Function to compute the score for each observation\ndef lin_score(X, w):\n    return X@w\n\n# Computing scores of each training observation and observing confusion matrix for the training data\nscores = lin_score(X_train[best_vars], w)\n\n# Normalizing the scores\nscores = (scores - scores.min()) / (scores.max() - scores.min())\n\n# Observing confusion matrix for the training data\nLR_pred = LR.predict(X_train[best_vars])\nC = confusion_matrix(y_train, LR_pred)\n\n# Establishing model predictions for the train data\ny_train_pred = LR.predict(X_train[best_vars])\nC = confusion_matrix(y_train, y_train_pred)\nloan_status = [\"Fully Repaid\", \"Defaulted\"]\n\n# Creating a heatmap for better confusion matrix visualization\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 7.5))\nmask1 = np.zeros_like(C, dtype=bool)\nmask1[:, 1] = True\nmask2 = np.zeros_like(C, dtype=bool)\nmask2[:, 0] = True\npurples = LinearSegmentedColormap.from_list(\"purple_gradient\", [\"#ce93d8\", \"#4a148c\"])\ndarkoranges = LinearSegmentedColormap.from_list(\"darkorange_gradient\", [\"#FFD580\", \"darkorange\"])\n# Plot the first column with purple gradient\nax1 = sns.heatmap(C, annot=True, fmt=\"d\", cmap=purples, \n                mask=mask1, cbar=False, xticklabels=[\"Fully Repaid\", \"Defaulted\"],\n                yticklabels=loan_status)\n\n# Plot the second column with orange gradient\nax2 = sns.heatmap(C, annot=True, fmt=\"d\", cmap=darkoranges, \n                mask=mask2, cbar=False, xticklabels=[\"Fully Repaid\", \"Defaulted\"],\n                yticklabels=loan_status)\n\n# Setting labels and title\nplt.xlabel(\"Predicted Loan Status\")\nplt.ylabel(\"True Loan Status\")\nplt.title(\"Lending Data Classification Confusion Matrix\")\n\nplt.show()\n\n# Printing confusion matrix results\nprint(f\"There were {C[0, 0]} fully repaid loan(s) that were predicted to be fully repaid.\")\nprint(f\"There were {C[0, 1]} fully repaid loan(s) that were predicted to be defaulted.\")\nprint(f\"There were {C[1, 0]} defaulted loan(s) that were predicted to be fully repaid.\")\nprint(f\"There were {C[1, 1]} defaulted loan(s) that were predicted to be defaulted.\")\n\n\n\n\n\n\n\n\n\nThere were 17702 fully repaid loan(s) that were predicted to be fully repaid.\nThere were 279 fully repaid loan(s) that were predicted to be defaulted.\nThere were 3135 defaulted loan(s) that were predicted to be fully repaid.\nThere were 1791 defaulted loan(s) that were predicted to be defaulted.\n\n\nCode above shows the linear scoring function used to determine model predictions and evaluate model accuracy. The scores were normalized to ensure that all scores are within the interval [0, 1]\nFigure 3\nFor this analysis, a linear scoring function is used. Specifically, the linear scoring function computes the score of each data observation by taking the dot product of each observation with the weights vector w. To ensure that all scores are values on the interval [0, 1], the scores are normalized after they are calculated (i.e. the scores are updated to be the difference between each raw score and the minimum raw score divided by the range of raw scores). Additionally, the confusion matrix for the model’s performance is displayed above. The model’s false positive rate is approximately \\(1.55\\%\\) while the true positive rate is approximately \\(36.34\\%\\). Conversely, the model’s false negative rate is approximately \\(63.66\\%\\) while the true negative rate is approximately \\(98.45\\%\\). With this information, it appears that the model rarely misclassifies fully repaid loans as defaulted (low FPR). However, the model frequently (moderately high FNR) misclassifies defaulted loans as being fully repaid. In general, with a very high TNR, the model is very accurate at detecting truly fully repaid loans but much less accurate (low TPR) at recognizing truly defaulted loans. It is important to note here that the initial objective for this model was to accurately predict if a prospective borrower will default on a loan, thus suggesting that perhaps the model and or the scoring function should be slightly redesigned to increase the TPR. However, given the introductory nature of this brief study, the analysis will continue with the current model as is.\n\nPlotting Distribution of Scores\n\n\nCode\n# Distribution of Scores - Code Provided by Prof. Chodrow\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nhist = ax.hist(scores, bins = 50, color = \"darkorange\", alpha = 0.75, linewidth = 1, edgecolor = \"purple\")\nlabs = ax.set(xlabel = r\"Scores (Normalized)\", ylabel = \"Frequency\") \n\n# Percentage (the majority) of scores below 0.5\nmaj_scores = (scores &lt;= 0.5).mean()\n\n\n\n\n\n\n\n\n\nFigure 4\nAs observed in the distribution of normalized scores above, it appears that the strong majority (\\(\\approx 91.81\\%\\))of normalized scores are less than or equal 0.5. This may indicate that a useful threshold value for maximizing model accuracy and or bank profit per borrower (as informed by the model) might be close to \\(t = 0.5\\)."
  },
  {
    "objectID": "posts/post_2/index.html#choosing-a-threshold",
    "href": "posts/post_2/index.html#choosing-a-threshold",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Choosing a Threshold",
    "text": "Choosing a Threshold\n\n\nCode\n# Threshold selection based on model accuracy\nbest_accuracy = 0\nbest_threshold = 0\n\n# Following code is adapted from Prof. Chodrow\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\nfor t in np.linspace(0, 1, 100): \n    y_pred = scores &gt;= t\n    acc = (y_pred == y_train).mean()\n    ax.scatter(t, acc, color = \"purple\", s = 12.5)\n    if acc &gt; best_accuracy: \n        best_accuracy = acc\n        best_threshold = t\nax.axvline(best_threshold, linestyle = \"--\", color = \"darkorange\", linewidth = 2.5, zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Model Accuracy\", title = \"Accuracy by Threshold\")\nax.text(0.75, 0.5, f\"Best Accuracy: {best_accuracy: .3f}\\n  At Threshold $t = ${best_threshold: .3f}\", ha = \"center\",\n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"grey\", boxstyle = \"round,pad=0.3\"))\nax.grid(False)\n\n\n\n\n\n\n\n\n\nFigure 5\nShown above is the trend in overall model accuracy as the threshold \\(t\\) increases from \\(0: 1\\). As outlines in the plot, the peak model accuracy (\\(\\approx 85\\%\\)) is achieved using a threshold of \\(t = 0.505\\). However, what is likely more interesting (especially for the bank) is how the profit-per-borrower can be maximized through selecting a value for \\(t\\). Below is an examination of thresholding for profit maximization.\n\n\nCode\n# Threshold selection based on profit for the bank created by the model\n\n# Updated variables\nbest_ppb = 0\nbest_threshold = 0\n\n# Following code is adapted from Prof. Chodrow\nfig, ax = plt.subplots(1, 1, figsize = (7.5, 5))\n\nfor t in np.linspace(0, 1, 100): \n    y_pred = scores &gt;= t\n    \n    # Determining whether each prediction is a a true or false negative to inform the amount of profit loss or gain for a given loan\n    X_train[\"TN\"] = ((y_pred == 0) & (y_train == 0)).astype(int)\n    X_train[\"FN\"] = ((y_pred == 0) & (y_train == 1)).astype(int)\n    \n    # Calculating the profit gain and loss for each borrower using the formulas provided by Prof. Chodrow\n    profit_gain = X_train[\"TN\"] * (X_train[\"loan_amnt\"] * ((1 + (0.25 * (X_train[\"loan_int_rate\"] / 100))) ** 10) - X_train[\"loan_amnt\"])\n    profit_loss = X_train[\"FN\"] * (X_train[\"loan_amnt\"] * ((1 + (0.25 * (X_train[\"loan_int_rate\"] / 100))) ** 3) - (1.7 * X_train[\"loan_amnt\"]))\n    \n    # Profit-per-borrower with the current threshold\n    ppb = (profit_gain + profit_loss).sum() / X_train.shape[0]\n    \n    # Ploting PPB point and updating variables\n    ax.scatter(t, ppb, color = \"purple\", s = 12.5)\n    if ppb &gt; best_ppb: \n        best_ppb = ppb\n        best_threshold = t\n\n# Plot styling\nax.axvline(best_threshold, linestyle = \"--\", color = \"darkorange\", linewidth = 2.5, zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Bank Profit (Per Borrower)\", title = \"Profit Per Borrower by Threshold\")\nax.text(0.75, 500, f\"Best Profit: ${best_ppb: .2f}\\n  At Threshold $t = ${best_threshold: .3f}\", ha = \"center\",\n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"grey\", boxstyle = \"round,pad=0.3\"))\nax.grid(False)\n\n\n\n\n\n\n\n\n\nIn the code above, the bank’s profit-per-borrower is plotted against the threshold \\(t\\). The profit per borrower is determined by calculating the profit gain and profit loss (using the formulas provided by Prof. Chordow) for each individual loan. The correct profit value (either gain or loss) is then added to the overall profit sum depending on whether or not the model correctly classified a given loan as a true negative or incorrectly classified a given loan as a false negative. The overall profit is then divided by the number of observations (borrowers) in the training data to yield the profit value per borrower.\nFigure 6\nAs depicted above, the bank’s profit-per-borrower is shown to increase as the threshold \\(t\\) increases before notably dropping and ultimately plateauing. The bank’s peak profit-per-borrower is approximately \\(\\$1450.00\\), corresponds to using a threshold value of \\(t = 0.495\\). Not surprisingly, both the model’s overall accuracy as well as the bank’s profit-per-borrower are maximized at very similar thresholds (about 0.5)."
  },
  {
    "objectID": "posts/post_2/index.html#perspective-of-the-bank",
    "href": "posts/post_2/index.html#perspective-of-the-bank",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Perspective of the Bank",
    "text": "Perspective of the Bank\n\n\nCode\n# Reading in and modifying the test data\ntest = pd.read_csv(\"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\")\ntest = test.dropna()\nXt = test.drop([\"loan_status\", \"loan_grade\"], axis = 1, errors = \"ignore\")\nyt = test[\"loan_status\"]\n\n# Creating one-hot encodings for qualitative variables\nXt = pd.get_dummies(Xt, drop_first = True)\n\n# Removing NAs from data while maintaining alignment\nclean = pd.concat([Xt, yt], axis = 1)\nclean = clean.dropna()\nX_test = clean.drop(\"loan_status\", axis = 1, errors = \"ignore\").copy()\ny_test = clean[\"loan_status\"].copy()\n\n# Computing and normalizing the test scores\ntest_scores = lin_score(X_test[best_vars], w)\n# Normalizing the scores\ntest_scores = (test_scores - scores.min()) / (test_scores.max() - test_scores.min())\n\n# Adding the model's calculated score for each observation in the testing data\ntest[\"score\"] = test_scores\ntest[\"score\"] = pd.to_numeric(test[\"score\"], errors=\"coerce\")\n\n# Determining predictions\ny_test_pred = test_scores &gt;= best_threshold\n\n# Calculating the profit-per-borrower for test data (copied from code chunk above)\n# Determining whether each prediction is a a true or false negative to inform the amount of profit loss or gain for a given loan\nX_test[\"TN\"] = ((y_test_pred == 0) & (y_test == 0)).astype(int)\nX_test[\"FN\"] = ((y_test_pred == 0) & (y_test == 1)).astype(int)\n\n# Calculating the profit gain and loss for each borrower using the foromulas provided by Prof. Chodrow\nprofit_gain_t = X_test[\"TN\"] * (X_test[\"loan_amnt\"] * ((1 + (0.25 * (X_test[\"loan_int_rate\"] / 100))) ** 10) - X_test[\"loan_amnt\"])\nprofit_loss_t = X_test[\"FN\"] * (X_test[\"loan_amnt\"] * ((1 + (0.25 * (X_test[\"loan_int_rate\"] / 100))) ** 3) - (1.7 * X_test[\"loan_amnt\"]))\n\n# Profit-per-borrower for test data using the best threshold\nppbt = (profit_gain_t + profit_loss_t).sum() / X_test.shape[0]\n\nprint(f\"The Bank's Profit-Per-Borrower from the test data: ${ppbt :.2f}\")\nprint(f\"(Test Data PPB to Training Data PPB Ratio: {((ppbt / best_ppb) * 100): .2f}%)\")\n\n\nThe Bank's Profit-Per-Borrower from the test data: $1326.67\n(Test Data PPB to Training Data PPB Ratio:  91.64%)\n\n\nIn the code above, the test data is processed the exact same way as the training data to ensure compatibility with the scoring function and profit-per-borrower calculations as previously done.\nAccording to the computation above, the bank’s expected profit-per-borrower on the test data is approximately \\(\\$1330.00\\). This value is roughly \\(92\\%\\) of (i.e. considerably similar to) the profit-per-borrower produced by the training data. Consider the most popular loan intent from the test data: Educational loans. According to the statistics provided by the Education Data Initiative, there were nearly 43 million student loan borrowers in the US in 2024. Additionally, MX Technologies reports there being about 4550 banks in the US. If the number of US educational loan borrowers is divided amongst the number of US banks, each US bank could have approximately 9450 educational loan borrowers in 2024. So suppose the hypothetical bank in this study has about 9450 educational loan borrowers. Thus, the bank’s profit (theoretically in 2024) from educational loans alone would be about $12.57 million. This is of course a wildly crude calculation that is ultimately not based on any reviewed methodology. But, it at least goes to show how large the hypothetical bank’s annual profit theoretically could be, even for only one type of loan."
  },
  {
    "objectID": "posts/post_2/index.html#perspective-of-the-borrower",
    "href": "posts/post_2/index.html#perspective-of-the-borrower",
    "title": "Post 2 - Exploring Automated Decision Models",
    "section": "Perspective of the Borrower",
    "text": "Perspective of the Borrower\n\nLoan Approval and Borrower Age\n\n\nCode\n# Function to convert numerical ages to age categories\ndef age_cat(age):\n    if (age &lt; 30):\n        return \"Young Adult\"\n    elif (age &lt; 60):\n        return \"Adult\"\n    else:\n        return \"Senior\"\n\ntest[\"age_cat\"] = test[\"person_age\"].apply(age_cat)\n\n# Independent and dependent variables for bar chart\nage_cats = np.array(test[\"age_cat\"].unique())\nprint(age_cats)\n\n# Avg. scores calculated using .groupby(...).aggregate(...)\nac_avg_scores = np.array([\n    test[test[\"age_cat\"] == \"Young Adult\"][\"score\"].mean(),\n    test[test[\"age_cat\"] == \"Adult\"][\"score\"].mean(),\n    test[test[\"age_cat\"] == \"Senior\"][\"score\"].mean(),\n    ])\nage_scores_df = pd.DataFrame({\"Age Cat\": age_cats, \"Avg Score\": ac_avg_scores})\n\n# Arrays of percentage of high scores and percentage of all scores above the threshold from each age cat\nnum_high_scores = (test[\"score\"] &gt;= best_threshold).sum()\n\n# Perc. of high scores among each age cat\nac_perc_high_scores_1 = np.array([(test[test[\"age_cat\"] == \"Young Adult\"][\"score\"] &gt;= best_threshold).mean() * 100, \n                           (test[test[\"age_cat\"] == \"Adult\"][\"score\"] &gt;= best_threshold).mean() * 100, \n                           (test[test[\"age_cat\"] == \"Senior\"][\"score\"] &gt;= best_threshold).mean() * 100])\n\n# Perc. of all high scores contributed by each age cat\nac_perc_high_scores_2 = np.array([(test[test[\"age_cat\"] == \"Young Adult\"][\"score\"] &gt;= best_threshold).sum(), \n                           (test[test[\"age_cat\"] == \"Adult\"][\"score\"] &gt;= best_threshold).sum(), \n                           (test[test[\"age_cat\"] == \"Senior\"][\"score\"] &gt;= best_threshold).sum()])\n\n# Displaying average model score for each age cat\nfig, ax = plt.subplots(2, 2, figsize = (15, 12.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Distribution of scores by age cat\nsns.boxenplot(test, x = \"age_cat\", y = \"score\", hue = \"age_cat\", legend = False, ax = ax[0, 0])\nax[0, 0].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0, 0].set_title(\"Distribution of Model Scores\", fontsize = 18)\nax[0, 0].set_ylabel(\"Score (Normalized)\", fontsize = 16)\nax[0, 0].set_xticks([0, 1, 2])\nax[0, 0].set_xticklabels([\"Young Adult\\n(Early-Career)\", \"Adult\\n(Mid-Career)\", \"Senior\\n(Late-Career/Retired)\"])\nax[0, 0].set_xlabel(\"\")\nax[0, 0].text(1.75, 0.85, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0, 0].text(1.5, 0.85, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Average score by age cat\nsns.barplot(x = age_cats, y = ac_avg_scores, data = age_scores_df, hue = age_cats, legend = False, ax = ax[0, 1])\nax[0, 1].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0, 1].set_title(\"Average Model Score\", fontsize = 18)\nax[0, 1].set_ylabel(\"Average Score (Normalized)\", fontsize = 16)\nax[0, 1].set_xticks([0, 1, 2])\nax[0, 1].set_xticklabels([\"Young Adult\\n(Early-Career)\", \"Adult\\n(Mid-Career)\", \"Senior\\n(Late-Career/Retired)\"])\nax[0, 1].text(1.75, 0.425, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0, 1].text(1.5, 0.425, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Percentage of high scores (above threshold) for each age cat\nsns.barplot(x = age_cats, y = ac_perc_high_scores_1, hue = age_cats, legend = False, ax = ax[1, 0])\nax[1, 0].set_title(\"Percentage of Model Scores\\nAbove Threshold \", fontsize = 18)\nax[1, 0].set_ylabel(\"% of Scores Above Threshold\", fontsize = 16)\nax[1, 0].set_xticks([0, 1, 2])\nax[1, 0].set_xticklabels([\"Young Adult\\n(Early-Career)\", \"Adult\\n(Mid-Career)\", \"Senior\\n(Late-Career/Retired)\"])\nax[1, 0].set_yticks([0, 2, 4, 6, 8, 10, 12])\nax[1, 0].set_yticklabels([\"0%\", \"2%\", \"4%\", \"6%\", \"8%\", \"10%\", \"12%\"])\n\n# Percentage of all high scores (above threshold) from each age cat\nsns.barplot(x = age_cats, y = ((ac_perc_high_scores_2 / num_high_scores) * 100), hue = age_cats, legend = False, ax = ax[1, 1])\nax[1, 1].set_title(\"Percentage of all Model Scores\\nAbove Threshold From each Age Category\", fontsize = 18)\nax[1, 1].set_ylabel(\"% of all Scores Above Threshold\", fontsize = 16)\nax[1, 1].set_xticks([0, 1, 2])\nax[1, 1].set_xticklabels([\"Young Adult\\n(Early-Career)\", \"Adult\\n(Mid-Career)\", \"Senior\\n(Late-Career/Retired)\"])\nax[1, 1].set_yticks([0, 10, 20, 30, 40, 50, 60, 70, 80])\nax[1, 1].set_yticklabels([\"0%\", \"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\", \"80%\"])\n\nfig.text(0.5, 0.95, \"Model Score Metrics by Borrower Age category\", ha = \"center\", fontsize = 20)\nfig.text(0.5, 0.05, \"Borrower Age Category\", ha = \"center\", fontsize = 18)\nplt.subplots_adjust(wspace = 0.25, hspace = 0.3)\n\n\n['Young Adult' 'Adult' 'Senior']\n\n\n\n\n\n\n\n\n\nCode above constructs 4 plots to observe how the model score varies across age category among borrowers. The top two plots simply display the distribution of model scores and the average score for each age category. The bottom two plots show the percentage of high model scores within each age category and the percentage of overall high model scores attributed to each age category (computed by taking the number of high model scores for each age category over the total number of high model scores).\nFigure 7\nAs displayed in the plots above, it appears that young adults and middle-aged borrowers receive lower scores than senior borrowers. Additionally, a smaller percentage of scores for both young adult and middle-aged borrowers are above the threshold than that of senior borrowers. Considering this, it is reasonable to hypothesize that the model makes it more difficult for older borrowers to be approved for loans than it is for younger borrowers. However, this is possibly a result of inconsistent age category sizes. That is, the percentage of all high scores (scores that advise the bank to deny a borrower a loan) attributed to young adults is much greater than that of middle-aged and senior borrowers. It is possible that the metrics found in this data are not truly representative of the population of senior borrowers.\n\n\nLoan Approval and Loan Intent\n\n\nCode\n# Independent and dependent variables for bar chart\nloan_intents = np.array(test[\"loan_intent\"].unique())\n\n# Avg. scores calculated using .groupby(...).aggregate(...)\nli_avg_scores = np.array([\n    test[test[\"loan_intent\"] == \"VENTURE\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"DEBTCONSOLIDATION\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"MEDICAL\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"HOMEIMPROVEMENT\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"EDUCATION\"][\"score\"].mean(),\n    test[test[\"loan_intent\"] == \"PERSONAL\"][\"score\"].mean()\n])\nli_scores_df = pd.DataFrame({\"Loan Intent\": loan_intents, \"Avg Score\": li_avg_scores})\n\n# Arrays of percentage of high scores and percentage of all scores above the threshold from each loan intent\nli_perc_high_scores_1 = np.zeros(0)\nli_perc_high_scores_2 = np.zeros(0)\n\nfor loan in loan_intents:\n    \n    # Calculating each metric\n    li_perc_high_score_1 = (test[test[\"loan_intent\"] == loan][\"score\"] &gt;= best_threshold).mean() * 100\n    li_perc_high_score_2 = ((test[test[\"loan_intent\"] == loan][\"score\"] &gt;= best_threshold).sum() / num_high_scores) * 100\n\n    # Adding each metric to corresponding array\n    li_perc_high_scores_1 = np.append(li_perc_high_scores_1, li_perc_high_score_1)\n    li_perc_high_scores_2 = np.append(li_perc_high_scores_2, li_perc_high_score_2)\n\n# Displaying average model score for each age cat\nfig, ax = plt.subplots(2, 2, figsize = (15, 12.5))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Distribution of scores by age cat\nsns.boxenplot(test, x = \"loan_intent\", y = \"score\", hue = \"loan_intent\", legend = False, ax = ax[0, 0])\nax[0, 0].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0, 0].set_title(\"Distribution of Model Scores\", fontsize = 18)\nax[0, 0].set_ylabel(\"Score (Normalized)\", fontsize = 16)\nax[0, 0].set_xticks([0, 1, 2, 3, 4 ,5])\nax[0, 0].set_xticklabels(loan_intents, rotation = 20)\nax[0, 0].set_xlabel(\"\")\nax[0, 0].text(4, 0.9, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0, 0].text(3.5, 0.9, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Average score by age cat\nsns.barplot(x = loan_intents, y = li_avg_scores, data = li_scores_df, hue = loan_intents, legend = False, ax = ax[0, 1])\nax[0, 1].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0, 1].set_title(\"Average Model Score\", fontsize = 18)\nax[0, 1].set_ylabel(\"Average Score (Normalized)\", fontsize = 16)\nax[0, 1].set_xticks([0, 1, 2, 3, 4, 5])\nax[0, 1].set_xticklabels(loan_intents, rotation = 20)\nax[0, 1].text(4, 0.45, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0, 1].text(3.5, 0.45, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Percentage of high scores (above threshold) for each age cat\nsns.barplot(x = loan_intents, y = li_perc_high_scores_1, hue = loan_intents, legend = False, ax = ax[1, 0])\nax[1, 0].set_title(\"Percentage of Model Scores\\nAbove Threshold \", fontsize = 18)\nax[1, 0].set_ylabel(\"% of Scores Above Threshold\", fontsize = 16)\nax[1, 0].set_xticks([0, 1, 2, 3, 5, 6])\nax[1, 0].set_xticklabels(loan_intents, rotation = 20)\nax[1, 0].set_yticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\nax[1, 0].set_yticklabels([\"0%\", \"1%\", \"2%\", \"3%\", \"4%\", \"5%\", \"6%\", \"7%\", \"8%\"])\n\n# Percentage of all high scores (above threshold) from each age cat\nsns.barplot(x = loan_intents, y = li_perc_high_scores_2, hue = loan_intents, legend = False, ax = ax[1, 1])\nax[1, 1].set_title(\"Percentage of all Model Scores\\nAbove Threshold From each Age Category\", fontsize = 18)\nax[1, 1].set_ylabel(\"% of all Scores Above Threshold\", fontsize = 16)\nax[1, 1].set_xticks([0, 1, 2, 3, 4, 5])\nax[1, 1].set_xticklabels(loan_intents, rotation = 20)\nax[1, 1].set_yticks([0, 5, 10, 15, 20])\nax[1, 1].set_yticklabels([\"0%\", \"5%\", \"10%\", \"15%\", \"20%\"])\n\nfig.text(0.5, 0.95, \"Model Score Metrics by Loan Intent\", ha = \"center\", fontsize = 20)\nfig.text(0.5, 0.05, \"Loan Intent\", ha = \"center\", fontsize = 18)\nplt.subplots_adjust(wspace = 0.25, hspace = 0.35)\n\n\n# Computing true defualt rates for medical, venture, and educational loans\nm = test[test[\"loan_intent\"] == \"MEDICAL\"].copy()\nv = test[test[\"loan_intent\"] == \"VENTURE\"].copy()\ne = test[test[\"loan_intent\"] == \"EDUCATION\"].copy()\nd = test[test[\"loan_intent\"] == \"DEBTCONSOLIDATION\"].copy()\np = test[test[\"loan_intent\"] == \"PERSONAL\"].copy()\nh = test[test[\"loan_intent\"] == \"HOMEIMPROVEMENT\"].copy()\ntrue_default_rates = np.array([\n    (m[\"loan_status\"] == 1).mean(),\n    (v[\"loan_status\"] == 1).mean(),\n    (e[\"loan_status\"] == 1).mean(),\n    (d[\"loan_status\"] == 1).mean(),\n    (p[\"loan_status\"] == 1).mean(),\n    (h[\"loan_status\"] == 1).mean(),\n])\n\ntrue_default_rates\n\n\narray([0.28424977, 0.14626556, 0.16751701, 0.28761062, 0.22044088,\n       0.25      ])\n\n\n\n\n\n\n\n\n\nCode above constructs 4 plots to observe how the model score varies across loan intent. The top two plots simply display the distribution of model scores and the average score for each loan intent. The bottom two plots show the percentage of high model scores within each loan intent and the percentage of overall high model scores attributed to each loan intent (computed by taking the number of high model scores for each loan intent over the total number of high model scores).\nFigure 8\nAs shown in the figures above, the the general distribution and average model scores across each loan intent are considerable similar. That is, it does not appear that any specific loan intents correspond to significantly high or low model scores. However, as shown in the bottom right plot, the largest percentage of all high model scores (scores that advise the bank to deny a borrower a loan), is attributed to medical loans. This may suggest that it is more difficult for borrowers to have medical loans approved. The actual rate of default for medical loans in the test data is about \\(30\\%\\), which is the second highest default rate (nearly equal to that of debt consolidation loans). Considering that medical loans make up the largest percentage of overall high model scores and have the highest true rate of default in the test data, it may be justified that the bank makes it more difficult for borrowers seeking these types of loans to be approved.\nAs for venture and educational loans, it appears that borrowers looking to take out these types of loans are similarly likely to be approved by the bank (they have very similar average model scores, percentage of high scores within themselves, and percentage makeups of the overall high model scores). Additionally, both venture loans and educational loans have similar default rates in the test data of about \\(15\\%\\) and \\(17\\%\\) respectively.\n\n\nLoan Approval and Borrower Income\n\n\nCode\n# Adding high/low score indicator col\ntest[\"high_score\"] = test[\"score\"] &gt;= best_threshold\ntest[\"low_inc\"] = (test[\"person_income\"] &lt;= 30000).astype(int)\ntest[\"high_inc\"] = (test[\"person_income\"] &gt;= 150000).astype(int)\ntest[\"person_inc_log\"] = np.log2(test[\"person_income\"])\n\nlow_inc = test[test[\"low_inc\"] == 1].copy()\nhigh_inc = test[test[\"high_inc\"] == 1].copy()\n\n# Plotting model score against borrower income\nfig, ax = plt.subplots(1, 3, figsize = (17.5, 10))\n\n# Score by income for low income borrowers\nsns.scatterplot(data = low_inc, x = \"person_income\", y = \"score\", hue = \"high_score\", palette = [\"darkorange\", \"purple\"], legend = False, ax = ax[0])\nax[0].set_title(\"Low Income\", fontsize = 16)\nax[0].set_ylabel(\"Score (Normalized)\", fontsize = 14)\nax[0].set_xlabel(\"\")\nax[0].set_xticks([5e3, 10e3, 15e3, 20e3, 25e3, 30e3])\nax[0].set_xticklabels([\"~$5000\", \"~$10000\", \"~$15000\", \"~$20000\", \"~$25000\", \"~$30000\"], rotation = 15)\nax[0].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[0].text(8000, 0.9, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[0].text(5000, 0.9, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Score by income for high income borrowers\nsns.scatterplot(data = high_inc, x = \"person_income\", y = \"score\", hue = \"high_score\", palette = [\"darkorange\", \"purple\"], legend = False, ax = ax[1])\nax[1].set_xscale(\"log\", base = 2)\nax[1].set_title(\"High Income\", fontsize = 16)\nax[1].set_ylabel(\"\")\nax[1].set_xlabel(\"\")\nax[1].set_xticks([2 ** 18, 2 ** 19, 2 ** 20])\nax[1].set_xticklabels([\"~$260000\", \"~$500000\", \"~$1M\"], rotation = 15)\nax[1].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[1].text(2 ** 17.75, 0.45, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[1].text(2 ** 17.325, 0.45, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\n# Score by income for all borrowers\nsns.scatterplot(data = test, x = \"person_income\", y = \"score\", hue = \"high_score\", palette = [\"darkorange\", \"purple\"], legend = False, ax = ax[2])\nax[2].set_xscale(\"log\", base = 2)\nax[2].set_title(\"All Borrowers\", fontsize = 16)\nax[2].set_ylabel(\"\")\nax[2].set_xlabel(\"\")\nax[2].set_xticks([2 ** 13, 2 ** 15, 2 ** 17, 2 ** 19, 2 ** 21])\nax[2].set_xticklabels([\"~$8000\", \"~$30000\", \"~$100000\", \"~$500000\", \"~$2M\"], rotation = 15)\nax[2].axhline(best_threshold, linestyle = \"--\", color = \"purple\", linewidth = 2.5)\nax[2].text(2 ** 18, 0.9, f\"     $t = 0.495$\", ha = \"center\", va = \"center\", fontsize = 10, \n        bbox = dict(facecolor = \"white\", alpha = 0.5, edgecolor = \"gray\", boxstyle = \"round,pad=0.3\"))\nax[2].text(2 ** 17, 0.9, \"\\u2013\", color = \"purple\", ha = \"left\", va = \"center\", fontsize = 15, fontweight=\"bold\")\n\nfig.suptitle(\"Observing Model Score Against Borrower Income\", fontsize = 20)\nfig.text(0.5, 0.01, \"Borrower Income ($\\log_2$ scale on rightmost subplots)\", ha = \"center\", fontsize = 16)\n\nplt.show()\n\n# Calculating default rates for low and high income borrowers\ntrue_default_rates_inc = np.array([\n    (low_inc[\"loan_status\"] == 1).mean(),\n    (high_inc[\"loan_status\"] == 1).mean()\n])\n\n\n\n\n\n\n\n\n\nCode above constructs scatter plots observing model score by borrower income for low-income borrowers, high-income borrowers, and all borrowers. The threshold is included in each plot to depict where the score cutoff is.\nFigure 9\nAs depicted in the plots above, there seems to be a general trend between borrower income and their corresponding model score. That is, it appears that as a borrower’s income increases, their score decreases. This may suggest that the model makes it easier for higher-income individuals to be approved for loans than this at lower-income levels. This is supported by the fact that a considerable proportion of low-income borrowers (\\(\\leq \\$30000\\) annual earnings) were scored above the threshold (i.e. likely not approved) while none of the high-income borrowers (\\(\\geq \\$150000\\) annual earnings) were scored above the threshold. This may be sufficient evidence for the bank, but it is important to note that a borrower’s income is not actually one of the features the model was trained on. Thus, this observation may be a due to other related factors."
  },
  {
    "objectID": "posts/post_1/index.html",
    "href": "posts/post_1/index.html",
    "title": "Post 1 - Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post, I’m taking on an introductory-level machine learning classification task. This task, classifying penguin species, is a ternary classification problem as there are three species options for which I am trying to construct a classifier model to correctly identify. The data for this task comes from the Palmer Station and was collected by Dr. Kristen Gorman. The following analysis begins with some preliminary data visualizations and summary statistics interpretation. From the preliminary visualizations and statistics table, I’m able to make some initial observations about the difference in penguin features across each species. The next sections show the initial model and feature selection as well as model refinement to improve the chosen model. While completing this analysis, I experimented with several different models: a logistic regression model, a decision tree classifier, and a random forest classifier. I ultimately decided to choose proceed with the random forest classifier model (see the third section in this post). For feature selection, I initially took an exhaustive search approach. However, after running into many bugs and difficulties with this approach, I opted to incorporate the use of prebuilt feature selection tools (see second section in this post). Following a thorough feature selection and cross-validation/model refinement process, I was able to produce a model with nearly \\(100\\%\\) testing accuracy. That is, my model was able to correctly identify all but a single observation from unseen test data (i.e. it made 1 classification error). In the following sections of this post are more in-depth explanations and analyses of my steps taken in accomplishing this classification task.\n\n\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# Accessing training data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# Code below provided by Prof. Chodrow\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nThe prepare_data(...) function above was provided by Prof. Chodrow\n\n\nCode\n# Includuing all additional imports\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\nimport seaborn as sns\n\n\nIncluding all additional imports\n\n\nCode\n# Creating a modified dataset for visualization and summary statistics\ntrain_viz = train[train[\"Sex\"] != \".\"].copy()\ntrain_viz[\"Culmen Ratio (L/D)\"] = train_viz[\"Culmen Length (mm)\"] / train_viz[\"Culmen Depth (mm)\"]\ntrain_viz.dropna()\n\n# Subsetting data by species to make regression plots for flipper length by body mass across each Species\nadelie = train_viz[['Species', 'Flipper Length (mm)', 'Body Mass (g)']]\nadelie = adelie[adelie['Species'] == 'Adelie Penguin (Pygoscelis adeliae)']\nadelie = adelie.dropna(subset = ['Species', 'Flipper Length (mm)', 'Body Mass (g)'])\nchinstrap = train_viz[['Species', 'Flipper Length (mm)', 'Body Mass (g)']]\nchinstrap = chinstrap[chinstrap['Species'] == 'Chinstrap penguin (Pygoscelis antarctica)']\nchinstrap = chinstrap.dropna(subset = ['Species', 'Flipper Length (mm)', 'Body Mass (g)'])\ngentoo = train_viz[['Species', 'Flipper Length (mm)', 'Body Mass (g)']]\ngentoo = gentoo[gentoo['Species'] == 'Gentoo penguin (Pygoscelis papua)']\ngentoo = gentoo.dropna(subset = ['Species', 'Flipper Length (mm)', 'Body Mass (g)'])\n\n\nAbove, I’m creating a modified dataset for visualization and summary statistics and making three subsets of the visualization data set corresponding to eac penguin species with the specific visualization features selected. This is later used for linear regression modeling by species.\n\n\n\n\n\nCode\n# Creating linear regression models and calculating r^2 values\n\n## Adelie\nc1 = np.polyfit(adelie['Body Mass (g)'], adelie['Flipper Length (mm)'], 1)\np1 = np.polyval(c1, adelie['Body Mass (g)'])\nr1 = adelie['Flipper Length (mm)'] - p1\nssr1 = np.sum(r1**2)\nsst1 = np.sum((adelie['Flipper Length (mm)'] - np.mean(adelie['Flipper Length (mm)']))**2)\nrs1 = 1 - (ssr1 / sst1)\n\n## Chinstrap\nc2 = np.polyfit(chinstrap['Body Mass (g)'], chinstrap['Flipper Length (mm)'], 1)\np2 = np.polyval(c2, chinstrap['Body Mass (g)'])\nr2 = chinstrap['Flipper Length (mm)'] - p2\nssr2 = np.sum(r2**2)\nsst2 = np.sum((chinstrap['Flipper Length (mm)'] - np.mean(chinstrap['Flipper Length (mm)']))**2)\nrs2 = 1 - (ssr2 / sst2)\n\n## Gentoo\nc3 = np.polyfit(gentoo['Body Mass (g)'], gentoo['Flipper Length (mm)'], 1)\np3 = np.polyval(c3, gentoo['Body Mass (g)'])\nr3 = gentoo['Flipper Length (mm)'] - p3\nssr3 = np.sum(r3**2)\nsst3 = np.sum((gentoo['Flipper Length (mm)'] - np.mean(gentoo['Flipper Length (mm)']))**2)\nrs3 = 1 - (ssr3 / sst3)\n\n\nAbove, I’m creating a linear regression model of flipper length by body mass for each species (using np.polyfit). I’m also extracting the \\(R^2\\) values (using predictions from np.polyval to calculate residuals) for each regression model to include in my plots below. I searched online how to use these two functions.\n\n\nCode\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize = (10, 7))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Setting up the initial scatter plot\nax[0] = sns.scatterplot(data = train_viz, x = 'Body Mass (g)', y = 'Flipper Length (mm)', hue = 'Species', palette = ['#B85ED4', '#2A7A7A', '#F28234'], style = 'Species', s = 50, ax = ax[0])\n\n# Adding regression lines for each species\nsns.regplot(data = adelie, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#F28234'}, ax = ax[0])\nsns.regplot(data = chinstrap, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#B85ED4'}, ax = ax[0])\nsns.regplot(data = gentoo, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#2A7A7A'}, ax = ax[0])\n\n# Plot styling to make colors match up and the text boxes look nicer\nax[0].set_title(\"Penguin Flipper Length (mm) by Body Mass (g)\\nColored by Species\")\nax[0].legend(frameon = True, prop = {'size': 9})\nax[0].text(5200, 175.75, '           \\n', fontsize = 20, bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'grey', boxstyle = 'round, pad=0.3'))\nax[0].text(5555, 180, f'     $R^2 = {rs3:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5555, 178, f'     $R^2 = {rs2:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5555, 176, f'     $R^2 = {rs1:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5200, 180.1, '\\u2013', color='#2A7A7A', ha='left', va='center', fontsize=15, fontweight='bold')\nax[0].text(5200, 178.1, '\\u2013', color='#B85ED4', ha='left', va='center', fontsize=15, fontweight='bold')\nax[0].text(5200, 176.1, '\\u2013', color='#F28234', ha='left', va='center', fontsize=15, fontweight='bold')\n\n# Setting up the box plot and doing some simple styling\nax[1] = sns.boxplot(train_viz, x = \"Species\", y = \"Culmen Ratio (L/D)\", hue = \"Sex\", ax = ax[1])\nax[1].set_xticks([0, 1, 2])\nax[1].set_xticklabels([\"Chinstrap\", \"Gentoo\", \"Adelie\"])\nax[1].legend(prop = {'size': 10}, frameon = True)\nax[1].set_title(\"Penguin Culmen Ratio (L/D) by Species\\nGrouped by Sex\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.2)\n\n\n\n\n\n\n\n\n\nIn the figure to the left, the relationship between penguin flipper length (mm) and body mass (g) for each species of penguin is displayed. Visually, there appears to be a notable positive, linear relationship between flipper length and body mass in general. That is, it seems that as penguin body mass increases, the flipper length increases as well regardless of species.\nAdditionally, there appears to be some clustering by species shown in this relationship. Gentoo penguins appear to have both the largest flipper lengths and body masses. There is less of a visually obvious distinction between Chinstrap and Adelie penguins shown by the relationship between body mass and flipper length. However, I believe it is reasonable to hypothesize that Chinstrap penguins have a slightly greater average flipper length while Adelie penguins have a slightly greater body mass.\nFrom the \\(R^2\\) values for the regression lines corresponding to the relationship between body mass and flipper length for each penguin species, I feel that it’s reasonable to say the positive linear relationships between the two variables in question for each species range from weak/moderate to moderate in strength. Further, while I do not have the statistical knowledge to confirm nor deny this hypothesis, perhaps the clear visual difference observed across the three species-specific regression lines could provide useful insight into this classification task.\nIn general, this plot reveals some useful information about the differences observed across each penguin species from two relevant variables. While this plot likely does not provide highly convincing information pertaining to the species classification of penguins from certain features, it does highlight a relationship between two key quantitative variables that displays a preliminary approach to this classification task.\nThe figure to the right displays the general distribution of the Culmen Ratio (L/D) feature across penguin sex and penguin species. The Culmen Ratio (L/D) is the ratio of culmen length (mm) to culmen depth (mm). With no prior knowledge about culmen size and dimensions in penguins, I thought it would be interesting to compute the length-depth ratio for each penguin and see how this ratio differed by sex and across each species. Observing the comparison of the culmen ratio between male and female penguins, there does not appear to be as much visually obvious information. However, to me, there appears to be minimal distribution overlap in the culmen ratio across each species. I believe this suggests that the culmen ratio or related features (culmen length and culmen depth) could be useful in model construction to successfully complete this classification task.\n\n\n\n\nCode\n# Summary Statistics\n\n# Helper method to calculate coefficient of variation (%)\ndef cv(col):\n    return (col.std() / col.mean()) * 100\n\n# Creating a table grouped by penguin species and sex, showing general summary stats for several quantitative variables\nsum_stats = train_viz.groupby(['Species', 'Sex']).aggregate({\"Flipper Length (mm)\" : [\"mean\", \"std\", cv], \n                                                             \"Body Mass (g)\" : [\"mean\", \"std\", cv], \"Culmen Length (mm)\": [\"mean\", \"std\", cv], \n                                                             \"Culmen Depth (mm)\": [\"mean\", \"std\", cv], \"Culmen Ratio (L/D)\": [\"mean\", \"std\", cv]})\nsum_stats = sum_stats.rename(columns = {'mean': 'Mean', 'std': 'STD', 'cv': 'CV (%)'})\nsum_stats = sum_stats.round(2)\nsum_stats\n\n\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nCulmen Ratio (L/D)\n\n\n\n\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n187.92\n5.43\n2.89\n3350.47\n262.87\n7.85\n37.43\n1.95\n5.20\n17.64\n0.92\n5.21\n2.13\n0.15\n6.96\n\n\nMALE\n192.33\n7.00\n3.64\n4052.87\n331.57\n8.18\n40.40\n2.37\n5.86\n19.08\n1.05\n5.48\n2.12\n0.17\n7.89\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n192.06\n5.90\n3.07\n3523.39\n294.95\n8.37\n46.72\n3.17\n6.78\n17.61\n0.80\n4.55\n2.66\n0.19\n7.21\n\n\nMALE\n200.69\n6.29\n3.13\n4005.77\n368.53\n9.20\n51.33\n1.60\n3.13\n19.27\n0.77\n3.97\n2.67\n0.10\n3.93\n\n\nGentoo penguin (Pygoscelis papua)\nFEMALE\n212.84\n3.47\n1.63\n4684.69\n297.46\n6.35\n45.46\n1.97\n4.34\n14.21\n0.54\n3.78\n3.20\n0.14\n4.45\n\n\nMALE\n221.20\n5.22\n2.36\n5476.70\n301.32\n5.50\n49.01\n2.29\n4.68\n15.73\n0.79\n5.00\n3.12\n0.18\n5.77\n\n\n\n\n\n\n\nIn creating the table above, I looked up how to calculate the CV for each of the columns to more easily interpret the STD and created the helper method\nAbove is a general statistics table for the quantitative features present in the penguins data. Some noteworthy observations from this table are below.\nAdelie penguins:\n\nFemales\n\nSmallest average flipper length, body mass, culmen length, and culmen ratio\nLargest average culmen depth\n\nMales\n\nSmallest average flipper length, culmen length, and culmen ratio\n\n\nChinstrap penguins:\n\nFemales\n\nLargest average culmen length\n\nMales\n\nSmallest average body mass\nLargest average culmen length and culmen depth\n\n\nGentoo penguins:\n\nFemales\n\nLargest average flipper length, body mass, and culmen ratio\nSmallest average culmen depth\n\nMales\n\nLargest average flipper length, body mass, and culmen ratio\nSmallest average culmen depth\n\n\nRegarding the variability in the distribution of each feature represented in the table above, the coefficient of variation (as a percentage) is below 10% for all feature distributions. According to some brief research on CV, the CV values shown in the table above suggest that there is a relatively low degree of variability in the distributions of each feature."
  },
  {
    "objectID": "posts/post_1/index.html#summary-statistics-and-preliminary-visualizations",
    "href": "posts/post_1/index.html#summary-statistics-and-preliminary-visualizations",
    "title": "Post 1 - Classifying Palmer Penguins",
    "section": "",
    "text": "Code\n# Creating linear regression models and calculating r^2 values\n\n## Adelie\nc1 = np.polyfit(adelie['Body Mass (g)'], adelie['Flipper Length (mm)'], 1)\np1 = np.polyval(c1, adelie['Body Mass (g)'])\nr1 = adelie['Flipper Length (mm)'] - p1\nssr1 = np.sum(r1**2)\nsst1 = np.sum((adelie['Flipper Length (mm)'] - np.mean(adelie['Flipper Length (mm)']))**2)\nrs1 = 1 - (ssr1 / sst1)\n\n## Chinstrap\nc2 = np.polyfit(chinstrap['Body Mass (g)'], chinstrap['Flipper Length (mm)'], 1)\np2 = np.polyval(c2, chinstrap['Body Mass (g)'])\nr2 = chinstrap['Flipper Length (mm)'] - p2\nssr2 = np.sum(r2**2)\nsst2 = np.sum((chinstrap['Flipper Length (mm)'] - np.mean(chinstrap['Flipper Length (mm)']))**2)\nrs2 = 1 - (ssr2 / sst2)\n\n## Gentoo\nc3 = np.polyfit(gentoo['Body Mass (g)'], gentoo['Flipper Length (mm)'], 1)\np3 = np.polyval(c3, gentoo['Body Mass (g)'])\nr3 = gentoo['Flipper Length (mm)'] - p3\nssr3 = np.sum(r3**2)\nsst3 = np.sum((gentoo['Flipper Length (mm)'] - np.mean(gentoo['Flipper Length (mm)']))**2)\nrs3 = 1 - (ssr3 / sst3)\n\n\nAbove, I’m creating a linear regression model of flipper length by body mass for each species (using np.polyfit). I’m also extracting the \\(R^2\\) values (using predictions from np.polyval to calculate residuals) for each regression model to include in my plots below. I searched online how to use these two functions.\n\n\nCode\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize = (10, 7))\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Setting up the initial scatter plot\nax[0] = sns.scatterplot(data = train_viz, x = 'Body Mass (g)', y = 'Flipper Length (mm)', hue = 'Species', palette = ['#B85ED4', '#2A7A7A', '#F28234'], style = 'Species', s = 50, ax = ax[0])\n\n# Adding regression lines for each species\nsns.regplot(data = adelie, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#F28234'}, ax = ax[0])\nsns.regplot(data = chinstrap, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#B85ED4'}, ax = ax[0])\nsns.regplot(data = gentoo, x = 'Body Mass (g)', y = 'Flipper Length (mm)', scatter = False, line_kws={'color': '#2A7A7A'}, ax = ax[0])\n\n# Plot styling to make colors match up and the text boxes look nicer\nax[0].set_title(\"Penguin Flipper Length (mm) by Body Mass (g)\\nColored by Species\")\nax[0].legend(frameon = True, prop = {'size': 9})\nax[0].text(5200, 175.75, '           \\n', fontsize = 20, bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'grey', boxstyle = 'round, pad=0.3'))\nax[0].text(5555, 180, f'     $R^2 = {rs3:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5555, 178, f'     $R^2 = {rs2:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5555, 176, f'     $R^2 = {rs1:.3f}$', ha = 'center', va = 'center', fontsize = 9, \n        bbox = dict(facecolor = 'white', alpha = 0.5, edgecolor = 'none', boxstyle = 'round,pad=0.3'))\nax[0].text(5200, 180.1, '\\u2013', color='#2A7A7A', ha='left', va='center', fontsize=15, fontweight='bold')\nax[0].text(5200, 178.1, '\\u2013', color='#B85ED4', ha='left', va='center', fontsize=15, fontweight='bold')\nax[0].text(5200, 176.1, '\\u2013', color='#F28234', ha='left', va='center', fontsize=15, fontweight='bold')\n\n# Setting up the box plot and doing some simple styling\nax[1] = sns.boxplot(train_viz, x = \"Species\", y = \"Culmen Ratio (L/D)\", hue = \"Sex\", ax = ax[1])\nax[1].set_xticks([0, 1, 2])\nax[1].set_xticklabels([\"Chinstrap\", \"Gentoo\", \"Adelie\"])\nax[1].legend(prop = {'size': 10}, frameon = True)\nax[1].set_title(\"Penguin Culmen Ratio (L/D) by Species\\nGrouped by Sex\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.2)\n\n\n\n\n\n\n\n\n\nIn the figure to the left, the relationship between penguin flipper length (mm) and body mass (g) for each species of penguin is displayed. Visually, there appears to be a notable positive, linear relationship between flipper length and body mass in general. That is, it seems that as penguin body mass increases, the flipper length increases as well regardless of species.\nAdditionally, there appears to be some clustering by species shown in this relationship. Gentoo penguins appear to have both the largest flipper lengths and body masses. There is less of a visually obvious distinction between Chinstrap and Adelie penguins shown by the relationship between body mass and flipper length. However, I believe it is reasonable to hypothesize that Chinstrap penguins have a slightly greater average flipper length while Adelie penguins have a slightly greater body mass.\nFrom the \\(R^2\\) values for the regression lines corresponding to the relationship between body mass and flipper length for each penguin species, I feel that it’s reasonable to say the positive linear relationships between the two variables in question for each species range from weak/moderate to moderate in strength. Further, while I do not have the statistical knowledge to confirm nor deny this hypothesis, perhaps the clear visual difference observed across the three species-specific regression lines could provide useful insight into this classification task.\nIn general, this plot reveals some useful information about the differences observed across each penguin species from two relevant variables. While this plot likely does not provide highly convincing information pertaining to the species classification of penguins from certain features, it does highlight a relationship between two key quantitative variables that displays a preliminary approach to this classification task.\nThe figure to the right displays the general distribution of the Culmen Ratio (L/D) feature across penguin sex and penguin species. The Culmen Ratio (L/D) is the ratio of culmen length (mm) to culmen depth (mm). With no prior knowledge about culmen size and dimensions in penguins, I thought it would be interesting to compute the length-depth ratio for each penguin and see how this ratio differed by sex and across each species. Observing the comparison of the culmen ratio between male and female penguins, there does not appear to be as much visually obvious information. However, to me, there appears to be minimal distribution overlap in the culmen ratio across each species. I believe this suggests that the culmen ratio or related features (culmen length and culmen depth) could be useful in model construction to successfully complete this classification task.\n\n\n\n\nCode\n# Summary Statistics\n\n# Helper method to calculate coefficient of variation (%)\ndef cv(col):\n    return (col.std() / col.mean()) * 100\n\n# Creating a table grouped by penguin species and sex, showing general summary stats for several quantitative variables\nsum_stats = train_viz.groupby(['Species', 'Sex']).aggregate({\"Flipper Length (mm)\" : [\"mean\", \"std\", cv], \n                                                             \"Body Mass (g)\" : [\"mean\", \"std\", cv], \"Culmen Length (mm)\": [\"mean\", \"std\", cv], \n                                                             \"Culmen Depth (mm)\": [\"mean\", \"std\", cv], \"Culmen Ratio (L/D)\": [\"mean\", \"std\", cv]})\nsum_stats = sum_stats.rename(columns = {'mean': 'Mean', 'std': 'STD', 'cv': 'CV (%)'})\nsum_stats = sum_stats.round(2)\nsum_stats\n\n\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nCulmen Ratio (L/D)\n\n\n\n\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\nMean\nSTD\nCV (%)\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n187.92\n5.43\n2.89\n3350.47\n262.87\n7.85\n37.43\n1.95\n5.20\n17.64\n0.92\n5.21\n2.13\n0.15\n6.96\n\n\nMALE\n192.33\n7.00\n3.64\n4052.87\n331.57\n8.18\n40.40\n2.37\n5.86\n19.08\n1.05\n5.48\n2.12\n0.17\n7.89\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n192.06\n5.90\n3.07\n3523.39\n294.95\n8.37\n46.72\n3.17\n6.78\n17.61\n0.80\n4.55\n2.66\n0.19\n7.21\n\n\nMALE\n200.69\n6.29\n3.13\n4005.77\n368.53\n9.20\n51.33\n1.60\n3.13\n19.27\n0.77\n3.97\n2.67\n0.10\n3.93\n\n\nGentoo penguin (Pygoscelis papua)\nFEMALE\n212.84\n3.47\n1.63\n4684.69\n297.46\n6.35\n45.46\n1.97\n4.34\n14.21\n0.54\n3.78\n3.20\n0.14\n4.45\n\n\nMALE\n221.20\n5.22\n2.36\n5476.70\n301.32\n5.50\n49.01\n2.29\n4.68\n15.73\n0.79\n5.00\n3.12\n0.18\n5.77\n\n\n\n\n\n\n\nIn creating the table above, I looked up how to calculate the CV for each of the columns to more easily interpret the STD and created the helper method\nAbove is a general statistics table for the quantitative features present in the penguins data. Some noteworthy observations from this table are below.\nAdelie penguins:\n\nFemales\n\nSmallest average flipper length, body mass, culmen length, and culmen ratio\nLargest average culmen depth\n\nMales\n\nSmallest average flipper length, culmen length, and culmen ratio\n\n\nChinstrap penguins:\n\nFemales\n\nLargest average culmen length\n\nMales\n\nSmallest average body mass\nLargest average culmen length and culmen depth\n\n\nGentoo penguins:\n\nFemales\n\nLargest average flipper length, body mass, and culmen ratio\nSmallest average culmen depth\n\nMales\n\nLargest average flipper length, body mass, and culmen ratio\nSmallest average culmen depth\n\n\nRegarding the variability in the distribution of each feature represented in the table above, the coefficient of variation (as a percentage) is below 10% for all feature distributions. According to some brief research on CV, the CV values shown in the table above suggest that there is a relatively low degree of variability in the distributions of each feature."
  },
  {
    "objectID": "posts/post_1/index.html#testing-the-model",
    "href": "posts/post_1/index.html#testing-the-model",
    "title": "Post 1 - Classifying Palmer Penguins",
    "section": "Testing the Model",
    "text": "Testing the Model\nAfter refining my model, I then tested it on the testing data. To my excitement, the model dispayed a \\(98.529\\%\\) testing accuracy.\n\n\nCode\n# Accessing test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Testing the model!\nX_test, y_test = prepare_data(test)\ntest_score = rfc_refined.score(X_test[X_train_selected.columns], y_test)\nts2 = rfc_base.score(X_test[X_train_selected.columns], y_test)\nprint(f'Refined (cross-validated) Model Test Accuracy: {test_score * 100: .3f}%')\n\n\nRefined (cross-validated) Model Test Accuracy:  98.529%\n\n\nBelow are the decision regions determined by the model for both the training and testing data. Visually, the decision regions for the training and testing data appear to be extremely similar. Additionally, I feel that it is reasonable to state that the decision regions for both the training and testing data do not display a high degree of model-overfitting.\n\n\nCode\n# Decision region plotting - code provided by Prof. Chodrow\ndef plot_regions(model, X, y, type):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 5))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      colors = ['#2A7A7A', '#B85ED4', '#F28234']\n      og = ['red', 'green', 'blue']\n      cmap = ListedColormap(colors)\n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = cmap, alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = cmap, vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i][7:] + \" Island\")\n      \n      patches = []\n      for color, spec in zip(['#F28234', '#B85ED4', '#2A7A7A'], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n\n      if (type):\n         fig.suptitle(\"Decision Regions for Training Data\")\n      else:\n         fig.suptitle(\"Decision Regions for Testing Data\")\n      \n      plt.tight_layout()\n\nplot_regions(rfc_refined, X_train_selected, y_train, 1)\nplot_regions(rfc_refined, X_test[X_train_selected.columns], y_test, 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI made some minor adjustments to the code above provided by Prof. Chodrow (primarily debugging, color changing, and adding titles)"
  },
  {
    "objectID": "posts/post_1/index.html#confusion-matrix-for-the-model",
    "href": "posts/post_1/index.html#confusion-matrix-for-the-model",
    "title": "Post 1 - Classifying Palmer Penguins",
    "section": "Confusion Matrix for the Model",
    "text": "Confusion Matrix for the Model\nBelow is the confusion matrix to evaluate the model’s testing performance. As expected with \\(&lt; 100\\%\\) (\\(98.529\\%\\)) testing accuracy, the confusion matrix has at least some (in this case exactly 1) non-zero entries off the diagonal. This indicates that the model made 1 misclassification. That is, all Adelie penguins and Chinstrap penguins were correctly classified, but there was one Gentoo penguin who was misclassified as an Adelie penguin (shucks).\n\n\nCode\n# Establishing model predictions for the test data\ny_test_pred = rfc_refined.predict(X_test[X_train_selected.columns])\nC = confusion_matrix(y_test, y_test_pred)\nspecies = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n\n# Creating a heatmap for better confusion matrix visualization\nplt.figure(figsize=(6, 5))\nsns.heatmap(C, annot = True, fmt = \"d\", cmap = 'Greens', cbar = False, xticklabels = species, yticklabels = species)\n\n# Setting labels and title\nplt.xlabel(\"Predicted Species Classification\")\nplt.ylabel(\"True Species Classification\")\nplt.title(\"Palmer Penguins Classification Confusion Matrix\")\n\nplt.show()\n\n# Printing confusion matrix results\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\n\n\n\n\n\n\n\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451A Blog - Spring 2025",
    "section": "",
    "text": "Post 2 - Exploring Automated Decision Models\n\n\n\n\n\nAn introductory analysis of an automated decision model in the context of credit-risk prediction\n\n\n\n\n\nFeb 28, 2025\n\n\nCol McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nPost 1 - Classifying Palmer Penguins\n\n\n\n\n\nA ternary classification of penguin species: Gentoo, Adelie, and Chinstrap\n\n\n\n\n\nFeb 17, 2025\n\n\nCol McDermott\n\n\n\n\n\n\nNo matching items"
  }
]