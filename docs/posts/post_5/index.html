<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Col McDermott">
<meta name="dcterms.date" content="2025-04-09">
<meta name="description" content="An introductory exploration of logistic regression and gradient descent.">

<title>Post 5 - Implementing Logistic Regression – Middlebury College CSCI 0451 Blog - Col McDermott</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6943a74fafdbf25ea2a644024beb669f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
      }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Middlebury College CSCI 0451 Blog - Col McDermott</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Blog Info</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Col-McDermott"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Post 5 - Implementing Logistic Regression</h1>
                  <div>
        <div class="description">
          An introductory exploration of logistic regression and gradient descent.
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Col McDermott </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 9, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>The main purpose of this brief study is to implement from scratch and investigate the procedures under the hood of logistic regression. Logistic regression is a well-known machine-learning method used primarily for binary classification. This widely-recognized ML technique involves several common ML algorithm characteristics such as a linear model, model scores, a weights vector, and the corresponding empirical risk optimization problem. This base-level analysis of logistic regression serves to develop a general understanding of many common ML binary classification architectures as well as introduce several relevant topics exercised and enhanced in more advanced ML algorithms.</p>
<p>The optimization problem in logistic regression aims to find the specific weights vector that minimizes the “loss” of the linear model and is solved using a gradient descent procedure. This introductory exploration of logistic regression involves implementing a basic linear model with a corresponding weights vector, designing a linear regression object to compute the empirical risk loss values and corresponding gradients, and a gradient descent optimizer that performs the weights vector optimization task.</p>
<p>To analyze my version of logistic regression, I have conducted the following experiments on my implementation:</p>
<ol type="1">
<li>Evaluating a standard gradient descent procedure to optimize the weights vector of the linear model to achieve high classification accuracy.<br>
</li>
<li>Evaluating a gradient descent method <em>with momentum</em> to achieve faster convergence (i.e.&nbsp;optimizing the weights vector to minimize the empirical risk) than standard gradient descent.</li>
<li>Exploring overfitting in which logistic regression is used to produce a linear model and a weights vector that performs flawlessly on training data but less well on unseen testing data.<br>
</li>
<li>Evaluating the efficacy of my logistic regression implementation on real-world, empirical data (using standard train-test-split, model fitting, and model scoring methods).</li>
</ol>
<p>For my implementation of logistic regression, check out <a href="https://github.com/Col-McDermott/Col-McDermott.github.io/blob/main/posts/post_5/logistic.py">logistic.py</a>.</p>
<section id="implementing-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="implementing-logistic-regression">Implementing Logistic Regression</h2>
<div id="cell-3" class="cell" data-execution_count="1193">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Including all additional imports</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LinearSegmentedColormap</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch <span class="im">as</span> tch</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Porting over logistic regression implementation</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> logistic <span class="im">import</span> LogisticRegression, GradientDescentOptimizer</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>tch.manual_seed(<span class="dv">100</span>) <span class="co"># For consistent data generation</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'seaborn-v0_8-whitegrid'</span>) <span class="co"># For consistent plotting</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
<p>For this introductory study, I have implemented a rudimentary version of logistic regression. This implementation involves three class definitions: <code>LinearModel</code>, <code>LogisticRegression</code>, and <code>GradientDescentOptimizer</code>.</p>
<p><strong><code>LinearModel</code></strong>:</p>
<ul>
<li><code>self.w</code>: An instance variable to store the weights vector <span class="math inline">\(\mathbf{w}\)</span> of a linear model.</li>
<li><code>score(X)</code>: A method to compute the score <span class="math inline">\(s_i\)</span> for each data point in the feature matrix <strong><span class="math inline">\(X\)</span></strong> using: <span class="math display">\[
s_i = \langle\mathbf{w}, x_i\rangle
\]</span></li>
<li><code>predict(X)</code>: A method to compute the vector of classification predictions <span class="math inline">\(\hat{y}\)</span> for each data point in the feature matrix <strong><span class="math inline">\(X\)</span></strong>: <span class="math display">\[
\hat{y}_i = \begin{cases}
1 &amp; \text{if} &amp; s_i &gt; 0.5\\
0 &amp; \text{else}
\end{cases}
\]</span></li>
</ul>
<p><strong><code>LogisticRegression</code></strong> (inherits from <strong><code>LinearModel</code></strong>):</p>
<ul>
<li><code>sig(x)</code>: A method that represents the logistic sigmoid function. This method computes the vector of values <span class="math inline">\(\sigma(x_i)\)</span> given an vector of inputs <span class="math inline">\(\mathbf{x}\)</span>: <span class="math display">\[
\sigma(x_i) = \frac{1}{1 + e^{-x_i}}
\]</span></li>
<li><code>loss(X, y)</code>: A method to compute the empirical risk <span class="math inline">\(L(\mathbf{w})\)</span> using the logistic loss function. Note that the scores are computed using <strong><span class="math inline">\(X\)</span></strong> and the loss computation involves <span class="math inline">\(\mathbf{y}\)</span>. The returned value is a scalar/real number that gives a quantitative measure of the empirical risk: <span class="math display">\[
L(\mathbf{w}) = \frac{1}{n}\sum_{i = 1}^n{[-y_ilog(\sigma(s_i)) - (1 - y_i)log(1 - \sigma(s_i))]}
\]</span></li>
<li><code>grad(X, y)</code>: A method to compute the gradient <span class="math inline">\(\nabla L(\mathbf{w})\)</span> of the empirical risk function <span class="math inline">\(L(\mathbf{w})\)</span>. Note that the scores are computed using <strong><span class="math inline">\(X\)</span></strong> and the gradient computation involves <span class="math inline">\(\mathbf{y}\)</span>. The returned value is a vector with <span class="math inline">\(p\)</span> entries that represents the direction and rate of change of the empirical risk function given the current weights vector <span class="math inline">\(\mathbf{w}\)</span>. Ultimately, this method computes the gradient used to perform a step of gradient descent in logistic regression: <span class="math display">\[
\nabla L(\mathbf{w}) = \frac{1}{n}\sum_{i = 1}^n{(\sigma(s_i) - y_i)x_i}
\]</span></li>
</ul>
<p><strong><code>GradientDescentOptimizer</code></strong>:</p>
<ul>
<li><code>self.lr</code>: An instance variable of a <code>LogisticRegression</code> object. This variable is used to access the implicit model’s current weights vector <span class="math inline">\(\mathbf{w_k}\)</span> during a gradient descent step.</li>
<li><code>self.prev_w</code>: An instance variable of the previous weights vector <span class="math inline">\(\mathbf{w_{k-1}}\)</span> (initialized to <code>self.lr.w</code>). This variable is used to access the model’s previous weights vector <span class="math inline">\(\mathbf{w_{k-1}}\)</span> during a gradient descent step.</li>
<li><code>step(X, y, alpha, beta)</code>: A method that computes a step of gradient descent <em>with momentum</em>. This method performs the optimization update to <span class="math inline">\(\mathbf{w}\)</span> using the gradient of the <code>LogisticRegression</code> object (which needs <strong><span class="math inline">\(X\)</span></strong> and <span class="math inline">\(\mathbf{y}\)</span>). The arguments <code>alpha</code> and <code>beta</code> are hyperparameters for the optimization update; <span class="math inline">\(\alpha\)</span> is the learning rate and <span class="math inline">\(\beta\)</span> is the momentum scalar. The update to the weights vector <span class="math inline">\(\mathbf{w}\)</span> performed by this method is: <span class="math display">\[
\mathbf{w_{k+1}} = \mathbf{w_{k}} - \alpha\nabla L(\mathbf{w}) + \beta(\mathbf{w_k} - \mathbf{w_{k-1}})
\]</span></li>
</ul>
</section>
<section id="experimenting-with-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="experimenting-with-logistic-regression">Experimenting With Logistic Regression</h2>
<p>In order to conduct some basic experiments with my logistic regression implementation, it is necessary to generate some simulated data for a binary classification problem. This experiment data is generated and visualized in the code cell below:</p>
<div id="cell-6" class="cell" data-execution_count="1194">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating data for binary classification - code provided by Prof. Chodrow</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classification_data(n_points <span class="op">=</span> <span class="dv">300</span>, noise <span class="op">=</span> <span class="fl">0.2</span>, p_dims <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> tch.arange(n_points) <span class="op">&gt;=</span> <span class="bu">int</span>(n_points <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> y</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> y[:, <span class="va">None</span>] <span class="op">+</span> tch.normal(<span class="fl">0.0</span>, noise, size <span class="op">=</span> (n_points,p_dims))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> tch.cat((X, tch.ones((X.shape[<span class="dv">0</span>], <span class="dv">1</span>))), <span class="dv">1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> classification_data(noise <span class="op">=</span> <span class="fl">0.5</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the generated data above</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>markers <span class="op">=</span> [<span class="st">"o"</span> , <span class="st">","</span>]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom color map</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">"purple"</span>, <span class="st">"darkorange"</span>]  </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> LinearSegmentedColormap.from_list(<span class="st">"my_cmap"</span>, colors, N<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Some code provided by Prof. Chodrow</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> y <span class="op">==</span> targets[i]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[ix, <span class="dv">0</span>], X[ix, <span class="dv">1</span>], s <span class="op">=</span> <span class="dv">20</span>,  c <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> y[ix] <span class="op">-</span> <span class="dv">1</span>, facecolors <span class="op">=</span> <span class="st">"none"</span>, edgecolors <span class="op">=</span> <span class="st">"none"</span>, cmap <span class="op">=</span> cmap, vmin <span class="op">=</span> <span class="op">-</span><span class="dv">2</span>, vmax <span class="op">=</span> <span class="dv">2</span>, alpha <span class="op">=</span> <span class="fl">0.75</span>, marker <span class="op">=</span> markers[i])</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"$x_1$"</span>, ylabel <span class="op">=</span> <span class="vs">r"$x_2$"</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Binary Classification Data"</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above generates some random binary classification data and visualizes the generated data (some code provided by Prof.&nbsp;Chodrow).</em></p>
<p><strong>Figure 1</strong></p>
<p>Above is a visualization of the generated binary classification data. While the data above does not appear to be highly conducive to perfectly separable decision boundaries, it still displays clear general regions of the two groups of data points.</p>
<div id="cell-8" class="cell" data-execution_count="1195">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model interpretation helper methods</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">## Loss value plotter</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_plot(loss_vec1, loss_vec2 <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plotting the loss values of the model across each optimization iteration</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    ax.plot(loss_vec1, color <span class="op">=</span> <span class="st">"purple"</span>, linewidth <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> <span class="st">"Evolution of Empirical Loss Value"</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (loss_vec2 <span class="op">!=</span> <span class="va">None</span>):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> <span class="st">"Gradient Descent Method Comparison</span><span class="ch">\n</span><span class="st">of Empirical Loss Value Convergence"</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        ax.plot(loss_vec2, color <span class="op">=</span> <span class="st">"darkorange"</span>, linewidth <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        ax.legend([<span class="st">"Standard"</span>, <span class="st">"Momentum"</span>], frameon <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        ax.axhline(loss_vec2[<span class="op">-</span><span class="dv">2</span>], color <span class="op">=</span> <span class="st">"black"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"Optimization Iteration"</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Model accuracy plotter</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> acc_plot(accs1, accs2 <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plotting the accuracies of the model across each optimization iteration</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    ax.plot(accs1, color <span class="op">=</span> <span class="st">"purple"</span>, linewidth <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (accs2 <span class="op">!=</span> <span class="va">None</span>):</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        ax.plot(accs2, color <span class="op">=</span> <span class="st">"darkorange"</span>, linewidth <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        ax.legend([<span class="st">"Training Accuracy"</span>, <span class="st">"Testing Accuracy"</span>], frameon <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">"Model Accuracy Across Optimization Iteration"</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="vs">r"Gradient Descent Iteration"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"Accuarcy"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision line plotting helper method - code provided by Prof. Chodrow</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_line(X, w, x_min, x_max, ax, <span class="op">**</span>kwargs):</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    w_ <span class="op">=</span> w.flatten()</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tch.linspace(x_min, x_max, X.shape[<span class="dv">0</span>])</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> (((w_[<span class="dv">0</span>] <span class="op">*</span> x) <span class="op">+</span> w_[<span class="dv">2</span>])<span class="op">/</span>w_[<span class="dv">1</span>])</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y, <span class="op">**</span>kwargs)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision region plotter</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decision_bound(model, X, y):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creating a mesh grid</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>(), X[:, <span class="dv">0</span>].<span class="bu">max</span>()</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Drawing the decision line</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    draw_line(X, model.w, x_min, x_max, ax, color <span class="op">=</span> <span class="st">"slategray"</span>, linewidth <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    markers <span class="op">=</span> [<span class="st">"o"</span> , <span class="st">","</span>]</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Custom color map</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [<span class="st">"purple"</span>, <span class="st">"darkorange"</span>]  </span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    cmap <span class="op">=</span> LinearSegmentedColormap.from_list(<span class="st">"my_cmap"</span>, colors, N<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Some code below provided by Prof. Chodrow</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> y <span class="op">==</span> targets[i]</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X[ix, <span class="dv">0</span>], X[ix, <span class="dv">1</span>], s <span class="op">=</span> <span class="dv">20</span>,  c <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> y[ix] <span class="op">-</span> <span class="dv">1</span>, facecolors <span class="op">=</span> <span class="st">"none"</span>, edgecolors <span class="op">=</span> <span class="st">"none"</span>, cmap <span class="op">=</span> cmap, vmin <span class="op">=</span> <span class="op">-</span><span class="dv">2</span>, vmax <span class="op">=</span> <span class="dv">2</span>, alpha <span class="op">=</span> <span class="fl">0.75</span>, marker <span class="op">=</span> markers[i])</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"$x_1$"</span>, ylabel <span class="op">=</span> <span class="vs">r"$x_2$"</span>)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">"Decision Regions of Logistic Regression Model"</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    ax.text(X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">*</span> <span class="fl">0.8</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">*</span> <span class="fl">0.85</span>, <span class="ss">f"Model Accuracy:</span><span class="ch">\n</span><span class="sc">{</span><span class="bu">round</span>(acc(model, X, y), <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>, fontsize <span class="op">=</span> <span class="dv">10</span>, ha <span class="op">=</span> <span class="st">"center"</span>, bbox <span class="op">=</span> <span class="bu">dict</span>(facecolor <span class="op">=</span> <span class="st">"white"</span>, alpha <span class="op">=</span> <span class="fl">0.75</span>, edgecolor <span class="op">=</span> <span class="st">"gray"</span>, boxstyle <span class="op">=</span> <span class="st">"round,pad = 0.3"</span>))</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate model accuracy</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> acc(model, X, y):</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute model predictions</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model.predict(X)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Determine the number of correct predictions</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    correct_preds <span class="op">=</span> ((preds <span class="op">==</span> y) <span class="op">*</span> <span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the rate of correct predictions</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tch.mean(correct_preds).item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><em>Code above defines plotting methods for observing the model’s empirical loss value evolution, the model’s accuracy, and the model’s classification decision boundaries (some code provided by Prof.&nbsp;Chodrow).</em></p>
<section id="experiment-1-standard-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="experiment-1-standard-gradient-descent">Experiment 1: Standard Gradient Descent</h3>
<div id="cell-10" class="cell" data-execution_count="1196">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Testing vanilla gradient descent - some code provided by Prof. Chodrow</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> GradientDescentOptimizer(LR)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Keeping track of the initial model accuracy</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>it <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>accs <span class="op">=</span> []</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(it):</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recording current loss value and model accuracy</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    losses.append(LR.loss(X, y))</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    accs.append(acc(LR, X, y))</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Displaying gradient descent progress for the first 5 iterations</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">20</span> <span class="op">==</span> <span class="dv">0</span>) <span class="op">&amp;</span> (i <span class="op">&lt;</span> <span class="dv">100</span>):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Current Loss value: </span><span class="sc">{</span><span class="bu">round</span>(losses[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Current Model Accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accs[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    opt.step(X, y, alpha <span class="op">=</span> <span class="fl">0.1</span>, beta <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"...</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>it<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final Loss value: </span><span class="sc">{</span><span class="bu">round</span>(losses[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final Model Accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accs[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 0:
Current Loss value: 0.861
Current Model Accuracy: 89.33%

Iteration 20:
Current Loss value: 0.734
Current Model Accuracy: 91.67%

Iteration 40:
Current Loss value: 0.646
Current Model Accuracy: 92.33%

Iteration 60:
Current Loss value: 0.583
Current Model Accuracy: 92.33%

Iteration 80:
Current Loss value: 0.536
Current Model Accuracy: 92.67%

...

Iteration 100:
Final Loss value: 0.501
Final Model Accuracy: 92.67%</code></pre>
</div>
</div>
<p><em>Code above conducts a standard gradient descent procedure with <span class="math inline">\(100\)</span> iterations using <code>LogisticRegression</code> and <code>GradientDescent</code> objects and outputs the results of every 20th iteration (some code provided by Prof.&nbsp;Chodrow)</em></p>
<p>The output above highlights the model’s empirical loss value and accuracy at every 20th iteration (<span class="math inline">\(100\)</span> iterations total) during a standard gradient descent procedure. Over the optimization iterations displayed above, the model’s empirical loss value decreases while its accuracy increases.</p>
<div id="cell-12" class="cell" data-execution_count="1197">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing standard gradient descent</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>loss_plot(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the evolution of the model’s empirical loss value across the optimization iterations of standard gradient descent.</em></p>
<p><strong>Figure 2</strong></p>
<p>As further shown in the figure above, the model’s empirical loss value decreases across each iteration of the optimization process. The initial empirical loss value is approximately <span class="math inline">\(0.86\)</span> while the post-optimization loss value is about <span class="math inline">\(0.5\)</span>, indicating a ~<span class="math inline">\(48\%\)</span> loss-value decrease. This suggests that my implementation of logistic regression works as is generally expected. That is, during a standard gradient procedure, the current linear model’s weights vector <span class="math inline">\(\mathbf{w}\)</span> yields a lower empirical loss value than that of the previous model during each iteration of optimization.</p>
<div id="cell-14" class="cell" data-execution_count="1198">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the evolution of the model accuracy</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>acc_plot(accs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the model accuracy at each iteration of the standard gradient descent procedure above.</em></p>
<p><strong>Figure 3</strong></p>
<p>As displayed above, the accuracy of the model generally increases with each iteration of the gradient procedure, climbing from ~<span class="math inline">\(89\%\)</span> to ~<span class="math inline">\(93\%\)</span> accuracy. This aligns with the fact that the empirical loss value of the model is shown to decrease with each iteration of gradient descent. That is, the model’s accuracy and its empirical loss value are inversely related.</p>
<div id="cell-16" class="cell" data-execution_count="1199">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the decision boundaries</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>decision_bound(LR, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the data along with the separation line of the refined model (displaying the model accuracy as well).</em></p>
<p><strong>Figure 4</strong></p>
<p>The figure above depicts the classification decision boundary of the refined model. While the classification line does not indicate flawless classification for <em>all</em> data points, it illustrates correct classification for most of the data. This is supported by the model’s fairly high ~<span class="math inline">\(93\%\)</span> accuracy.</p>
</section>
<section id="experiment-2-gradient-descent-with-momentum" class="level3">
<h3 class="anchored" data-anchor-id="experiment-2-gradient-descent-with-momentum">Experiment 2: Gradient Descent With Momentum</h3>
<div id="cell-19" class="cell" data-execution_count="1200">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Testing gradient descent with momentum - some code provided by Prof. Chodrow</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>LR_v <span class="op">=</span> LogisticRegression()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>opt_v <span class="op">=</span> GradientDescentOptimizer(LR_v) <span class="co"># Vanilla gradient descent optimizer</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>LR_m <span class="op">=</span> LogisticRegression()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>opt_m <span class="op">=</span> GradientDescentOptimizer(LR_m) <span class="co"># Momentum gradient descent optimizer</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Arrays to store the loss values of the models optimized with and without momentum</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>losses_v <span class="op">=</span> []</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>losses_m <span class="op">=</span> []</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>at_tol <span class="op">=</span> <span class="va">False</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteration counters to track convergence</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>v_i <span class="op">=</span> np.iinfo(np.int64).<span class="bu">max</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>m_i <span class="op">=</span> np.iinfo(np.int64).<span class="bu">max</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> (at_tol <span class="op">!=</span> <span class="va">True</span>):</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recording current loss value and model accuracy</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    loss_v <span class="op">=</span> LR_v.loss(X, y)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    loss_m <span class="op">=</span> LR_m.loss(X, y)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (loss_v <span class="op">&gt;</span> <span class="fl">0.3</span>):</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        losses_v.append(loss_v)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        opt_v.step(X, y, alpha <span class="op">=</span> <span class="fl">0.1</span>, beta <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> (v_i <span class="op">&gt;</span> i):</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        v_i <span class="op">=</span> i</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (loss_m <span class="op">&gt;</span> <span class="fl">0.3</span>):</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        losses_m.append(loss_m)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        opt_m.step(X, y, alpha <span class="op">=</span> <span class="fl">0.1</span>, beta <span class="op">=</span> <span class="fl">0.9</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> (m_i <span class="op">&gt;</span> i):</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        m_i <span class="op">=</span> i</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Terminating condition</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (loss_v <span class="op">&lt;</span> <span class="fl">0.3</span>) <span class="op">&amp;</span> (loss_m <span class="op">&lt;</span> <span class="fl">0.3</span>):</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>        at_tol <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Displaying gradient descent progress for the first 5 iterations</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">20</span> <span class="op">==</span> <span class="dv">0</span>) <span class="op">&amp;</span> (i <span class="op">&lt;</span> <span class="dv">100</span>):</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Current Loss value (Standard Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_v[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Current Loss value (Momentum Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_m[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"...</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard Gradient Descent Converges at </span><span class="sc">{</span>v_i<span class="sc">}</span><span class="ss"> Iterations"</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Momentum Gradient Descent Converges at </span><span class="sc">{</span>m_i<span class="sc">}</span><span class="ss"> Iterations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 0:
Current Loss value (Standard Gradient Descent): 0.749
Current Loss value (Momentum Gradient Descent): 0.982

Iteration 20:
Current Loss value (Standard Gradient Descent): 0.651
Current Loss value (Momentum Gradient Descent): 0.437

Iteration 40:
Current Loss value (Standard Gradient Descent): 0.584
Current Loss value (Momentum Gradient Descent): 0.311

Iteration 60:
Current Loss value (Standard Gradient Descent): 0.535
Current Loss value (Momentum Gradient Descent): 0.301

Iteration 80:
Current Loss value (Standard Gradient Descent): 0.498
Current Loss value (Momentum Gradient Descent): 0.301

...

Standard Gradient Descent Converges at 576 Iterations
Momentum Gradient Descent Converges at 47 Iterations</code></pre>
</div>
</div>
<p><em>Code above initializes <code>LogisticRegression</code> and <code>GradientDescentOptimizer</code> objects for both standard gradient descent and gradient descent with momentum optimizers. The output above shows convergence progress every <span class="math inline">\(20\)</span> iterations and depicts the number of optimization iterations needed for each gradient descent process to achieve a loss value <span class="math inline">\(&lt; 0.3\)</span> (some code provided by Prof.&nbsp;Chodrow).</em></p>
<p>In this experiment, I compare the rate of convergence between the standard gradient descent and gradient descent with momentum procedures. For this experiment, I loosely define “convergence” to be when the optimized model’s empirical loss value is <span class="math inline">\(&lt; 0.3\)</span>. Note that for this analysis, I have set the hyperparameters to <span class="math inline">\(\alpha_s, \alpha_m = 0.1\)</span> for both optimizers and <span class="math inline">\(\beta_s = 0\)</span> (standard gradient descent), <span class="math inline">\(\beta_m = 0.9\)</span> (gradient descent with momentum). As presented in the output above, the gradient descent with momentum optimizer achieves a loss value <span class="math inline">\(&lt; 0.3\)</span> in significantly fewer iterations than that of the standard gradient descent optimizer. That is, the gradient descent with momentum process appears to converge in about <span class="math inline">\(\frac{1}{12}\)</span> of the iterations needed for the standard gradient descent process to converge.</p>
<div id="cell-21" class="cell" data-execution_count="1201">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparing the convergence rates of the two gradient descent methods</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>loss_plot(losses_v, losses_m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the convergence rates of the empirical loss values for both the standard gradient descent and gradient descent with momentum procedures.</em></p>
<p><strong>Figure 5</strong></p>
<p>This figure above is a visual accompaniment to the output from above. In line with the aforementioned output, the gradient descent with momentum optimizer clearly converges to the tolerance (<span class="math inline">\(&lt; 0.3\)</span>) empirical loss value rapidly quicker (at <span class="math inline">\(47\)</span> iterations) than the standard gradient descent optimizer (at <span class="math inline">\(576\)</span> iterations).</p>
</section>
<section id="experiment-3-investigating-overfitting" class="level3">
<h3 class="anchored" data-anchor-id="experiment-3-investigating-overfitting">Experiment 3: Investigating Overfitting</h3>
<p>To observe an instance of my logistic regression implementation leading to overfitting, I have generated two new sets of binary classification data. The two new data sets are identically designed, where the number of features per data point exceeds the number of data points. One dataset will be used as the “training data” and the other the “testing data”.</p>
<div id="cell-24" class="cell" data-execution_count="1202">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating binary classification data for overfitting - code provided by Prof. Chodrow</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> classification_data(n_points <span class="op">=</span> <span class="dv">50</span>, noise <span class="op">=</span> <span class="fl">0.5</span>, p_dims <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> classification_data(n_points <span class="op">=</span> <span class="dv">50</span>, noise <span class="op">=</span> <span class="fl">0.5</span>, p_dims <span class="op">=</span> <span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><em>Code above generates the “training” and “testing” data sets for the overfitting experiment (some code provided by Prof.&nbsp;Chodrow).</em></p>
<div id="cell-26" class="cell" data-execution_count="1203">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Investigating overfitting as the number of dimensions in the data grows past the number of data points</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>LR  <span class="op">=</span> LogisticRegression()</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> GradientDescentOptimizer(LR)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Current accuracy variable</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>accs <span class="op">=</span> []</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>tst_accs <span class="op">=</span> []</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>curr_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>j <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> (curr_acc <span class="op">&lt;</span> <span class="fl">1.0</span>):</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculating the current model accuracy</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    curr_acc <span class="op">=</span> acc(LR, X_train, y_train)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    tst_acc <span class="op">=</span> acc(LR, X_test, y_test)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    accs.append(curr_acc)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    tst_accs.append(tst_acc)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    opt.step(X_train, y_train, alpha <span class="op">=</span> <span class="fl">0.1</span>, beta <span class="op">=</span> <span class="fl">0.9</span>) <span class="co"># Using gradient descent with momentum</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteration counter</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    j<span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model Achieved 100.0% Accuracy on Training Data in </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss"> Iterations (Gradient Descent w/ Momentum)"</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model Accuracy on Testing Data: </span><span class="sc">{</span><span class="bu">round</span>(tst_accs[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model Achieved 100.0% Accuracy on Training Data in 14 Iterations (Gradient Descent w/ Momentum)
Model Accuracy on Testing Data: 86.0%</code></pre>
</div>
</div>
<p><em>Code above conducts a model overfitting experiment by first setting the number of data features to be greater than the number of data points, then fitting a model using <code>LogisticRegression</code> and <code>GradientDescentOptimizer</code> objects, and finally comparing the model’s training-data performance to its testing-data performance.</em></p>
<p>As indicated by the output above, the logistic regression model is able to achieve maximum classification accuracy on the training data in a fairly small number of iterations. However, when evaluated on the unseen testing data, the model’s accuracy drops significantly to about <span class="math inline">\(86\%\)</span>. This is clear sign of overfitting as the model’s flawless performance on the training data does not translate to its performance on the testing data.</p>
<div id="cell-28" class="cell" data-execution_count="1204">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the model's training and testing accuracies as training accuracy approaches 100%</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>acc_plot(accs, tst_accs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the model’s training and testing accuracies at each iteration of optimization during the overfitting experiment.</em></p>
<p><strong>Figure 6</strong></p>
<p>As shown in the above figure, the model’s training accuracy climbs all the way to <span class="math inline">\(100\%\)</span> while its testing accuracy plateaus at around <span class="math inline">\(86\%\)</span> and does not increase further (in <span class="math inline">\(14\)</span> iterations). This figure stands as a visual representation of how the model is overfitting to the training data and consequently sacrificing performance on the unseen testing data.</p>
</section>
<section id="experiment-4-evaluating-the-performance-of-logisticregression-and-gradientdescentoptimizer-on-empirical-data" class="level3">
<h3 class="anchored" data-anchor-id="experiment-4-evaluating-the-performance-of-logisticregression-and-gradientdescentoptimizer-on-empirical-data">Experiment 4: Evaluating The Performance of <code>LogisticRegression</code> and <code>GradientDescentOptimizer</code> on Empirical Data</h3>
<p>The final experiment in this brief study involves evaluating the performance of my logistic regression implementation on real-world data. For this experiment, I will be using a public data set published to <a href="https://www.kaggle.com/datasets/juice0lover/users-vs-bots-classification?resource=download">kaggle.com</a> by Aleksei Zagorskii. This dataset contains user profile data from the popular Russian social media and networking platform VKontakte (<a href="https://vk.com/">VK.com</a>) and is used to classify users as either real-human users or bots. The data features a combination of numerical and categorical information extracted from public <em>VK.com</em> user profiles.</p>
<div id="cell-31" class="cell" data-execution_count="1205">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reading-in and processing the data to evaluate the model with</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./bots_vs_users.csv"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Selecting a subset of the features to use - selecting only the features with float64 data types</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> []</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> df.columns:</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (df[col].dtype <span class="op">==</span> <span class="st">"float64"</span>):</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        cols.append(col)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>cols.append(<span class="st">"target"</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>df_pruned <span class="op">=</span> df[cols].copy()</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>df_pruned <span class="op">=</span> df_pruned.dropna() <span class="co"># Dropping any rows with missing values</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting the dataframe into tch.tensors</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tch.tensor(df_pruned[cols[:<span class="op">-</span><span class="dv">1</span>]].values).<span class="bu">float</span>()</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tch.tensor(df_pruned[cols[<span class="op">-</span><span class="dv">1</span>]].values).<span class="bu">float</span>()</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalizing the data through standardization to ensure effective and expected behavior from logistic regression</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> tch.mean(X, dim <span class="op">=</span> <span class="dv">0</span>, keepdim <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> tch.std(X, dim <span class="op">=</span> <span class="dv">0</span>, keepdim <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>X_s <span class="op">=</span> (X <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting the data into training (60%) and testing/validation (40%) data</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>X_train, X_tmp, y_train, y_tmp <span class="op">=</span> train_test_split(X_s, y, test_size <span class="op">=</span> <span class="fl">0.4</span>, random_state <span class="op">=</span> <span class="dv">1</span>, stratify <span class="op">=</span> y)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting the testing/validation data into testing (50%, 20% overall) and validation (40%, 20% overall) data</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>X_val, X_test, y_val, y_test <span class="op">=</span> train_test_split(X_tmp, y_tmp, test_size <span class="op">=</span> <span class="fl">0.5</span>, random_state <span class="op">=</span> <span class="dv">1</span>, stratify <span class="op">=</span> y_tmp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><em>Code above imports the dataset used for this experiment, preprocesses it by extracting only the columns with a <code>float64</code> data type, converts the data into <code>tch.tensors</code> for the feature matrix <span class="math inline">\(\mathbf{X}\)</span> and the target vector <span class="math inline">\(\mathbf{y}\)</span>, and standardizes the data to ensure expected logistic regression behavior and avoid machine-precision error. Finally, I conduct a test-train-split procedure to split the data into <span class="math inline">\(60\%\)</span> for training, <span class="math inline">\(20\%\)</span> for validation, and <span class="math inline">\(20\%\)</span> for testing.</em></p>
<p>Originally, the full dataset contained <span class="math inline">\(60\)</span> features, many of which have missing values (<code>NaN</code>, or <code>"Unknown"</code>). To avoid problems converting the missing data into a form that is both interpretable by my logistic regression model and will not distort the classification performance, I have chosen to take a subset of the original columns present in the data. For the purposes of this experiment, I have chosen to select only the columns with a <code>float64</code> data type. The pruned dataset I used for this experiment has <span class="math inline">\(15\)</span> columns (including the target column):</p>
<ol type="1">
<li><code>posts_count</code></li>
<li><code>avg_likes</code></li>
<li><code>links_ratio</code></li>
<li><code>hashtags_ratio</code></li>
<li><code>avg_keywords</code></li>
<li><code>avg_text_length</code></li>
<li><code>attachments_ratio</code></li>
<li><code>avg_comments</code></li>
<li><code>reposts_ratio</code></li>
<li><code>ads_ratio</code></li>
<li><code>avg_views</code></li>
<li><code>posting_frequency_days</code></li>
<li><code>phone_numbers_ratio</code></li>
<li><code>avg_text_uniqueness</code></li>
<li><code>target</code></li>
</ol>
<p>Before fitting my logistic regression models, the data is first standardized (using <span class="math inline">\(\frac{\mathbf{x} - \mu}{\sigma}\)</span> where <span class="math inline">\(\mu, \sigma\)</span> are the mean and STD. of the data). The purpose of this is to ensure numerical stability and optimization symmetry to support my logistic regression model’s ability to converge to an appropriate weights vector <span class="math inline">\(\mathbf{w}\)</span> and minimize the empirical loss function. Finally, the data is split into training (<span class="math inline">\(60\%\)</span> of original), validation (<span class="math inline">\(20\%\)</span> of original), and testing (<span class="math inline">\(20\%\)</span> of original) subsets.</p>
<div id="cell-33" class="cell" data-execution_count="1206">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evlauating my logistic repression implementation on binary classification on the real-world data</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co">## Initializing the models for standard gradient descent and gradient descent with momentum</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>LR_v <span class="op">=</span> LogisticRegression()</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>opt_v <span class="op">=</span> GradientDescentOptimizer(LR_v)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>LR_m <span class="op">=</span> LogisticRegression()</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>opt_m <span class="op">=</span> GradientDescentOptimizer(LR_m)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Bookkeeping arrays</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>it <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>losses_v_tr <span class="op">=</span> []</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>losses_m_tr <span class="op">=</span> []</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>losses_v_val <span class="op">=</span> []</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>losses_m_val <span class="op">=</span> []</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>accs_v_tr <span class="op">=</span> []</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>accs_m_tr <span class="op">=</span> []</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>accs_v_val <span class="op">=</span> []</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>accs_m_val <span class="op">=</span> []</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(it):</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recording current loss and accuracy values on the training and validation data for standard and momentum gradient descent</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    losses_v_tr.append(LR_v.loss(X_train, y_train))</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    losses_m_tr.append(LR_m.loss(X_train, y_train))</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    losses_v_val.append(LR_v.loss(X_val, y_val))</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    losses_m_val.append(LR_m.loss(X_val, y_val))</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    accs_v_tr.append(acc(LR_v, X_train, y_train))</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    accs_m_tr.append(acc(LR_m, X_train, y_train))</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>    accs_v_val.append(acc(LR_v, X_val, y_val))</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    accs_m_val.append(acc(LR_m, X_val, y_val))</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    opt_v.step(X_train, y_train, alpha <span class="op">=</span> <span class="fl">0.1</span>, beta <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    opt_m.step(X_train, y_train, alpha <span class="op">=</span> <span class="fl">0.1</span>, beta <span class="op">=</span> <span class="fl">0.9</span>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="va">True</span>):</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">25</span> <span class="op">==</span> <span class="dv">0</span>) <span class="op">&amp;</span> ((i <span class="op">&gt;</span> <span class="dv">0</span>) <span class="op">&amp;</span> (i <span class="op">&lt;</span> <span class="dv">76</span>)):</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"--------------"</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Iteration: </span><span class="sc">{</span>i<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Training Performance:"</span>)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Current Loss value (Standard Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_v_tr[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Current Loss value (Momentum Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_m_tr[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Current Accuracy (Standard Gradient Descent): </span><span class="sc">{</span>(<span class="bu">round</span>(accs_v_tr[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Current Accuracy (Momentum Gradient Descent): </span><span class="sc">{</span>(<span class="bu">round</span>(accs_m_tr[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Validation Performance:"</span>)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Current Loss value (Standard Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_v_val[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Current Loss value (Momentum Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_m_val[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Current Accuracy (Standard Gradient Descent): </span><span class="sc">{</span>(<span class="bu">round</span>(accs_v_val[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Current Accuracy (Momentum Gradient Descent): </span><span class="sc">{</span>(<span class="bu">round</span>(accs_m_val[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"..."</span>)</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final Iteration: </span><span class="sc">{</span>it<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Training Performance:"</span>)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loss value (Standard Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_v_tr[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loss value (Momentum Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_m_tr[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (Standard Gradient Descent): </span><span class="sc">{</span>(<span class="bu">round</span>(accs_v_tr[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (Momentum Gradient Descent): </span><span class="sc">{</span>(<span class="bu">round</span>(accs_m_tr[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Validation Performance:"</span>)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loss value (Standard Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_v_val[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loss value (Momentum Gradient Descent): </span><span class="sc">{</span><span class="bu">round</span>(losses_m_val[<span class="op">-</span><span class="dv">1</span>], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (Standard Gradient Descent): </span><span class="sc">{</span>(<span class="bu">round</span>(accs_v_val[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (Momentum Gradient Descent): </span><span class="sc">{</span>(<span class="bu">round</span>(accs_m_val[<span class="op">-</span><span class="dv">1</span>], <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--------------
Iteration: 25

Training Performance:
Current Loss value (Standard Gradient Descent): 0.905
Current Loss value (Momentum Gradient Descent): 0.622
Current Accuracy (Standard Gradient Descent): 79.62%
Current Accuracy (Momentum Gradient Descent): 85.37%

Validation Performance:
Current Loss value (Standard Gradient Descent): 0.998
Current Loss value (Momentum Gradient Descent): 0.762
Current Accuracy (Standard Gradient Descent): 78.42%
Current Accuracy (Momentum Gradient Descent): 82.37%

--------------
Iteration: 50

Training Performance:
Current Loss value (Standard Gradient Descent): 0.74
Current Loss value (Momentum Gradient Descent): 0.592
Current Accuracy (Standard Gradient Descent): 84.53%
Current Accuracy (Momentum Gradient Descent): 88.01%

Validation Performance:
Current Loss value (Standard Gradient Descent): 0.823
Current Loss value (Momentum Gradient Descent): 0.73
Current Accuracy (Standard Gradient Descent): 82.37%
Current Accuracy (Momentum Gradient Descent): 85.61%

--------------
Iteration: 75

Training Performance:
Current Loss value (Standard Gradient Descent): 0.672
Current Loss value (Momentum Gradient Descent): 0.587
Current Accuracy (Standard Gradient Descent): 85.97%
Current Accuracy (Momentum Gradient Descent): 87.89%

Validation Performance:
Current Loss value (Standard Gradient Descent): 0.752
Current Loss value (Momentum Gradient Descent): 0.715
Current Accuracy (Standard Gradient Descent): 83.45%
Current Accuracy (Momentum Gradient Descent): 85.97%

...
Final Iteration: 100
Final Training Performance:
Loss value (Standard Gradient Descent): 0.644
Loss value (Momentum Gradient Descent): 0.584
Accuracy (Standard Gradient Descent): 86.93%
Accuracy (Momentum Gradient Descent): 88.01%

Final Validation Performance:
Loss value (Standard Gradient Descent): 0.728
Loss value (Momentum Gradient Descent): 0.707
Accuracy (Standard Gradient Descent): 84.53%
Accuracy (Momentum Gradient Descent): 85.97%
</code></pre>
</div>
</div>
<p><em>Code above fits my logistic regression model to the real-world training data and records the classification accuracy on the real-world validation data during model training.</em></p>
<p>To evaluate my implementation of logistic regression on the real-world data, I fit two logistic regression models to the training data: One model is optimized with standard gradient descent and the other uses gradient descent with momentum. As displayed by the output above, the empirical losses for each model decrease over the iterations of gradient descent on both the training and validation data. Along these lines, the corresponding accuracies of each model gradually increase, which is expected.</p>
<section id="training-data-empirical-loss-evolution-and-accuracies" class="level4">
<h4 class="anchored" data-anchor-id="training-data-empirical-loss-evolution-and-accuracies"><strong>Training Data Empirical Loss Evolution and Accuracies</strong></h4>
<div id="cell-36" class="cell" data-execution_count="1207">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the losses for training data</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>loss_plot(losses_v_tr, losses_m_tr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the loss value evolution for standard and momentum gradient descent on the training data.</em></p>
<p><strong>Figure 7 (7a above &amp; 7b below)</strong></p>
<p>Above shows the loss value evolution of the standard and momentum gradient descent models optimizing over the training data. As expected the loss values decrease over the optimization iterations for both models, but the model using momentum gradient descent minimizes the loss much faster. Below are the changing model accuracies for the training data. Inline with the plot above, the accuracies for each model improve during the optimization process, but the model using momentum gradient descent is able to achieve a higher accuracy faster than the model using standard gradient descent.</p>
<div id="cell-38" class="cell" data-execution_count="1208">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the accuracies for training data</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>acc_plot(accs_v_tr, accs_m_tr)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">"Standard"</span>, <span class="st">"Momentum"</span>], frameon <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the accuracies for standard and momentum gradient descent on the training data.</em></p>
</section>
<section id="validation-data-empirical-loss-evolution-and-accuracies" class="level4">
<h4 class="anchored" data-anchor-id="validation-data-empirical-loss-evolution-and-accuracies"><strong>Validation Data Empirical Loss Evolution and Accuracies</strong></h4>
<div id="cell-41" class="cell" data-execution_count="1209">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the losses for training data</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>loss_plot(losses_v_val, losses_m_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the loss value evolution for standard and momentum gradient descent on the validation data.</em></p>
<p><strong>Figure 8 (8a above &amp; 8b below)</strong></p>
<p>Above shows the loss value evolution of the standard and momentum gradient descent models now optimizing over the validation data. Again as expected the loss values decrease over the optimization iterations for both models, and the model using momentum gradient descent continues to minimize the loss much faster. Below shows the changing model accuracies now for the validation data. Further aligned with the plot above, the accuracies for each model still improve during the optimization process, and the model using momentum gradient descent is again able to achieve a higher accuracy faster than the model using standard gradient descent.</p>
<div id="cell-43" class="cell" data-execution_count="1210">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the accuracies for training data</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>acc_plot(accs_v_val, accs_m_val)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">"Standard"</span>, <span class="st">"Momentum"</span>], frameon <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><em>Code above plots the accuracies for standard and momentum gradient descent on the validation data.</em></p>
</section>
<section id="testing-data-empircal-loss-value-and-accuracy" class="level4">
<h4 class="anchored" data-anchor-id="testing-data-empircal-loss-value-and-accuracy"><strong>Testing Data Empircal Loss Value and Accuracy</strong></h4>
<div id="cell-46" class="cell" data-execution_count="1211">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating the loss and accuracy of each model on the testing data</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>loss_v_tst <span class="op">=</span> LR_v.loss(X_test, y_test)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>loss_m_tst <span class="op">=</span> LR_m.loss(X_test, y_test)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>acc_v_tst <span class="op">=</span> acc(LR_v, X_test, y_test)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>acc_m_tst <span class="op">=</span> acc(LR_m, X_test, y_test)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing Data Empirical Loss:"</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model 1 (Standard Gradient Descent) | </span><span class="sc">{</span><span class="bu">round</span>(loss_v_tst, <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model 2 (Momentum Gradient Descent) | </span><span class="sc">{</span><span class="bu">round</span>(loss_m_tst, <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-----</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing Data Accuracy:"</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model 1 (Standard Gradient Descent) | </span><span class="sc">{</span>(<span class="bu">round</span>(acc_v_tst, <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model 2 (Momentum Gradient Descent) | </span><span class="sc">{</span>(<span class="bu">round</span>(acc_m_tst, <span class="dv">4</span>) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Testing Data Empirical Loss:
Model 1 (Standard Gradient Descent) | 0.622
Model 2 (Momentum Gradient Descent) | 0.538
-----

Testing Data Accuracy:
Model 1 (Standard Gradient Descent) | 89.61%
Model 2 (Momentum Gradient Descent) | 91.04%</code></pre>
</div>
</div>
<p><em>Code above computes the empirical loss and model accuracy on the testing data for each logistic regression model.</em></p>
<p>Finally, on the unseen testing data, the model using gradient descent with momentum yields both a lower loss value (<span class="math inline">\(0.538\)</span>) and higher accuracy (<span class="math inline">\(91.04\%\)</span>) than those of the model using standard gradient descent (<span class="math inline">\(0.622\)</span>, <span class="math inline">\(89.61\%\)</span>). Interestingly, both models yield smaller loss values and achieve higher accuracies from the unseen testing data than the training or validation data.</p>
</section>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Over the course of this introductory yet comprehensive exploration of logistic regression, I investigated the fundamental mathematical processes that operate under the hood of logistic regression ML models, implemented my own basic version of a logistic regression model, and evaluated its classification capabilities through a series of experiments.</p>
<p>Through the four rudimentary experiments, I first examined the rate of convergence/optimization of my logistic regression model by comparing its ability to minimize the empirical loss function using two methods of gradient descent: standard gradient descent and gradient descent with momentum. Overall, I found that both optimization methods yield desirable results (decreasing the empirical loss value and increasing the classification accuracy over each iteration), but that a model using gradient descent with momentum is able to minimize the empirical loss function and produce a refined weights vector <span class="math inline">\(\mathbf{w}\)</span> much quicker than a model using standard gradient descent.</p>
<p>I then experimented with overfitting, allowing my model to achieve flawless classification accuracy on training data with more features than data points. While my model was able to reach <span class="math inline">\(100\%\)</span> accuracy on the training data in very few gradient descent iterations, it was not able to translate its training-data classification abilities to an analogous set of unseen testing data. In this experiment, I show how while my logistic regression model works effectively for basic classification tasks, it is still subject to overfitting in certain circumstances (which is much like many other ML models out there!).</p>
<p>Lastly, I evaluated the classification performance of my model on a real-world, empirical dataset. Using a public data set published to <a href="https://www.kaggle.com/datasets/juice0lover/users-vs-bots-classification?resource=download">kaggle.com</a> by Aleksei Zagorskii, I examined my model’s ability to correctly classify data points as either real-human users or bots from public profiles on the popular Russian social media and networking platform VKontakte (<a href="https://vk.com/">VK.com</a>). In this final experiment, I opted to select a subset of the data containing only numerical columns to avoid data conversion issues and support expected model behavior. I had to convert the data from a <code>pandas</code> data frame to <code>torch</code> tensors and further standardize the data to ensure numerical stability and symmetry and prevent computation errors during the gradient descent optimization process. Using training, validation, and testing data subsets, I examined the optimization processes and classification abilities of my logistic regression model using both of the gradient descent methods explored previously in this study. Overall, I found that both gradient descent versions of my model were able to achieve a low empirical loss and fairly high classification accuracy on the real-world data (for training, validation, and testing), but that the model using gradient descent with momentum consistently outperformed the model using standard gradient descent.</p>
<p>Overall, in this analysis I’ve developed a deeper understanding of the inner-workings of logistic regression and the benefits of different gradient descent methods. This post provided me with the opportunity to gain more insight into how many classic ML models operate and why certain aspects of their design, like their optimization procedures, are designed cleverly to achieve desirable results faster. Further, this study gave me experience fitting a custom-built model to real data, exposing me to the difficulties and challenges that come along with preprocessing and standardizing data.</p>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Zagorskii Aleksei. (2025). Users vs bots classification [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/6999922</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>